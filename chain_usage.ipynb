{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danliu/miniconda3/envs/chatbot/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from api.chains.off_topic_chain import create_off_topic_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature=0, model_name = \"gpt-3.5-turbo\", streaming = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.chains.llm_finetune_qa_chain import create_llm_finetun_chain\n",
    "llm_fine_tune_chain = create_llm_finetun_chain(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danliu/miniconda3/envs/chatbot/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.predict_messages` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Fine-tuning is the process of updating a pre-trained language model using labeled data to do a specific task, in order to refine its capabilities and improve performance in that particular task or domain. It involves training the model on smaller, specific datasets to make it more specialized and effective for the target task.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_fine_tune_chain.invoke({\"question\": \"what is finetuning\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi, that seems off-topic.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = create_off_topic_chain(llm)\n",
    "chain.invoke({\"question\": \"what is the color of sky\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/llm_finetune/urls/urls.txt\"\n",
    "from service.llama_index_retrive import LlamaRetriever\n",
    "from service.data_collect import WebPagesToDocuments\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "if_clean_texts = False\n",
    "docs = WebPagesToDocuments(path = path, clean_texts=if_clean_texts).docs\n",
    "llm = ChatOpenAI(temperature=0, model_name = \"gpt-3.5-turbo\", streaming = True)\n",
    "retriever = LlamaRetriever(db_path=\"test_db\",\n",
    "                           embeddings_model=OpenAIEmbedding(model=\"text-embedding-3-small\"),\n",
    "                           docs = docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.chains.clean_texts_from_url_chain import create_clean_texts_from_url_chain\n",
    "from langchain_core.runnables.base import RunnableSequence\n",
    "from typing import List\n",
    "from langchain.chat_models.base import  BaseChatModel\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "class DocumentRefiner:\n",
    "    def __init__(self, llm: BaseChatModel,\n",
    "                 chunk_size:int = 3000, \n",
    "                 chunk_overlap:int = 0):\n",
    "        self.chain = create_clean_texts_from_url_chain(llm)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def get_sentence_splitter(self):\n",
    "        return SentenceSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "        )\n",
    "\n",
    "    def refine_html_files(self, docs: List[Document]) -> List[Document]:\n",
    "        cleaned_docs = []\n",
    "        splitter = self.get_sentence_splitter()\n",
    "        print(\"www\")\n",
    "        for doc in docs:\n",
    "            cleaned_doc_segments = \"\"\n",
    "            doc_segments = splitter.get_nodes_from_documents([doc])\n",
    "\n",
    "            for seg in doc_segments:\n",
    "                print(seg.text)\n",
    "                try:\n",
    "                    \n",
    "                    response = self.chain.invoke({\"question\": seg.text})\n",
    "                    cleaned_doc_segments += response + \"\\n\"\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing segment: {e}\")\n",
    "\n",
    "            doc.text = cleaned_doc_segments\n",
    "            cleaned_docs.append(doc)\n",
    "\n",
    "        return cleaned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from service.data_collect import WebPagesToDocuments\n",
    "path = \"data/llm_finetune/urls/urls.txt\"\n",
    "docs = WebPagesToDocuments(path = path, clean_texts=False).docs\n",
    "llm = ChatOpenAI(temperature=0, model_name = \"gpt-3.5-turbo\", streaming = True)\n",
    "cleaner = DocumentRefiner(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www\n",
      "Join our upcoming webinar âDeriving Business Value from LLMs and RAGs.â\n",
      "\n",
      "[Register\n",
      "now](https://www.superannotate.com/webinar?utm_source=alert_bar&utm_medium=banner&utm_campaign=website_alert_bar)\n",
      "\n",
      "[![superannotate logo](https://assets-global.website-\n",
      "files.com/612770618d97595db63a9470/6127731d30dc5270fa629b99_logoDark.svg)![](https://assets-\n",
      "global.website-\n",
      "files.com/612770618d97595db63a9470/612dea788938e85714b0d752_Logo_Footer.svg)](/)\n",
      "\n",
      "Platform\n",
      "\n",
      "[î¤FineTuneCreate top-quality training data across all data\n",
      "types.](/annotation-tool)\n",
      "\n",
      "[LLM![](https://assets-global.website-\n",
      "files.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/llms)[Image![](https://assets-\n",
      "global.website-\n",
      "files.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/image-\n",
      "annotation-tool)[Video![](https://assets-global.website-\n",
      "files.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/video-\n",
      "annotation)[Text![](https://assets-global.website-\n",
      "files.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/text-\n",
      "annotation)[Audio![](https://assets-global.website-\n",
      "files.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/audio-\n",
      "annotation)\n",
      "\n",
      "[î¤ExploreManage, version, and debug your data and create more accurate\n",
      "datasets faster.](/data-curation)[î¤OrchestrateCreate CI/CD AI pipelines\n",
      "using our built-in neural networks, Python SDK, webhooks, and advanced\n",
      "orchestration.](/orchestrate)[î¤WForceAccess a global marketplace of 400+\n",
      "vetted annotation service teams.](/annotation-services)\n",
      "\n",
      "LLMs & GenAI\n",
      "\n",
      "[î¤ LLM & GenAI Software](/llms)[î¤LLM Expert Workforce](/llms#hire-\n",
      "experts)[î¤Free Playground](/llms-genai-playground)\n",
      "\n",
      "Solutions\n",
      "\n",
      "Industries and use cases\n",
      "\n",
      "[î¤ LLMs & GenAI](/llms-\n",
      "genai)[î¤Agriculture](/agriculture)[î¤Healthcare](/healthcare)[î¤Insurance](/insurance)[î¤Sports](/sports)[î¤Autonomous\n",
      "driving](/autonomous-driving)[î¤Robotics](/robotics)[î¤Aerial\n",
      "imagery](/aerial-imagery)[î¤NLP](/nlp)[î¤Security and\n",
      "surveillance](/security)\n",
      "\n",
      "â¦ and many more\n",
      "\n",
      "Case studies\n",
      "\n",
      "[î¤Hinge Health](https://www.superannotate.com/blog/hinge-health-case-\n",
      "study)[î¤OneCup AI](https://www.superannotate.com/blog/onecup-case-\n",
      "study)[î¤Percepto](https://www.superannotate.com/blog/how-superannotate-\n",
      "helped-percepto-cut-the-time-to-complete-annotation-projects)[î¤Orsi\n",
      "Academy](https://www.superannotate.com/blog/orsi-case-study)\n",
      "\n",
      "Resources\n",
      "\n",
      "Company\n",
      "\n",
      "[î¤Blog](/blog)[î¤Podcast](/podcast)[î¤Webinar](/webinar)[î¤Careers](/careers)[î¤About\n",
      "us](/company)\n",
      "\n",
      "Platform\n",
      "\n",
      "[î¤Documentation](https://doc.superannotate.com/docs)[î¤Whatâs\n",
      "new](https://www.superannotate.com/blog-category/product)[î¤Python\n",
      "SDK](https://doc.superannotate.com/docs/python-sdk)[î¤Integrations and\n",
      "Security](/security-at-superannotate)\n",
      "\n",
      "[Pricing](/pricing)\n",
      "\n",
      "[Sign\n",
      "In](https://auth.superannotate.com/login?__hstc=17958374.8b8562dd440376db6b7733af5a05b8a4.1672215330596.1672215330596.1672215330596.1&__hssc=17958374.7.1672215330597&__hsfp=357064021)[Request\n",
      "Demo](/request-demo)\n",
      "\n",
      "[LLM](/blog-category/llm)\n",
      "\n",
      "# Fine-tuning large language models (LLMs) in 2024\n",
      "\n",
      "February 5, 2024\n",
      "\n",
      "â¢\n",
      "\n",
      "12 min\n",
      "\n",
      "![llm fine tuning](https://assets-global.website-\n",
      "files.com/614c82ed388d53640613982e/653775b2bdff592188a789dd_large-language-\n",
      "models-llm-fine-tuning.webp)\n",
      "\n",
      "[![](https://assets-global.website-\n",
      "files.com/612770618d97595db63a9470/617674c3fb077ab741e5847c_ReadMore-\n",
      "Arrow.svg)Back to Blog](/blog)\n",
      "\n",
      "#### Contents\n",
      "\n",
      "Table of content Item\n",
      "\n",
      "Itâs no secret that [large language models\n",
      "(LLMs)](https://www.superannotate.com/blog/llm-overview) are evolving at a\n",
      "wild speed and are turning heads in the [generative\n",
      "AI](https://www.superannotate.com/blog/generative-ai-explained) industry.\n",
      "Enterprises aren't just intrigued; they're obsessed with LLMs, looking for\n",
      "ways to integrate this technology into their operations. Billions of dollars\n",
      "have been poured into LLM research and development recently. Industry leaders\n",
      "and tech enthusiasts are showing a growing appetite to deepen their\n",
      "understanding of LLMs. While the LLM frontier keeps expanding more and more,\n",
      "staying informed is critical. The value LLMs may add to your business depends\n",
      "on your knowledge and intuition around this technology.\n",
      "\n",
      "A large language model life cycle has several key steps, and today we're going\n",
      "to cover one of the juiciest and most intensive parts of this cycle - the\n",
      "fine-tuning process. This is a laborious, heavy, but rewarding task that's\n",
      "involved in many language model training processes.\n",
      "\n",
      "## Large language model lifecycle\n",
      "\n",
      "Before going over LLM fine-tuning, it's important to understand the LLM\n",
      "lifecycle and how it works.\n",
      "\n",
      "![llm project lifecycle](https://assets-global.website-\n",
      "files.com/614c82ed388d53640613982e/65b7a47ae46886939395ad02_llm-project-\n",
      "lifecycle.webp)\n",
      "\n",
      "1\\. **Vision & scope:** First, you should define the project's vision.\n",
      "Determine if your LLM will be a more universal tool or target a specific task\n",
      "like named entity recognition. Clear objectives save time and resources.\n",
      "\n",
      "2 **. Model selection:** Choose between training a model from scratch or\n",
      "modifying an existing one. In many cases, adapting a pre-existing model is\n",
      "efficient, but some instances require fine-tuning with a new model.\n",
      "\n",
      "3\\. **Model 's performance and adjustment:** After preparing your model, you\n",
      "need to assess its performance. If itâs unsatisfactory, try [prompt\n",
      "engineering](https://www.superannotate.com/blog/llm-prompting-tricks) or\n",
      "further fine-tuning. We'll focus on this part. Ensure the model's outputs are\n",
      "in sync with human preferences.\n",
      "\n",
      "4\\. **Evaluation & iteration:** Conduct evaluations regularly using metrics\n",
      "and benchmarks. Iterate between prompt engineering, fine-tuning, and\n",
      "evaluation until you reach the desired outcomes.\n",
      "\n",
      "5\\. **Deployment:** Once the model performs as expected, deploy it. Optimize\n",
      "for computational efficiency and user experience at this juncture.\n",
      "\n",
      "## What is LLM fine-tuning?\n",
      "\n",
      "Large language model (LLM) fine-tuning is the process of taking pre-trained\n",
      "models and further training them on smaller, specific datasets to refine their\n",
      "capabilities and improve performance in a particular task or domain. Fine-\n",
      "tuning is about turning general-purpose models and turning them into\n",
      "specialized models. It bridges the gap between generic pre-trained models and\n",
      "the unique requirements of specific applications, ensuring that the language\n",
      "model aligns closely with human expectations. Think of OpenAI's GPT-3, a\n",
      "state-of-the-art large language model designed for a broad range of [natural\n",
      "language processing (NLP)](https://www.superannotate.com/blog/what-is-natural-\n",
      "language-processing) tasks. Suppose a healthcare organization wants to use\n",
      "GPT-3 to assist doctors in generating patient reports from textual notes.\n",
      "While GPT-3 can understand and create general text, it might not be optimized\n",
      "for intricate medical terms and specific healthcare jargon.\n",
      "\n",
      "To enhance its performance for this specialized role, the organization fine-\n",
      "tunes GPT-3 on a dataset filled with medical reports and patient notes. It\n",
      "might use tools like [SuperAnnotate's LLM custom\n",
      "editor](https://www.superannotate.com/llms) to build its own model with the\n",
      "desired interface. Through this process, the model becomes more familiar with\n",
      "medical terminologies, the nuances of clinical language, and typical report\n",
      "structures. After fine-tuning, GPT-3 is primed to assist doctors in generating\n",
      "accurate and coherent patient reports, demonstrating its adaptability for\n",
      "specific tasks.\n",
      "\n",
      "![what does fine tuning do for the model](https://assets-global.website-\n",
      "files.com/614c82ed388d53640613982e/65b7a5e7f00d1466ff618b26_what-does-fine-\n",
      "tuning-do-for-the-model.webp)\n",
      "\n",
      "This sounds great to have in every large language model, but remember that\n",
      "everything comes with a cost. We'll discuss that in more detail soon.\n",
      "\n",
      "## When to use fine-tuning\n",
      "\n",
      "Our article about large language models touches upon topics like in-context\n",
      "learning and zero/one/few shot inference. Hereâs aÂ quick recap:\n",
      "\n",
      " **In-context learning** is a method for improving the prompt through specific\n",
      "task examples within the prompt, offering the LLM a blueprint of what it needs\n",
      "to accomplish.\n",
      "\n",
      " **Zero-shot inference** incorporates your input data in the prompt without\n",
      "extra examples. If zero-shot inference doesn't yield the desired results,\n",
      "**'**[ **one-shot**](https://www.superannotate.com/blog/one-shot-annotation)\n",
      "**' or 'few-shot inference'** can be used. These tactics involve adding one or\n",
      "multiple completed examples within the prompt, helping smaller LLMs perform\n",
      "better.\n",
      "\n",
      "![in context learning](https://assets-global.website-\n",
      "files.com/614c82ed388d53640613982e/65b7a642af158d41a94768de_in-context-\n",
      "learning.webp)\n",
      "\n",
      "These are techniques used directly in the user prompt and aim to optimize the\n",
      "model's output and better fit it to the user's preferences. The problem is\n",
      "that they donât always work, especially for smaller LLMs. Here's an example\n",
      "of how in-context learning may fail.\n",
      "\n",
      "Other than that, any examples you include in your prompt take up valuable\n",
      "space in the context window, reducing the space you have to include additional\n",
      "helpful information. And here, finally, comes fine-tuning. Unlike the pre-\n",
      "training phase, with vast amounts of unstructured text data, fine-tuning is a\n",
      "supervised learning process. This means that you use a dataset of labeled\n",
      "examples to update the weights of LLM. These labeled examples are usually\n",
      "prompt-response pairs, resulting in a better completion of specific tasks.\n",
      "\n",
      "## Supervised fine-tuning (SFT)\n",
      "\n",
      "Supervised fine-tuning means updating a pre-trained language model using\n",
      "labeled data to do a specific task. The data used has been checked earlier.\n",
      "This is different from unsupervised methods, where data isn't checked.\n",
      "Usually, the initial training of the language model is unsupervised, but fine-\n",
      "tuning is supervised.\n",
      "\n",
      "### How is fine-tuning performed?\n",
      "\n",
      "Let's get into more details of fine-tuning in LLMs. For preparing the training\n",
      "data, there are many open-source datasets that offer insights into user\n",
      "behaviors and preferences, even if they aren't directly formatted as\n",
      "instructional data. For example, we can take the large data set of Amazon\n",
      "product reviews and turn them into instruction prompt datasets for fine-\n",
      "tuning. Prompt template libraries include many templates for different tasks\n",
      "and different datasets.\n",
      "\n",
      "![how is fine tuning performed](https://assets-global.website-\n",
      "files.com/614c82ed388d53640613982e/65b7a67b97500a346be929a1_how-is-fine-\n",
      "tuning-performed.webp)\n",
      "\n",
      "Once your instruction data set is ready, as with standard supervised learning,\n",
      "you divide the data set into training validation and test splits. During fine-\n",
      "tuning, you select prompts from your training data set and pass them to the\n",
      "LLM, which then generates completions.\n",
      "\n",
      "During the fine-tuning phase, when the model is exposed to a newly labeled\n",
      "dataset specific to the target task, it calculates the error or difference\n",
      "between its predictions and the actual labels. The model then uses this error\n",
      "to adjust its weights, typically via an optimization algorithm like gradient\n",
      "descent. The magnitude and direction of weight adjustments depend on the\n",
      "gradients, which indicate how much each weight contributed to the error.\n",
      "Weights that are more responsible for the error are adjusted more, while those\n",
      "less responsible are adjusted less.\n",
      "\n",
      "Over multiple iterations (or epochs) of the dataset, the model continues to\n",
      "adjust its weights, honing in on a configuration that minimizes the error for\n",
      "the specific task. The aim is to adapt the previously learned general\n",
      "knowledge to the nuances and specific patterns present in the new dataset,\n",
      "thereby making the model more specialized and effective for the target task.\n",
      "\n",
      "During this process, the model is updated with the labeled data. It changes\n",
      "based on the difference between its guesses and the actual answers. This helps\n",
      "the model learn details found in the labeled data. By doing this, the model\n",
      "improves at the task for which it's fine-tuned.\n",
      "\n",
      "Let's take an example to picture this better; if you ask a pre-trained\n",
      "model,\"Why is the sky blue?\" it might reply, \"Because of the way the\n",
      "atmosphere scatters sunlight.\" This answer is simple and direct. However, the\n",
      "answer might be too brief for a chatbot for a science educational platform. It\n",
      "may need more scientific detail or context based on your guidelines. This is\n",
      "where supervised fine-tuning helps.\n",
      "\n",
      "![base model vs fine tuned model](https://assets-global.website-\n",
      "files.com/614c82ed388d53640613982e/65b7a6bd12a6f621f37ebdfa_base-model-vs-\n",
      "fine-tuned-model.webp)\n",
      "\n",
      "After fine-tuning, the model can give a more in-depth response to scientific\n",
      "questions. After fine-tuning, when asked, \"Why is the sky blue?\", the model\n",
      "might provide a more detailed explanation like:\n",
      "\n",
      "\"The sky appears blue because of a phenomenon called Rayleigh scattering. As\n",
      "sunlight enters Earth's atmosphere, it consists of different colors, each with\n",
      "its own wavelength. Blue light has a shorter wavelength and is scattered in\n",
      "all directions by the gases and particles in the atmosphere. This scattering\n",
      "causes the direct sunlight to appear white, but the sky itself to take on a\n",
      "blue hue.\" This enriched response is comprehensive and suitable for a science\n",
      "educational platform.\n",
      "\n",
      "## Fine-tuning methods\n",
      "\n",
      "LLM fine-tuning is a supervised learning process where you use a dataset of\n",
      "labeled examples to update the weights of LLM and make the model improve its\n",
      "ability for specific tasks. Let's explore some of the notable fine-tuning\n",
      "methods.\n",
      "\n",
      "### Instruction fine-tuning\n",
      "\n",
      "One strategy used to improve a model's performance on various tasks is\n",
      "instruction fine-tuning. It's about training the machine learning model using\n",
      "examples that demonstrate how the model should respond to the query. The\n",
      "dataset you use for fine-tuning large language models has to serve the purpose\n",
      "of your instruction. For example, suppose you fine-tune your model to improve\n",
      "its summarization skills. In that case, you should build up a dataset of\n",
      "examples that begin with the instruction to summarize, followed by text or a\n",
      "similar phrase. In the case of translation, you should include instructions\n",
      "like âtranslate this text.â These prompt completion pairs allow your model\n",
      "to \"think\" in a new niche way and serve the given specific task.\n",
      "\n",
      "![using prompts to fine tune llms with instruction](https://assets-\n",
      "global.website-\n",
      "files.com/614c82ed388d53640613982e/65b7a73f7ee95b6c9e261863_using-prompts-to-\n",
      "fine-tune-llms-with-instruction.webp)\n",
      "\n",
      "### Full fine-tuning\n",
      "\n",
      "Instruction fine-tuning, where all of the model's weights are updated, is\n",
      "known as full fine-tuning. The process results in a new version of the model\n",
      "with updated weights. It is important to note that just like pre-training,\n",
      "full fine-tuning requires enough memory and compute budget to store and\n",
      "process all the gradients, optimizers, and other components being updated\n",
      "during training.\n",
      "\n",
      "### Parameter-efficient fine-tuning\n",
      "\n",
      "Training a language model is a computationally intensive task. For a full LLM\n",
      "fine-tuning, you need memory not only to store the model, but also the\n",
      "parameters that are necessary for the training process. Your computer might be\n",
      "able to handle the model weights, but allocating memory for optimizing states,\n",
      "gradients, and forward activations during the training process is a\n",
      "challenging task. Simple hardware cannot handle this amount of hurdle. This is\n",
      "where PEFT is crucial. While full LLM fine-tuning updates every model's weight\n",
      "during the supervised learning process, **PEFT methods only update a small set\n",
      "of parameters**. This transfer learning technique chooses specific model\n",
      "components and \"freezes\" the rest of the parameters. The result is logically\n",
      "having a much smaller number of parameters than in the original model (in some\n",
      "cases, just 15-20% of the original weights; LoRA can reduce the number of\n",
      "trainable parameters by 10,000 times). This makes memory requirements much\n",
      "more manageable. Not only that, but PEFT is also dealing with catastrophic\n",
      "forgetting. Since it's not touching the original LLM, the model does not\n",
      "forget the previously learned information. Full fine-tuning results in a new\n",
      "version of the model for every task you train on. Each of these is the same\n",
      "size as the original model, so it can create an expensive storage problem if\n",
      "you're fine-tuning for multiple tasks.\n",
      "\n",
      "### Other types of fine-tuning\n",
      "\n",
      "Let's learn a few more types of learning:\n",
      "\n",
      " **Transfer learning:** Transfer learning is about taking the model that had\n",
      "learned on general-purpose, massive datasets and training it on distinct,\n",
      "task-specific data. This dataset may include labeled examples related to that\n",
      "domain. Transfer learning is used when there is not enough data or a lack of\n",
      "time to train data; the main advantage of it is that it offers a higher\n",
      "learning rate and accuracy after training. You can take existing LLMs that are\n",
      "pre-trained on vast amounts of data, like GPT Â¾ and BERT, and customize them\n",
      "for your own use case.\n",
      "\n",
      " **Task-specific fine-tuning:** Task-specific fine-tuning is a method where\n",
      "the pre-trained model is fine-tuned on a specific task or domain using a\n",
      "dataset designed for that domain. This method requires more data and time than\n",
      "transfer learning but can result in higher performance on the specific task.\n",
      "\n",
      "For example, translation using a dataset of examples for that task.\n",
      "Interestingly, good results can be achieved with relatively few examples.\n",
      "Often, just a few hundred or thousand examples can result in good performance\n",
      "compared to the billions of pieces of text that the model saw during its pre-\n",
      "training phase. However, there is a potential downside to fine-tuning on a\n",
      "single task. The process may lead to a phenomenon called **catastrophic\n",
      "forgetting**.\n",
      "\n",
      "Catastrophic forgetting happens because the full fine-tuning process modifies\n",
      "the weights of the original LLM. While this leads to great performance on a\n",
      "single fine-tuning task, it can degrade performance on other tasks. For\n",
      "example, while fine-tuning can improve the ability of a model to perform\n",
      "certain NLP tasks like [sentiment\n",
      "analysis](https://www.superannotate.com/blog/sentiment-analysis-explained) and\n",
      "result inÂ quality completion, the model may forget how to do other tasks.\n",
      "This model knew how to carry out named entity recognition before fine-tuning\n",
      "correctly identifying.\n",
      "\n",
      " **Multi-task learning:** Multi-task fine-tuning is an extension of single-\n",
      "task fine-tuning, where the training dataset consists of example inputs and\n",
      "outputs for multiple tasks. Here, the dataset contains examples that instruct\n",
      "the model to carry out a variety of tasks, including summarization, review\n",
      "rating, code translation, and entity recognition. You train the model on this\n",
      "mixed dataset so that it can improve the performance of the model on all the\n",
      "tasks simultaneously, thus avoiding the issue of catastrophic forgetting. Over\n",
      "many epochs of training, the calculated losses across examples are used to\n",
      "update the weights of the model, resulting in a fine-tuned model that knows\n",
      "how to be good at many different tasks simultaneously. One drawback of multi-\n",
      "task fine-tuned models is that they require a lot of data. You may need as\n",
      "many as 50-100,000 examples in your training set. However, assembling this\n",
      "data can be really worthwhile and worth the effort. The resulting models are\n",
      "often very capable and suitable for use in situations where good performance\n",
      "at many tasks is desirable.\n",
      "\n",
      " **Sequential fine-tuning:** Sequential fine-tuning is about sequentially\n",
      "adapting a pre-trained model on several related tasks. After the initial\n",
      "transfer to a general domain, the LLM might be fine-tuned on a more specific\n",
      "subset. For instance, it can be fine-tuned from general language to medical\n",
      "language and then from medical language to pediatric cardiology.\n",
      "\n",
      "Note that there are other fine-tuning examples â adaptive, behavioral, and\n",
      "instruction, [reinforced fine-\n",
      "tuning](https://www.superannotate.com/blog/reinforced-fine-tuning) of large\n",
      "language models. These cover some important specific cases for training\n",
      "language models.\n",
      "\n",
      "### Retrieval augmented generation (RAG)\n",
      "\n",
      "[Retrieval augmented generation (RAG)](https://www.superannotate.com/blog/rag-\n",
      "explained) is a well-known alternative to fine-tuning and is a combination of\n",
      "natural language generation and information retrieval. RAG ensures that\n",
      "language models are grounded by external up-to-date knowledge sources/relevant\n",
      "documents and provides sources. This technique bridges the gap between\n",
      "general-purpose models' vast knowledge and the need for precise, up-to-date\n",
      "information with rich context. Thus, RAG is an essential technique for\n",
      "situations where facts can evolve over time.\n",
      "[Grok](https://www.superannotate.com/blog/grok-ai-elon-musk), the recent\n",
      "invention of xAI, uses RAG techniques to ensure its information is fresh and\n",
      "current.\n",
      "\n",
      "![retrieval augmented generation](https://assets-global.website-\n",
      "files.com/614c82ed388d53640613982e/65b7aa718561628120ca45c1_retrieval-\n",
      "augmented-generation.webp)\n",
      "\n",
      "One advantage that RAG has over fine-tuning is information management.\n",
      "Traditional fine-tuning embeds data into the model's architecture, essentially\n",
      "'hardwriting' the knowledge, which prevents easy modification. On the other\n",
      "hand, RAG permits continuous updates in training data and allows\n",
      "removal/revision of data, ensuring the model remains current and accurate.\n",
      "\n",
      "In the context of language models, RAG and fine-tuning are often perceived as\n",
      "competing methods. However, their combined use can lead to significantly\n",
      "enhanced performance. Particularly, [fine-tuning can be applied to RAG\n",
      "systems](https://www.superannotate.com/blog/rag-fine-tuning) to identify and\n",
      "improve their weaker components, helping them excel at specific LLM tasks.\n",
      "\n",
      "## Fine-tuning in SuperAnnotate\n",
      "\n",
      "Choosing the right tool means ensuring your AI understands exactly what you\n",
      "need, which can save you time, money, and protect your reputation. Look at the\n",
      "[Air Canada situation](https://www.theguardian.com/world/2024/feb/16/air-\n",
      "canada-chatbot-lawsuit), for example. Their AI chatbot\n",
      "[hallucinated](https://www.superannotate.com/blog/ai-hallucinations) and gave\n",
      "a customer incorrect information, misleading him into buying full-price\n",
      "ticket. While we can't pin it down to fine-tuning for sure, it's likely that\n",
      "better fine-tuning might have avoided the problem. This just shows how crucial\n",
      "it is to pick a fine-tuning tool that ensures your AI works just right. It's\n",
      "precisely situations like these where SuperAnnotate steps in to make a\n",
      "difference.\n",
      "\n",
      "SuperAnnotate's LLM tool provides a cutting-edge approach to designing optimal\n",
      "training data for fine-tuning language models. Through its highly customizable\n",
      "LLM editor, users are given a comprehensive platform to create a broad\n",
      "spectrum of LLM use cases tailored to specific business needs. As a result,\n",
      "customers can ensure that their training data is not only high-quality but\n",
      "also directly aligned with the requirements of their projects.\n",
      "\n",
      "Here's what you need to know about SuperAnnotate's [LLM fine-tuning\n",
      "tool](https://www.superannotate.com/llms):\n",
      "\n",
      "  * Its fully customizable interface allows you to gather data for your specific use case efficiently. Even if it's unique.\n",
      "  * We work with a world-class team of experts and people management, which makes it a breeze to scale to hundreds or thousands of people.\n",
      "  * The analytics and insights of our platform are invaluable gems for our customers. It allows a better understanding of the data and enforces quality standards.\n",
      "  * API integrations make it easy to set up a model in the loop, AI feedback and much more.\n",
      "\n",
      "The tool has practical applications in various areas.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[39m=\u001b[39m cleaner\u001b[39m.\u001b[39;49mrefine_html_files(docs[:\u001b[39m1\u001b[39;49m])\n",
      "Cell \u001b[0;32mIn[17], line 37\u001b[0m, in \u001b[0;36mDocumentRefiner.refine_html_files\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mprint\u001b[39m(seg\u001b[39m.\u001b[39mtext)\n\u001b[1;32m     35\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchain\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: seg\u001b[39m.\u001b[39;49mtext})\n\u001b[1;32m     38\u001b[0m     cleaned_doc_segments \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m response \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   2500\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   2501\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2502\u001b[0m             patch_config(\n\u001b[1;32m   2503\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   2504\u001b[0m             ),\n\u001b[1;32m   2505\u001b[0m         )\n\u001b[1;32m   2506\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:158\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     \u001b[39minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    154\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[1;32m    155\u001b[0m     config \u001b[39m=\u001b[39m ensure_config(config)\n\u001b[1;32m    156\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\n\u001b[1;32m    157\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 158\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    159\u001b[0m             [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[1;32m    160\u001b[0m             stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    161\u001b[0m             callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    162\u001b[0m             tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    163\u001b[0m             metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    164\u001b[0m             run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    165\u001b[0m             run_id\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mrun_id\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    166\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    167\u001b[0m         )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[1;32m    168\u001b[0m     )\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:560\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    554\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    558\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    559\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 560\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:421\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    420\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e, response\u001b[39m=\u001b[39mLLMResult(generations\u001b[39m=\u001b[39m[]))\n\u001b[0;32m--> 421\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    422\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    423\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)  \u001b[39m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    425\u001b[0m ]\n\u001b[1;32m    426\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:411\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    409\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 411\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    412\u001b[0m                 m,\n\u001b[1;32m    413\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    414\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    415\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    416\u001b[0m             )\n\u001b[1;32m    417\u001b[0m         )\n\u001b[1;32m    418\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    419\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:632\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 632\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    633\u001b[0m             messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    634\u001b[0m         )\n\u001b[1;32m    635\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:434\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[39mif\u001b[39;00m should_stream:\n\u001b[1;32m    431\u001b[0m     stream_iter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream(\n\u001b[1;32m    432\u001b[0m         messages, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    433\u001b[0m     )\n\u001b[0;32m--> 434\u001b[0m     \u001b[39mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[1;32m    435\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    436\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[1;32m    437\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    438\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: stream} \u001b[39mif\u001b[39;00m stream \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}),\n\u001b[1;32m    439\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    440\u001b[0m }\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:67\u001b[0m, in \u001b[0;36mgenerate_from_stream\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate from a stream.\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m generation: Optional[ChatGenerationChunk] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m stream:\n\u001b[1;32m     68\u001b[0m     \u001b[39mif\u001b[39;49;00m generation \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m:\n\u001b[1;32m     69\u001b[0m         generation \u001b[39m=\u001b[39;49m chunk\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/langchain_community/chat_models/openai.py:398\u001b[0m, in \u001b[0;36mChatOpenAI._stream\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m}\n\u001b[1;32m    397\u001b[0m default_chunk_class \u001b[39m=\u001b[39m AIMessageChunk\n\u001b[0;32m--> 398\u001b[0m \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(\n\u001b[1;32m    399\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    400\u001b[0m ):\n\u001b[1;32m    401\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(chunk, \u001b[39mdict\u001b[39;49m):\n\u001b[1;32m    402\u001b[0m         chunk \u001b[39m=\u001b[39;49m chunk\u001b[39m.\u001b[39;49mdict()\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/openai/_streaming.py:46\u001b[0m, in \u001b[0;36mStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[_T]:\n\u001b[0;32m---> 46\u001b[0m     \u001b[39mfor\u001b[39;49;00m item \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator:\n\u001b[1;32m     47\u001b[0m         \u001b[39myield\u001b[39;49;00m item\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/openai/_streaming.py:58\u001b[0m, in \u001b[0;36mStream.__stream__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m process_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39m_process_response_data\n\u001b[1;32m     56\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_events()\n\u001b[0;32m---> 58\u001b[0m \u001b[39mfor\u001b[39;49;00m sse \u001b[39min\u001b[39;49;00m iterator:\n\u001b[1;32m     59\u001b[0m     \u001b[39mif\u001b[39;49;00m sse\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mstartswith(\u001b[39m\"\u001b[39;49m\u001b[39m[DONE]\u001b[39;49m\u001b[39m\"\u001b[39;49m):\n\u001b[1;32m     60\u001b[0m         \u001b[39mbreak\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/openai/_streaming.py:50\u001b[0m, in \u001b[0;36mStream._iter_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_iter_events\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[ServerSentEvent]:\n\u001b[0;32m---> 50\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoder\u001b[39m.\u001b[39miter_bytes(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39miter_bytes())\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/openai/_streaming.py:280\u001b[0m, in \u001b[0;36mSSEDecoder.iter_bytes\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39miter_bytes\u001b[39m(\u001b[39mself\u001b[39m, iterator: Iterator[\u001b[39mbytes\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[ServerSentEvent]:\n\u001b[1;32m    279\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Given an iterator that yields raw binary data, iterate over it & yield every event encountered\"\"\"\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m     \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iter_chunks(iterator):\n\u001b[1;32m    281\u001b[0m         \u001b[39m# Split before decoding so splitlines() only uses \\r and \\n\u001b[39;49;00m\n\u001b[1;32m    282\u001b[0m         \u001b[39mfor\u001b[39;49;00m raw_line \u001b[39min\u001b[39;49;00m chunk\u001b[39m.\u001b[39;49msplitlines():\n\u001b[1;32m    283\u001b[0m             line \u001b[39m=\u001b[39;49m raw_line\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/openai/_streaming.py:291\u001b[0m, in \u001b[0;36mSSEDecoder._iter_chunks\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Given an iterator that yields raw binary data, iterate over it and yield individual SSE chunks\"\"\"\u001b[39;00m\n\u001b[1;32m    290\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 291\u001b[0m \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m iterator:\n\u001b[1;32m    292\u001b[0m     \u001b[39mfor\u001b[39;49;00m line \u001b[39min\u001b[39;49;00m chunk\u001b[39m.\u001b[39;49msplitlines(keepends\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m):\n\u001b[1;32m    293\u001b[0m         data \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m line\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/httpx/_models.py:829\u001b[0m, in \u001b[0;36mResponse.iter_bytes\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    827\u001b[0m chunker \u001b[39m=\u001b[39m ByteChunker(chunk_size\u001b[39m=\u001b[39mchunk_size)\n\u001b[1;32m    828\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request):\n\u001b[0;32m--> 829\u001b[0m     \u001b[39mfor\u001b[39;49;00m raw_bytes \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter_raw():\n\u001b[1;32m    830\u001b[0m         decoded \u001b[39m=\u001b[39;49m decoder\u001b[39m.\u001b[39;49mdecode(raw_bytes)\n\u001b[1;32m    831\u001b[0m         \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m chunker\u001b[39m.\u001b[39;49mdecode(decoded):\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/httpx/_models.py:883\u001b[0m, in \u001b[0;36mResponse.iter_raw\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    880\u001b[0m chunker \u001b[39m=\u001b[39m ByteChunker(chunk_size\u001b[39m=\u001b[39mchunk_size)\n\u001b[1;32m    882\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request):\n\u001b[0;32m--> 883\u001b[0m     \u001b[39mfor\u001b[39;49;00m raw_stream_bytes \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream:\n\u001b[1;32m    884\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_bytes_downloaded \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39mlen\u001b[39;49m(raw_stream_bytes)\n\u001b[1;32m    885\u001b[0m         \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m chunker\u001b[39m.\u001b[39;49mdecode(raw_stream_bytes):\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/httpx/_client.py:126\u001b[0m, in \u001b[0;36mBoundSyncStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m typing\u001b[39m.\u001b[39mIterator[\u001b[39mbytes\u001b[39m]:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream:\n\u001b[1;32m    127\u001b[0m         \u001b[39myield\u001b[39;49;00m chunk\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/httpx/_transports/default.py:113\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m typing\u001b[39m.\u001b[39mIterator[\u001b[39mbytes\u001b[39m]:\n\u001b[1;32m    112\u001b[0m     \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 113\u001b[0m         \u001b[39mfor\u001b[39;49;00m part \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_httpcore_stream:\n\u001b[1;32m    114\u001b[0m             \u001b[39myield\u001b[39;49;00m part\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:367\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    366\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m exc \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:363\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39mbytes\u001b[39m]:\n\u001b[1;32m    362\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         \u001b[39mfor\u001b[39;49;00m part \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream:\n\u001b[1;32m    364\u001b[0m             \u001b[39myield\u001b[39;49;00m part\n\u001b[1;32m    365\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/httpcore/_sync/http11.py:349\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[39mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    348\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 349\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/httpcore/_sync/http11.py:341\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    340\u001b[0m     \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mreceive_response_body\u001b[39m\u001b[39m\"\u001b[39m, logger, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request, kwargs):\n\u001b[0;32m--> 341\u001b[0m         \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection\u001b[39m.\u001b[39;49m_receive_response_body(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs):\n\u001b[1;32m    342\u001b[0m             \u001b[39myield\u001b[39;49;00m chunk\n\u001b[1;32m    343\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    344\u001b[0m     \u001b[39m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     \u001b[39m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[1;32m    346\u001b[0m     \u001b[39m# before raising that exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/httpcore/_sync/http11.py:210\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_body\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    207\u001b[0m timeout \u001b[39m=\u001b[39m timeouts\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    209\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_event(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    211\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(event, h11\u001b[39m.\u001b[39mData):\n\u001b[1;32m    212\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mbytes\u001b[39m(event\u001b[39m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[39mif\u001b[39;00m event \u001b[39mis\u001b[39;00m h11\u001b[39m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_network_stream\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    225\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mREAD_NUM_BYTES, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    228\u001b[0m     \u001b[39m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[39m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[39m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[39m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mtheir_state \u001b[39m==\u001b[39m h11\u001b[39m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv(max_bytes)\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/ssl.py:1295\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1292\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1293\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1294\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(buflen)\n\u001b[1;32m   1296\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.11/ssl.py:1168\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m, buffer)\n\u001b[1;32m   1167\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1168\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m)\n\u001b[1;32m   1169\u001b[0m \u001b[39mexcept\u001b[39;00m SSLError \u001b[39mas\u001b[39;00m x:\n\u001b[1;32m   1170\u001b[0m     \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m SSL_ERROR_EOF \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

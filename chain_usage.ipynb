{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danliu/miniconda3/envs/chatbot/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from api.chains.off_topic_chain import create_off_topic_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature=0, model_name = \"gpt-3.5-turbo\", streaming = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Fine-tuning is the process of taking a pre-trained model and further training it on a specific dataset or task to improve its performance on that particular task. In the context of AutoTrain documentation from Hugging Face, fine-tuning large language models (LLMs) involves adjusting the model's parameters based on new data to enhance its performance on tasks like causal language modeling (CLM) and masked language modeling (MLM).\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from api.chains.qa_chain_with_rag import create_llm_finetun_chain\n",
    "llm_fine_tune_chain = create_llm_finetun_chain(llm)\n",
    "llm_fine_tune_chain.invoke({\"question\": \"what is finetuning\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Other'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from api.chains.classification_chain import create_classification_chain\n",
    "classification_chain = create_classification_chain(llm)\n",
    "classification_chain.invoke({\"question\": \"color of sky\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLM finetuning refers to the process of fine-tuning large language models (LLMs) on your own data using AutoTrain. It supports different types of LLM finetuning, such as Causal Language Modeling (CLM) and Masked Language Modeling (MLM). The data for LLM finetuning should be in CSV format, and there are specific data formats required for different types of trainers within AutoTrain, such as SFT/Generic Trainer, Reward Trainer, and DPO Trainer. The parameters for running AutoTrain LLM include options for training, deploying, and running inference, as well as specifying the model, project name, data path, batch size, epochs, learning rate, optimizer, scheduler, and other training configurations.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from api.full_chain import init_full_chain\n",
    "full_chain = init_full_chain(llm)\n",
    "full_chain.invoke({\"question\": \"what is the llm finetuning\", \"memory\": \"eee\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"docstore/metadata": {"https://www.superannotate.com/blog/llm-fine-tuning": {"doc_hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134"}, "https://huggingface.co/docs/autotrain/en/llm_finetuning": {"doc_hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5"}, "7efdbe16-bdab-48e3-8a39-40e3ea7d9056": {"doc_hash": "45175820ed54f636dde5bd2d21ad59ecfdb847577056abbbfb9141da21a4ff4b"}, "cb839fa2-4af5-4d52-a855-b900a7fcfa4a": {"doc_hash": "d70974e38b21b88e7e009ef43596ca7e74c72543f0cbed59d39569282260caba"}, "bbaa93e8-f909-4e1c-ac49-bf09a93c7129": {"doc_hash": "72dc1137e7ddc6f01fc32ae6a147c779116266a42e198625f39dfb0372fdfd82"}, "8f47bd05-4993-421c-a4de-1d820c1ecea4": {"doc_hash": "912952e8dcf723d716d4fce59f8fa980aa1e52002151621c8ab589d21bbd5c11"}, "504434b7-d7ac-44ee-b82f-d66314dc879f": {"doc_hash": "69eba67d3fcec2cb381aff2d47aea3fcf7dacb148b6e2878bbc4a68b27aaea75"}, "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5": {"doc_hash": "4d4bd45d6c96facb0b607dfad2fd0a2f058b3356811b4734a5825ac1bceb8225"}, "e4937f0c-b264-411d-8d2e-dbca9b578b6e": {"doc_hash": "4b364895e6d406da7a27def36e3c3eb08ef5616f522e8a807332c8d1b0e9ad8d"}, "379c133f-b178-41b8-b255-09fb40527f5b": {"doc_hash": "e22983e0a515690fe60d55f5b048f5e6039f18641bb488a10a660dd85d8dcb3c"}, "85ccb18b-1e90-4eb2-961c-fba10142d3b9": {"doc_hash": "4832a0d177dd6918766af58eea501f73c529330643672681eb9b815871fc123f"}, "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e": {"doc_hash": "12aeaf684445dcd0966210d0ca8c77e24dc614c54f152c228187b9c09b4be7f2"}, "b46805fa-d37e-4c10-b49b-378a48c3c60f": {"doc_hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c"}, "fd01a0da-e11d-4095-b518-a3aebffc89eb": {"doc_hash": "73d9fa8bf5100471ac4eabaebbb21f397c030a5c1df11fcf79ea5e796617ebe8"}, "85737b80-6c4a-445e-b173-8c9277e49204": {"doc_hash": "d9db6be5a9fd5547c873968f57fe09d7b8c22f23c70f384c38d503c102be7349"}, "413315cd-b11e-4878-a2b9-e653548db0f6": {"doc_hash": "e0e069e9ca3b22ce573393121a17287569ea2db6bf5d45a98bf4fcc268494792"}, "a9256579-366e-4b1d-821b-7e410b884184": {"doc_hash": "71badfa87b1f3c3c6d030d4c81c9923d18ec9dd7de00c195d6d2c580deaf4ef9"}, "f13e8ae9-34c4-472f-9fd5-2bd7c44a33f8": {"doc_hash": "6dd31c1c995cf86349c7687f43981212c5ee766b5d67518a18b5cdf72485a1c6"}, "d29ca23e-6ef3-4619-8c0c-64dc12de09ea": {"doc_hash": "c07a6846437159e61da96c2fa769a41ac2ae58b6b0d32f724041f03a2da043a8"}, "831b1971-2e28-49f8-814b-0a40a7f215fa": {"doc_hash": "4963fc5809ddfe8a173611fdd30ab87b4fdc33ecef856d6e0bfef54db617ff6b"}, "36c1daaa-43c5-42e8-b0ba-bbe215b2e595": {"doc_hash": "a805057fee31841cba092e63eb61b704af42513c8e2bf73b6bc44fc6d5e15712"}, "da08e13e-b1f1-495a-aa53-69625e6f7ef2": {"doc_hash": "66c31170505404d19d1ae297f62cb6988b8b2d2255225bed758c5888c684cdac"}, "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3": {"doc_hash": "58014fba4b5e7e50ea702c9c1297186a1940f527c691085df70e28ef425d64d1"}, "2d6d4703-f15a-41d4-9b36-b455938824f6": {"doc_hash": "74fed6190a90bf72e874bd898bc96859f048532fcff22effd34a399d3095d6c2"}, "82569dcb-23d6-4577-baed-6323c76765ea": {"doc_hash": "017169e57c627ecb68cc28d6022ea21305b3e50cb4af772a64397ee25d225a33"}, "18ff9d18-48df-4362-bc4b-738fa5157cb1": {"doc_hash": "c4879cdc148df642fb2e8850b7972c139e6a3050e57ea297b07820f2ccfd7999"}, "cc729e7a-34e8-45f6-abd5-d3b10024dcad": {"doc_hash": "6f4102705c6161304466e1b9c91538e5af3a661097f9e5f47082186b09e98061"}, "cd6a3964-9ede-4b76-8b13-3d149bf79ae2": {"doc_hash": "528ba9d7376ff4c0cbb18f8bf530e07a651ea68cfb6314498447d0b6789204b3"}, "52fb437e-fae3-4323-9973-bf7c1dd46d9e": {"doc_hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e"}, "580ee1e9-d087-44e6-bbdf-ae04f60f8643": {"doc_hash": "d693abb8777af6cf42a9663b9aa339505146e1acad10011ffcd728c8dcc0541d"}, "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc": {"doc_hash": "2336043e2c8a1a0ddb2ab140426f27b6fe6caf514e27af1a9c68a151729ec4cd"}, "a26995a6-144e-4b45-aafd-f1ffbdd56f4e": {"doc_hash": "ec703d73b100cd17aadfacd29e051feca3d7650572877bf414eaae2a01e44ff5"}, "2ee4cf9f-afe0-41ab-b5d2-70d8df27e172": {"doc_hash": "a19ff2c5ac780aaec5e8185aab48cd64af19c50690f9b31bd7d0ab733fb27416"}, "c8b90a72-370e-4b48-9eec-86788c3a9950": {"doc_hash": "1c68029a7036f48fb893967cccc62842799cee09ef9424e100c525d7df52dd29"}, "628dbad3-bd1a-4245-92c3-3c7d359e0257": {"doc_hash": "77da058f1715f5d3ca97732b9153af04e7c0f099642b0c2dd378105aba8017b1"}, "7fda0c6f-7416-4350-b866-6f69b4ed5c49": {"doc_hash": "3938bedc029eb416bbb86079d7e27c236d80b43251ed6e1bd1c85db5adeca2e5"}, "f94d1948-ee6c-464e-b5bb-c14f88e3ca0c": {"doc_hash": "8bb1d0cd499db38799f407140a913d86d09149e710883ba27c2e875e73cb1a11", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "f06dacdd-bb77-4a42-896a-75a3736b2b39": {"doc_hash": "53c68ed8b745181350ba5b57e409a8dd8fc12ffe10e92459f77649d8e995cc3c", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "d4382cf6-99dd-4b2b-b332-212af5327c13": {"doc_hash": "7fd59e624730f40b630443527958d6f1551bfd01c7d0c174fc0831718af24078", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "741f7e47-9921-4edd-9cd0-7ff58e7f9c67": {"doc_hash": "55dd7a559a869741a954593ca973e00a57d53dd3a95006d0ea8bd31ec7db88ae", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "23dcb314-9ad3-4007-8243-af7bf1df0e9a": {"doc_hash": "8725a256b4888e2480612ffbdb7ccac7e2bf5f4e958c7d24d7b6477784c5ca7c", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "0dd1f595-0a51-49a1-99fb-63ea3d1ae3b7": {"doc_hash": "062fa8642d089009877fc9c3d1afbbb230cf92e0e2f8e5a1d63e9a18b61396c1", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "54373d6a-6875-464b-8364-d3581dfe5519": {"doc_hash": "1e824cdd675cb7b105e27243aaaecc2af7ee8f00ce23854d769e89f4c3099d67", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "384b183a-eb02-4243-83a8-330f6a6eadc2": {"doc_hash": "bc416026870d660aea44038a5a6dd49c23619fce2c3113d276b87f755afd5ca8", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "0c92f851-0657-4251-9eaa-e7646d2ce529": {"doc_hash": "b7ee5970f781aebfe7db91eddee55042751763a66658f53ac4d161f0cd33b532", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "ee171899-adb4-4797-8477-328097365f8a": {"doc_hash": "e6dcf382566177dfab1de7d4b5c9c52bea1b282308965ffb7754170dcf84180d", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "b024f6ac-6450-4c36-9e43-1f3bf589c73f": {"doc_hash": "254ce87b549c7a8574eec7c90184dfbdaeb27a162ee7e6a3370f91ce44cec33d", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "23cd2a1a-9902-4670-ab32-2d5be4a62ee2": {"doc_hash": "b5bacdf4b43f1d5089a30b5507d6d1ad7e390a4fb717d8f775e0fece57ee19a0", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "9b4f227a-b568-4023-a30d-db8de1128926": {"doc_hash": "0ccbea578918760783fbe0468c3b125e255a2645d16fc66c88441b70a81c357c", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "06903398-8cd2-4835-a060-aecab6577375": {"doc_hash": "67ec3b5ef6e9837aad268bdb7245b4c466df3ebfd34719a3eeac54bf74c01e98", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "45895980-0f75-448b-9d50-8d3e93145a68": {"doc_hash": "4522d045a49f4161fe91bfd6c3618ecec60b9e8b7ee891e556cd07cbdae7f24d", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "de7ec9b3-91c3-4123-aec6-4bb68b397612": {"doc_hash": "6c08f89b7968d05ef433f8b1b9da314b580744975db53773cfa0c0816ca6990a", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "876b4dd4-25ec-476b-b32e-abad8f02fb33": {"doc_hash": "24038efd1087f57bd671df80d91e637dc5a171d42b494058044efd48d2330916", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "3e9cdc58-b77a-4b8c-8e83-f7a63cbf56b8": {"doc_hash": "556887598123fd706336a0f3e712494f6008b7feb4613ce35a9d3744a41a61fe", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "1ab2448a-3142-4d3d-abac-ffc5a31e9448": {"doc_hash": "8cedf186b7a075d459cc2e2ed43379e22c5dbe0065e727765d2283f69fcfe7bd", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "64af5183-d123-45cb-81d2-d70b27da9ee5": {"doc_hash": "b4e1e4a1a40bcf4c31eaa0dd5a58de7303c29f5f298abcf9155b6c77f51bf097", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "62fe50b0-c220-4d5a-98a3-b7e0dad4e09b": {"doc_hash": "9a8b657e6b6486d564d10d7e4a417d055f5d0af1f407be72ec4135037029ce72", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "100555c8-f5f0-4d37-8fc2-4ad818e00ba0": {"doc_hash": "ef5424d6b5d04bb11ef8b7f24b2d42f3fc7ce3fa616d009dab2839aecfee50aa", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "810a954e-8b1f-4f6b-a84d-e80e88a3188a": {"doc_hash": "41e35d778ea4c4d9c2092268a6f6e725a3913b62c6775345f87731309579d091", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "8ade6fdf-a752-45b9-8550-13a82a7812d1": {"doc_hash": "489f428b516eeb5a7701f0c5e76168bf80a9823d574bc6b9d42e1ab7a70eab03", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "5c0a244d-acc3-4094-910e-5988ec67274b": {"doc_hash": "9b84e3acc0e689d6d1903d54c095a1a9669ba2feb78de1632c9ed26b4dcf004b", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "b8e7bd3a-eb79-470b-9c7f-9f18924f7180": {"doc_hash": "ff740548fabfde9eef0e213290b64542b350f23cdc5bdc01652d5f9803170c8e", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "72bfe78f-4633-41de-85db-72a526402837": {"doc_hash": "d5ef95a586d1e4db0843d6f05c6953d202d9e89d6480c06891ec1d42f0f7c37d", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "87ddb007-8915-412d-95c1-dd312c4f6945": {"doc_hash": "06a362b634d27fdc4de1837c5e78f47505a665b5e05d07c23315ec9028277c66", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "30c1fe9b-ad2d-4d93-8eae-d7a3c64c44b0": {"doc_hash": "b9df16be74324a10dfe55ce8d705675594062309fbe601558330c688e82556fe", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "718cc305-9a79-47d7-b02b-d02771a7187f": {"doc_hash": "8ccc21ae7f28f191151b83b505058a761b78c01dd2275fe78f0bc6d4ecf74b47", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "8399cdd6-cbd5-4218-83d9-40d3473ac522": {"doc_hash": "44a660d10d2ab565ef8e7e5d3ec1751953ada72bd51b9b92d7cb2d8cd2bddd82", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "a4751216-0a29-400f-9a72-4cdf18a9c02a": {"doc_hash": "9811254dcf9f6be4a1c2acacb9fda0bb4a2ea3f3ee00fa5bab381e38e3461193", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "ff16ff62-ab9b-4e68-8bed-5ac504cf92e8": {"doc_hash": "e4c8a8806927caa031d6120887d9ea30c227aee7cb6d9110baa3163898f68f62", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "d34612ce-ab70-4e93-a1b4-e67e1df12df4": {"doc_hash": "f4367c3aca49209f9d68efc20b1b1ac923e5f468cff92e0998a8c336e8d112bc", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "30d290b0-8efc-473d-aa86-2281ee58825d": {"doc_hash": "a3753e4e8093a77035949cb7d541b754e3d301dd3ec1945c6a081579418cd853", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "e89e5570-8955-4c3d-966a-0248986cd9d3": {"doc_hash": "a54177783f73691d68b681534d24e7c94530e5d0cb4e5b8d188f9618bae86d4a", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "842e2e3f-7318-4b95-b722-bf7051d9da91": {"doc_hash": "fe28831e15f229b6956985b21254d0504c5684ccdee3db783fdd33a53a9eb994", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "bb243c37-4061-43f1-9ce5-9577d4946318": {"doc_hash": "4c06679462790015939f90015fadad6169c3841b55b748b29b19853f0bfa2961", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "455ce9a8-9201-41cb-9b0d-3254dbae2895": {"doc_hash": "20d9ed58917eefc8048099babb43648502e028c5a500216fdd0a3ab3db3e2166", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "2f914ba8-86fe-4391-8260-71cff2a54df7": {"doc_hash": "cb203cf9e858d47be0c25b0a2a1d1966e4c894cb0243f31fe82d93645b724cbd", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "8a36ca3a-34f2-45b1-9def-e832893dc387": {"doc_hash": "04618021b18670ef96598157cf816b453b80a0a7042fe308018d66299ef89cb7", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "83fb0a1a-4a22-48f1-940e-4213ad6a4ab5": {"doc_hash": "0c2913e878ba4fa510cc0fa761fa968e9134acb0b01df8f8f0791db5c2220349", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "3523fb4e-129f-49f4-a394-9daa7e810301": {"doc_hash": "591f3ce2871fceb81b9cea5925649ce6530ff1e73d76302b3d2de02ee1091078", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "f6148fe8-35b4-4408-b743-3a1abccc07a5": {"doc_hash": "d3148164ec187515355d59d59281b2284508aef2646cb4fc4fd093caf4597381", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "01dc11d3-78ae-4e07-93c4-09ea9af5801d": {"doc_hash": "17fc52655853ecf55f497878b41cf7dfa1cb543266a3d821faa40ba175b4ce42", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "1f49e87f-4462-460e-94cd-4c73e0f81a38": {"doc_hash": "daaa0b89229b0fef45420f5b64a852b445c6f6a8ce8faee2eb6c2108094f2e75", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "65cc9163-2a7d-43b3-9c34-a5e1d12b10be": {"doc_hash": "c287e3e34deaeb2439b9be54970feafa7f6348d24399d35f03f057547af2cc81", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "cbf38aa9-72c3-43e3-861b-62fb00774098": {"doc_hash": "3f078e59f7539467b60ef3b254392a271c2d78e26a4fdb05c04713c70b874974", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "cbb64c94-8642-456b-a6f0-8c7dfe44e6fc": {"doc_hash": "c522ad865a17c127d857afbc1ad66451722ac627faadab18a916d21253f66d38", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "49bff0a3-ce68-4285-8d73-6e6fc893f5b9": {"doc_hash": "f0673c29c742361b10ee9477a4ab6e2e5849bfbce72aa49e021e235b91c24abf", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "0dfd3540-93b3-4a0e-a548-0cf68d4623f1": {"doc_hash": "4019051414692c35f4783fedef0e8f56183b7d9c9aaf5d6780ba38e5f9ad6334", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "32b9570b-acf7-4fca-bd57-681e32fdd8b1": {"doc_hash": "d622515d0595d5756418f4635a82709d7ffe93611b436a1ed51fdc9431c53568", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "fd7fb78e-9768-4d1b-84fd-40a0caef149f": {"doc_hash": "472d3bfb0cc117b3c40094b1ea6fa9e9060c734bba2ba6cc4da1fb3d5d4b1425", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "eace7e82-9a0f-48b3-baee-bee0b7657b63": {"doc_hash": "f548184a29f46a93c7800c438da09bb3f83afa3a70278afeb2c58cdb26bbb8a7", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "c79289c8-e668-4e12-9c36-6cf92b14360e": {"doc_hash": "c80a5e1764c875329fb211284e93e731ee428c450e65c5202c14cb3c1a77db1c", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "cc3d174a-81c7-4960-ba16-e1b6c46e2834": {"doc_hash": "fdf0b9a239a9f1cc80496030b07178b12ed85f828e677378a737fdb43d326ef9", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "23a0eef4-12af-4b6c-b0ab-15d963aefa6d": {"doc_hash": "1d26691d143e94152996d564f37a51126f5d9db144c48f77f5239108dd2c2d95", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "9619a267-a0f1-44d7-b79b-6a56d3e8039f": {"doc_hash": "d611f8473f3eca6df679c7bed065b04f29896a2d9f55a384013ed0f6763fcf6d", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "f448b4c5-38ee-45eb-a835-d718198d070e": {"doc_hash": "ad5141c36b0e44e2414a9f190b8d52ba3ded2252286ae158a84983fdcbc70ff8", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "bab3e682-c0e4-4177-a8b1-59831676d909": {"doc_hash": "4ab1e5a52675964a4790e8b1e9c3e8d41a530a85e8e46e78b13e1bd14ca0b139", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "63b1dec9-b8bb-4e9f-82fb-0735584a1c22": {"doc_hash": "f3aef25587b07332188bc9da699946df8a1dd4cbe56aabcfc822bb6cc377e0eb", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "e4a089cc-2bd8-4db5-8ec9-41ed5bc5102f": {"doc_hash": "f3a8af47a5e29990501a4766d9fb401461ce5fe9064ceabcb7f594e7f5f61d3c", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "26463b83-57c4-4244-bb49-4e1c5b342160": {"doc_hash": "a9b0970eea95d5161d4b0ef3f5f207059b408572b3265267c3eaca2cecfc1a8c", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "64be0457-cfaf-4b19-8165-4c5aa714e33d": {"doc_hash": "c22434c9a46de71fe64d59083a36a94d6d9d419a84fec14c0106a1b228e233ca", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "ac756579-76b6-4183-8b28-8f3a22d67544": {"doc_hash": "d5ee6d890ab80e1d4542c621a53ead7c9b3440ef0ad7ebd8b2b8e862eef1cdca", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "e1b1ad77-54f3-4cdb-bb79-29f2674028a1": {"doc_hash": "a3c0b6c9900cdbe5b039239a78ba93705aae913f9a591b2c113dfc0b767fe4aa", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "dcb42538-6220-4772-b0a7-d8bea30d2331": {"doc_hash": "7ae9a3d165c4c593ed6582cbc3a2a2d4bb6f22b2886feff5d8bcf2fb4f9c828f", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "e199603d-31e1-4dfb-98a7-84b7373689a6": {"doc_hash": "daee68295119da5473483470057e050fc2ec6e05dc57a9506e8d6321cb46c938", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "3784e315-0244-45bb-b458-37f1036b454b": {"doc_hash": "843f065b76304d8aa716683e49086b41a4eb39878d0b2a0eab425d3778a1a4cb", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "78d83296-f12c-4cce-93cb-97936f1faa5a": {"doc_hash": "13b0ff422f646c71597d7929b15752b940e779a14ea2eba16b404d4bdff6fa55", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "96e60dc1-83df-4021-9df4-e5974c186b56": {"doc_hash": "4426e1f277505f8bf4fce395db9f1e1be29a628f24b1a7693fb1dc414a62463b", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "9a7b3c81-f443-4de8-9ede-60e868b977e5": {"doc_hash": "bbbd0500450b693aaa5425c0ea3cf1b539452386a09304c6bdf1fc08776c87b2", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "775c306d-46b4-4877-aecd-a944a2eff900": {"doc_hash": "122c33155ee542f8b5618e7b37fad296767b20ae4d24713937595c88f724385b", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "73f9c664-c7ee-4725-af3b-27a16029d5f6": {"doc_hash": "d578126ed60756983aa643e97126b3d0e55620f2265474b3d6e20ab3755721b3", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "83557f20-1819-463b-9a03-036c1277df77": {"doc_hash": "7d830104974782b5d93d519d860f943692aded2e8a78c129830c8d685634c18b", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "5e58be94-725b-4e4b-8088-c259fdee4810": {"doc_hash": "f5dd48043dafad12aa6e8a4ef22237b679b22d79034b9529f16f15c9dbc8b1a9", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "68786a78-46a3-4807-b246-f1ba1bb9e37c": {"doc_hash": "48a617718614781f160adc1caf46205b7f862250a296fec9c48b5a60603ab13f", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "e688571c-0cff-4639-883a-d66cce8c8fc2": {"doc_hash": "94319ccde04864b36a8afc49840e2e26460978112609b02c9eaea9271dc18257", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "0571f844-a333-4b0a-b40c-cdb70a2251e3": {"doc_hash": "91980170e5f6705621e51e2052de9332e0b46032c2e8ac3e3a8cbd4149b66593", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "076ebf5d-ab83-4f01-973c-096553b1910c": {"doc_hash": "089468ad0b38849aa655590775b18b776498c954cc261567c655d5b9ade06862", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "38fb71f6-a496-404e-8d99-ea7b3e83567f": {"doc_hash": "7fd2f21de4ac9fa489f0d738ae4b89d5c321ff3c1e5f9885526fceb9b88997ce", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "da11d020-a4fc-4431-999a-d9cb47657f5b": {"doc_hash": "6433718cf8f267fa7c3d6a2d7924904f6dc417dd4be8e39ba87854ee02ab9035", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "c7484247-c86b-4189-b48b-0c08f424bda8": {"doc_hash": "3635aa8bc74940b28265dd1c635e94cca6cb0f4d9988d0e33d118f828477f0b0", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "a8765501-bcef-48d0-a826-7b0f794f9fe9": {"doc_hash": "e1947f75a739df81db7a27902279870d621df08c85445c5d40fff89f063c9f63", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "1a44c7d7-ffca-42c0-8e1a-a01a1e2f2c53": {"doc_hash": "1192c51892127b0700ba792070b117a3892e8e9d5289837ba9cccfab7122973b", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "b8f3cd1e-3210-40fb-8174-b3eebf9a59f1": {"doc_hash": "4f1b11c9568eed82d678ac9eafad3e4b6212eb0d3cc920632d76d9c03f52e1f1", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "ad48d0a7-0f21-43f5-b9ee-5c1d670f3c22": {"doc_hash": "a4565b4fb5ad43a57a869598bb4b17cf59f389b6c6a285b0f86e76f7f455e0fb", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "2d457b2f-f3d2-40a3-9517-e4de22d2bc82": {"doc_hash": "e93fae687da4bf128dfa3d111d5ccb8f6db8be53cb9a0fb2811aa72c89c77dad", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "94fed250-f519-4536-8c07-c90093e78ebb": {"doc_hash": "a4c879fe375f35809278ba6090cd24fd6d8610a7b148893346b1ad51d37e6775", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "f0c745d4-3a7f-4f0c-8063-7b52899f9f27": {"doc_hash": "486ec1fd2048e3379231d46f19ccd98cf245e644cae0d9623e79c75a13f299d9", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "73a292f3-edcf-47bd-9ac9-d1bad5c4ab20": {"doc_hash": "9182cb9637762f882e560677a01f9704ced23da1313d7805b3e936b970b63f40", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "5efb0620-9b41-4bb1-b0dc-7903c0ff9115": {"doc_hash": "72065276b86d469b964a4edf3ee5f1dcc4f2632adf7904e42d477cde3d8145d7", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "4b904198-04e8-4195-9c1c-cb5566dc8699": {"doc_hash": "0b9dc3076c08a033431ec8951218c426b3b8cef8ed03630d53ba69d518b66377", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "3a45e6f5-9483-4208-ac22-dfd1c0fad688": {"doc_hash": "47ae96eab59df9f91cb064d132976acb57d8a5473f980edaedd51cba32446626", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "862f6663-9f5f-4159-8f7b-44be541b460e": {"doc_hash": "67978ca69933e4d3668fd27f1e7f2c29821bf61cbcb4a2f9ec0f3957c32e8766", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "f5a02177-bfe4-4a7b-81f7-f992d504db40": {"doc_hash": "8b461c068f1c0ef1d1fe87e00e7149afca4a6cb08e124351481fcdddce81acf7", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "1b51df82-f83a-4b9a-b025-1ff697ea7a72": {"doc_hash": "93d259ce53298aaad0ea103dcf50dcf100a46d23dc927f786bea06c6da933aa3", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "76fff27d-ec57-4f76-9abf-d7514a066cb2": {"doc_hash": "f2e474e22b2233b5eb7aedfbe2ff937d8a85cdd37c68f621024ed3eaaf76fc76", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "cdebcd48-0b3a-4282-877d-2c0ada2a9c7e": {"doc_hash": "5fc4b66ec3a88b9f5edb4564705dfc83dddce0439846634cf66ba3976a5ed525", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "e1c0fc9f-662f-4873-baff-6c2162912757": {"doc_hash": "f390c0dc9d6392be427469988223e6e8a73996712352443c4becbdf7299a8748", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "7a6cd7bf-241b-4562-b507-9a3b32179abe": {"doc_hash": "b4040e8fd36d02e81c225f3ce42aaa4e1a573bc99511160e9319b15ee9aac3af", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "2cf02d5a-a64a-4c5d-a5d4-b971bf3036ab": {"doc_hash": "0b34046b2e23c8bcfb31e52ae92d656f8b2a6dda2811c1497d32d5464a26d420", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "7f6e8130-49fd-48c6-bd66-624513c23beb": {"doc_hash": "5010ca03473d85a575a943a483a34fdb5af7f65a087dba3683e50d6ecc53722c", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "80989c87-f448-4c06-82ea-41e0d1dcaf50": {"doc_hash": "e2e8631a4080727d0d004d0a042b942be3587e47d323692fc723db6f4f16b8b4", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "db7ec477-380b-41d7-933c-46074411dc38": {"doc_hash": "aafcf2f552f18b7ee176724356e2048aa09bff24f3b2d224a6803a78b3523765", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "908057c1-5538-4f52-9917-7457ce407ca1": {"doc_hash": "22a149b05294f8bca56bc76029e10304f8ec64a4abf8ceaca33402d54b2f31ae", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "71ba71a4-c96f-489f-b1d5-411da70b0b41": {"doc_hash": "92a01d97a818c401e4a549d2ea635230c839e0cdd7513aea82c0d4d410397692", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "9c7e4e77-cbba-4412-8575-71d8061af191": {"doc_hash": "fdf53b4ae3fcd4592dc2b01ef91e6baf940c6b7e3abe872311086de1df940b2a", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "2a3d3d70-4e9a-42f3-a3b6-4c17bcfabb67": {"doc_hash": "35984df7446c763edc734d406bf467e819fbfc405ce3fbcb1a33da464b9fef24", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "ebf078fa-58e4-4f2b-b6da-6f5ac78b2476": {"doc_hash": "e2c61f32b33895a1067c76709c017b2a81cbdd150bd8201a79147fd0c213b894", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "f6d4c295-eb5c-4356-9226-67a8f15eb309": {"doc_hash": "8599640b72794e971cd40187659ef1c53e6273afa68e51b7e7b3021dedc1e5cb", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "0afca207-c02f-4409-b471-c00a944d363b": {"doc_hash": "e839e2148622e773ef69122a7a01a9ce64c05d279c4455304a45bc6de5a6f19d", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "a00ef065-91e7-4375-abb3-f74b161d709f": {"doc_hash": "8ee33e8e605be4959231fa6303c052bd73889ca3fcefd1b1a572f5cd790d69e5", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "d3d6a8b6-b585-4417-bbc7-d0f7a3aa2d4e": {"doc_hash": "34cb2a7e1326a11466982a3f1ce942db07384c980a469d447538bbc8a2fb8868", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "1706ca01-2bce-4b8d-9c89-6e85f6a8d497": {"doc_hash": "ebd1e75f3ec6731a9e54249b6531b2dd89e22447fcbe7de2369671ac9b936248", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "c57d1b90-550c-4e6d-b7eb-ea8523bd43dc": {"doc_hash": "3737740bf8af2733dca5ffe0789c11ef5da0c68c5b8e4efb37c0c90f707e59bc", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "04840b54-4582-4c13-b842-dac58201c2f0": {"doc_hash": "2d0d1a8b47b1583a0309b8b3ef9c461a45e5703d94081425cd4b6e5fbd461b8e", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "1461b113-2471-48f7-ade4-00c860818132": {"doc_hash": "de51ae543d222c147d8008b29093fefd17bba6a7749de626981ec20269c2e6d2", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "899863e5-7edc-457d-993c-d7c6f08707f1": {"doc_hash": "49d335795431640bfcd721f67afcba08d606c9e4d37cc74d295569410c0c8129", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "2198b04d-c2f6-4ffd-bb28-3ca717be4f67": {"doc_hash": "1479c0d7a105f857f28a45eb1645b139c876a330c9fc04dd4e400448971b5164", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "7a6adc43-b828-4ba6-b1dd-3f2ad607d173": {"doc_hash": "f06b5a2308ea1a3be244120ab34a90a1cb970f57236c28b189362bd8f65f5149", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "de652578-8a02-4abb-b8f5-08833817b266": {"doc_hash": "a164df2f8554d26191fd2f2a36b86db501c916f30df4f650aaadae7fe11fa3fe", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "b383db9b-cc81-4f4c-8ee6-85035f2aeb03": {"doc_hash": "48fc252ed0c242d717866f33701f522464738de489588ab63921483c7645ea28", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "9e4bdfad-2611-461d-b1d8-05791b197111": {"doc_hash": "fd6528dc821d820f4752064a4f0450ea844a62dc08286814af33222b7c97dffb", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "945b8fd4-cfd3-4779-bf9e-21287c9fa77d": {"doc_hash": "56d4e16a3786425211daeaf72b16f08e00adf2af00a3c4d4f0025163ed589952", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "1980432a-01e9-4bf7-84a0-6183853f8b62": {"doc_hash": "382972336e2d930ee46f7065ed639a13a5109c03998efa0e3d6275777d482821", "ref_doc_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056"}, "fae173d3-b045-4aef-99e9-e7244c40fb5f": {"doc_hash": "4bbde10289a7797e1c5ec15acab30934e63139cc055275c5e0de2dd2e9a4603a", "ref_doc_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056"}, "541eeb4d-b287-4ae5-8cdf-9f1c414d22cd": {"doc_hash": "3d850d25b8fa83ea797ea96a69c156addd1679a96ff4f4da3c8f2937b4981638", "ref_doc_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056"}, "f90ea413-7fae-40b5-ace3-061f1c0e9bc8": {"doc_hash": "f6e84423ea51610226dbd377e2f7c739a0a2d0d244c18ca90b597280f29231dc", "ref_doc_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056"}, "b0e3f0f6-e08b-48ba-bed9-d9ce642e11e2": {"doc_hash": "19e851d1cb4aef606f7f3f261ae79d320dbb4c8ab0c6b8fd35ed8aef098ed529", "ref_doc_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056"}, "add81db4-62a9-4661-8ac2-a19ba07bda8d": {"doc_hash": "abfb4fedad481dc1490734cf3e4886d39ec138038cde8766c7d7ccccbd981767", "ref_doc_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056"}, "af2f235b-36d9-4c47-be0b-d5252a5e7e01": {"doc_hash": "c14f372911b3d69bc715493e3feed636bba1ddd3558a94f5bc9a91a4cb9b2b16", "ref_doc_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056"}, "df545778-2cd9-4d2a-8e1d-68b917880033": {"doc_hash": "e4f50242a27a04a0e0066baa555f421415807aa8a68bacc90312727c3c6ff804", "ref_doc_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056"}, "271c8e91-7f36-4ed0-96d4-c03d3ad387f9": {"doc_hash": "9d7b6ee877d74c4206b053303f15b8061ccb8f5c6a9eacdd7343e37c4cd1172b", "ref_doc_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056"}, "8c9695df-4096-409a-a198-9edc65754726": {"doc_hash": "334c56026047b70ba57f5c0f9374d2dda6312c8c8f489d4816cd6b2e9f38ff17", "ref_doc_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056"}, "6b39f7de-e90c-4163-b46d-3a31b5186b98": {"doc_hash": "9cd26d61cb067e12a7b5d0ace9c01916252f31185f5d4b5fab65653d6094df94", "ref_doc_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a"}, "abe7815f-4a0a-42c8-8a49-6419e4a9a9a3": {"doc_hash": "16b30735a3d0fd5c90eb0ccd13f60b746a14017035f66ece89dc775e72dc5006", "ref_doc_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a"}, "c8c52cce-ec85-408b-bac7-47281d0b973f": {"doc_hash": "8957c4799cf67d5b387ce3cd21eff8b668028d9268371a25fc1d1f693f177dec", "ref_doc_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a"}, "870889d5-121f-4093-92fa-f2edaf2b9a62": {"doc_hash": "9f84b31f53720e47653fea077bbd0cc7138ae690bfe97ecb6a6410dfc95f40ee", "ref_doc_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a"}, "28a4f583-d4f1-40b8-bc58-92cbc9012602": {"doc_hash": "2c37c482777a8e55bc9bba375be9430c0c54c0c63ffca872e2ee704411063da4", "ref_doc_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a"}, "c8d11877-bc39-46b0-b67e-d255e6e27225": {"doc_hash": "4ba663435daee78e720d48a2f32bb41591c6cc5e0b16d0abdf0085ee75294967", "ref_doc_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a"}, "7b1892aa-1166-484b-9f07-a58dc722146f": {"doc_hash": "5556e5c8a05bb2e93535bf3b13f75a766c22cdb105c54b044a8d2673e5bb6f75", "ref_doc_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a"}, "d5e9b0a2-356a-4720-b40a-f0cc8ef54d28": {"doc_hash": "a60d506501572322d78b354ab001e7bf4d136fe4a9d1511816af9f1af76777de", "ref_doc_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a"}, "7534e32d-b763-475c-a4ac-a55776b5b47a": {"doc_hash": "ce5369f8981db5b55ff5f0dd46e37170f6636ad090cc7b313127b47a8e906a59", "ref_doc_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a"}, "ca7d69d9-5d42-411a-a918-fba0fbd8e3c4": {"doc_hash": "98ceb992374462177d371b19f84d66acb6abea66c4a73aad93cc8c217a636338", "ref_doc_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a"}, "5bf4882e-7fd9-4615-9e27-6696bccd312b": {"doc_hash": "d5cf7814fbdd2b41884b882815739f091624a15a408f9557d2f7e2daff8290c7", "ref_doc_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a"}, "597e1f74-a8f8-41a5-8c5c-b09664850aea": {"doc_hash": "20476f0f8cff3b66395e97c0472b2ac5284a517bff48a863f2ffca0e163f7f72", "ref_doc_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a"}, "8faea956-a09b-4548-9943-9553d9a30fa0": {"doc_hash": "7ff7efe6670bd2a031fcb2eabdda6a81cb371a1174e3536a8ee2a6ac6632ba06", "ref_doc_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a"}, "d2c4d149-ea88-482d-8aa1-84a02f4d4512": {"doc_hash": "a6309f9484545ef806ad2fc5fd29610a881a29c3a0aa3fe3d246c9dc3c7bf82f", "ref_doc_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a"}, "59b4af55-e8b4-4dd2-b5a5-3e3fbb97e799": {"doc_hash": "66b4c3ce574742bc909718e1f6ec6ecf5e19366a454ed8d5f55fbd9222351066", "ref_doc_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a"}, "5dd6cf36-a226-45b7-9ae8-f18a1019719c": {"doc_hash": "8978c47a3a3607eb4c0f922aa1b99c920405aee7c9ccb7520f72c31ae2314afb", "ref_doc_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129"}, "d0f4bd7d-63d6-498c-a521-103d8e327481": {"doc_hash": "43ada4f3a4e902c628f8e1218b9c1c11903513326db2ce5a7dfd306b4878a371", "ref_doc_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129"}, "50bc0444-5d77-45c7-8a94-c26767d3ca62": {"doc_hash": "11d7d3b082e3e595ed9e84551512df35d96cf7fdf817cec7e8ff3d2f253a6fde", "ref_doc_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129"}, "b170d507-85cb-43e8-9d01-1a6e8ad2b4a1": {"doc_hash": "be3fa62c3d8345a45185169549ff1fd305aceb5456ed4d9db578f0d6b53c0453", "ref_doc_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129"}, "470c222f-3c8c-4dee-a761-52ad973f69e6": {"doc_hash": "67cb1d10ca04fd2d8fa41fb631f87402ebc336cae8c5cbfca965e2ef0ce30cb5", "ref_doc_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129"}, "841439fc-e91a-4df8-a208-e215dfbd5daf": {"doc_hash": "313a7c9fa6fcb33c559d3c48f03119365425a39da3d460c03cf6d15c8b1027c2", "ref_doc_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129"}, "6f64e898-47f6-4761-8624-474c4c9705c3": {"doc_hash": "d09f3f52784494375d9c78328cb7caeb3ece61a26a8da40905fcd5b35845499b", "ref_doc_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129"}, "d1dfc197-1d47-41a5-b039-c784c347c4e9": {"doc_hash": "8c2188afba4dc4f23dbf6b35b87651b3cec5d742ef00c25de7661e0b7f07201b", "ref_doc_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129"}, "5e04024f-cffa-499c-a471-fc8beae013c4": {"doc_hash": "63f631412950479740e06c02b0bf850e84c9f61f913dfe2c0898f1c55c570b1c", "ref_doc_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129"}, "3d0e057f-7749-4c71-8cd6-b87c52e844a2": {"doc_hash": "08a7a0c7782a5b3599eb601a8814ff719a5252e13e94b6fcfec0a79f67ca1a08", "ref_doc_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129"}, "62d35df2-8b78-467c-81fd-33d864e3d891": {"doc_hash": "793d5dde8e5a6e3c8b5498cf8d4fcd6cd4c6ab50242d6d2b9dee0b8923d47bf5", "ref_doc_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129"}, "7fb8e670-71d6-4cdb-aaf6-68dcdf95d0a5": {"doc_hash": "22b42dc97edcb051ca17544cd722d67e7137f10b87dc865ba9a73c014e15954f", "ref_doc_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129"}, "34237fa6-9bac-46d1-a490-9e5f747efba9": {"doc_hash": "50af8a3ff214feb7bf3b1a9d564ff3afe5646fa8f267cdd7d9afa79dfefd2c43", "ref_doc_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129"}, "3cefa59c-f7df-4e30-a366-c88c0ed43775": {"doc_hash": "f806019f475c3ae714faede241fc549b93d59c33476bd04c5e97ea7e4f863dbd", "ref_doc_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129"}, "6f9159f4-891e-442c-a9d1-1345a55fae34": {"doc_hash": "8ea5394e92917a59f3235b3d192de34c8c5dace7a5f97689d607ad3a0cc6f443", "ref_doc_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129"}, "cdee7984-597f-46b9-b8cf-0f0102c8d8ee": {"doc_hash": "43028dc5efbbd7cdecffe55f1044681d54aff3a0c4d9aa2ee25a8952f85030ce", "ref_doc_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129"}, "3112f268-0c61-46c6-b498-a3ce351b73df": {"doc_hash": "0828acf51dbad27c895f07e67678d0df7c89fcab7b9079275b8ff3efe2642dc8", "ref_doc_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4"}, "0716e08d-9822-403e-a133-03733eb2b16a": {"doc_hash": "f0d022db2430bb06a6333973348c5f9cece90adda0c8fa82323438fd558a7587", "ref_doc_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4"}, "514aeb59-695b-4304-b7ed-e3856e745009": {"doc_hash": "ffe2a37026be0bcaeccb3ed95af5cec8e2c3a6626a6422b5a75dd87e7542e5fa", "ref_doc_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4"}, "da15dbb1-b0be-4c94-9127-d8080dc4e416": {"doc_hash": "61bc8cb923e79c65df19d68a250456a5d734d38d842b9e16a61dd705037dcbbd", "ref_doc_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4"}, "248abee2-d68e-4f2b-8de4-ada1424bce93": {"doc_hash": "5de0de7fc37202f99ece39a7404db27d901c7690b94daec17ab9de5b0f783ba6", "ref_doc_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4"}, "178f6a48-9367-44b2-a33e-049035c423fb": {"doc_hash": "346bff67e0753f599473aa3e88f4de883d008389f7018795a0f300937e117094", "ref_doc_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4"}, "4428b4dd-66e9-42b0-baf1-1bc6a25a8080": {"doc_hash": "9e665ac86fb0ad6f667a50d2891f24d7fcae918dd500a134fbc928593a148913", "ref_doc_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4"}, "e1bf8130-a00d-48b4-81e3-6a53b4fd579a": {"doc_hash": "b1f9f123a0e094c6bf901457d0dc3a06efa523eac0a70c271db85c16f794ce15", "ref_doc_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4"}, "fc7234a0-cb8b-4648-804c-fbeb6daf4489": {"doc_hash": "0935f4d57dae15961b9ecd696af2f457cbd6d5b5457831286b5e16c15ac783cc", "ref_doc_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4"}, "27dfb214-c52b-42e4-882a-b285da59f623": {"doc_hash": "d7a65ca96024531ff462e14670d55645509261ede013f614931e18c649e0ed12", "ref_doc_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4"}, "a5dd264e-ae72-4dd0-83d8-1cfa9ce6dbc5": {"doc_hash": "25360a8047cb2bfbeee2e5a692a9401407d104b81114c00227dbee7e7b0a1005", "ref_doc_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4"}, "e371f207-1421-4299-81ac-26503a01c555": {"doc_hash": "54102d8a5deab35b542dc66e03a7b3f0932a2f6a75fc61940eebdd2a40c736e5", "ref_doc_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4"}, "31a72be0-7df7-43a2-b84e-cc1979ea92dc": {"doc_hash": "702d8d482688d3a925b9b61b84812dde6c3f18351e3e10882c457d7be827f9d7", "ref_doc_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4"}, "2d51c778-a178-4e67-b40b-99059cd9f1e8": {"doc_hash": "732cdf19ea01770e0e2099ad0dd28cd249d0821ea134ff76ff57840a734ccb6a", "ref_doc_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4"}, "2db946e8-2f2c-45df-90dd-549c66cb2764": {"doc_hash": "155b0b36b9a117bd23e601dbfaae027df6e7eaff5b64978a364afe448475dc30", "ref_doc_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4"}, "0b56ecdb-6a66-4781-bb4e-4e01f4bec81c": {"doc_hash": "26a364ad4ec1a36a08072c7305f3d9e4efa8c97b548a41225bc685af3de731af", "ref_doc_id": "504434b7-d7ac-44ee-b82f-d66314dc879f"}, "4678b6aa-f137-4bf1-834d-8afc2add4a16": {"doc_hash": "be36ec16b99db1efee241e754461d396a1d3df08d732a9af209cf117744997a2", "ref_doc_id": "504434b7-d7ac-44ee-b82f-d66314dc879f"}, "f8d5c18f-d5b8-43b0-a3e7-9f5f01b2a177": {"doc_hash": "ad3a2443b2d98451e9cfeb3a0ec28718faaba73ebff72ca9f7999afcc39bb921", "ref_doc_id": "504434b7-d7ac-44ee-b82f-d66314dc879f"}, "7c92efd4-ba54-499c-8f74-383c7f0bf8f4": {"doc_hash": "98554ff97ed56c995bbfa8b4d609c3e6794cb257ef4616d20962ea4ee3ee378f", "ref_doc_id": "504434b7-d7ac-44ee-b82f-d66314dc879f"}, "410c0ab8-7f89-4670-bdc0-c166c1f44715": {"doc_hash": "29e3b41fe6d958b7df279241332d0d8db1e9d0db253b542f710239705709527d", "ref_doc_id": "504434b7-d7ac-44ee-b82f-d66314dc879f"}, "c30f3b3e-c08b-4991-8127-a8315df260b6": {"doc_hash": "733c2877498b26b7bb8f470f80af2bc4ec77c1cfe4d6e64c63a3d66164edaf07", "ref_doc_id": "504434b7-d7ac-44ee-b82f-d66314dc879f"}, "edfaaeec-115d-4194-8233-7b72f4f85843": {"doc_hash": "1426e7d0b86e616c7dbc297b12dae796557ece5ae7a1369b95133b66ac45e1a1", "ref_doc_id": "504434b7-d7ac-44ee-b82f-d66314dc879f"}, "7a3a5af5-513e-4f79-8372-7c640f019673": {"doc_hash": "15e239d23a0b216d7f465f489c5663362a9263512a43879d4a27fc5d21ee81f1", "ref_doc_id": "504434b7-d7ac-44ee-b82f-d66314dc879f"}, "f6cb09ba-958d-4be7-bbcb-41ed6e52f166": {"doc_hash": "91347e9122dedf3f36b3f1493f807c7a341f120ba17fed5d4b2667f9836710bd", "ref_doc_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5"}, "cfc3015e-c7ec-414b-9535-2fd4f42bfd90": {"doc_hash": "8cad636fc41d323a4485c00c6e790ad5eed2f42a402af00716a5bb1920ffb67f", "ref_doc_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5"}, "3a985fc9-8ec1-49ce-8382-c2517ca4fba4": {"doc_hash": "e06a0754d6b26e7c5c34dcaff22ee253745f39e695bf545231f4325f1dc2922e", "ref_doc_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5"}, "271b6b37-0c1b-4b6e-ab63-0bc5ff989a1a": {"doc_hash": "025209a5cf64fb178172ffafe53a0089d01d9e60e70d4b14d2ab5c59b05b09ca", "ref_doc_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5"}, "2ff0eb82-2737-43af-9f43-651f258b8f51": {"doc_hash": "3d8e5caa5dfbb1008bbe5809ba67b3e6d940b1a767be5207c87c5fbed05feb66", "ref_doc_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5"}, "1f305f1e-513a-4372-8911-57d407d90cc9": {"doc_hash": "1a9173d3106f7820fd2d0f8f4dbb322303b34778f7107171e3d7ae7b9146d45a", "ref_doc_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5"}, "66ebaa18-f6a4-4ce8-aeb3-066261e8ce81": {"doc_hash": "d52e269e2dbcd2b3fb91abfd5dad177ac45ed729ddf562e269633defb8c7d58c", "ref_doc_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5"}, "c5c48b96-117e-411d-9fd8-b1c0866f4781": {"doc_hash": "73439da66da8955ee71e91af18517aa0f2494bff2a104532bdcf55e72001b896", "ref_doc_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5"}, "158031f8-feeb-4b5b-a25a-76e77de0418c": {"doc_hash": "6cdff4ea2d3ce87882504dd2f74455e56839e38fbc72a357bba3b09272f9283e", "ref_doc_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5"}, "23edd47c-2e1c-4975-a3bd-6603995ec8fc": {"doc_hash": "6bb203386e18e659838df71079a669aa1b092fe961369dbd7bf5491dfaf60bfb", "ref_doc_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5"}, "678fbc52-5b8c-4a9d-a131-b7a5151582bb": {"doc_hash": "165e8ca8cf515aba29484f7158ecebd11ff0c92564a5dd1bca20dd4af842e9b5", "ref_doc_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5"}, "9674adea-790e-404a-b66e-df9ebe95ff68": {"doc_hash": "ed3f2f5dbeb3e270b84be52fae7c0bc35152f16615d538f582cac5296cad1a95", "ref_doc_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e"}, "14ce6290-a6d5-402f-821e-f631931a0fb2": {"doc_hash": "cfcb5dce54306c44fce93b4bd427ada582fec9f961ed9f20709ebe3addec3153", "ref_doc_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e"}, "ad39720b-3e57-4eda-9f64-1d0f1527bc27": {"doc_hash": "eb68793cd51e5e1ad80c174c5f1764f3415d3c62a571495c2dcbc9b5b0ca477d", "ref_doc_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e"}, "531b4297-4bb5-46a8-aab1-969ad39503a7": {"doc_hash": "f10a343ebc090cc7260635a1429b8215005c353073f567506eb268526394fd54", "ref_doc_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e"}, "3933681c-f4f2-451f-bff3-43d1ea83f0ac": {"doc_hash": "778b21e25e580e707884c8df98a4ecf17a4e200e1537587a1f152e0463c99b65", "ref_doc_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e"}, "019c7249-24d1-434a-ab38-5f569a61dc87": {"doc_hash": "ffbfeae40529dddf1e93ecdbac50ade3e6a2ac70786142d0d1d5a56ef3de91e5", "ref_doc_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e"}, "baaf5bc6-958b-4e75-a1c0-dccca96eed11": {"doc_hash": "655bee70fff591d05680afeeff1fdc5d14e769a5478ec5c0761c8110a8a2e83d", "ref_doc_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e"}, "f53e45aa-787b-4b90-b28d-2e56e93f3b35": {"doc_hash": "25c2e4673608673c6a289464250a53cf7cf6cb665a20710f825bed340daf9d98", "ref_doc_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e"}, "1341f585-eafd-453c-a502-a412336daa81": {"doc_hash": "c8346ecb0d918fe1257204c6fe5035a90c4816283c57e54b5256f0769d3e53a5", "ref_doc_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e"}, "8275cf49-52b2-444c-ae80-56f1b4dfa854": {"doc_hash": "d790fc84cb8a81e7d51dba04d2acc00ed27f08d6f1d47dbebb6f1c59664cd4a5", "ref_doc_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e"}, "bd04cbd9-f8a2-415e-87ee-31fae4d25536": {"doc_hash": "4383ca70e66e29ce9ec02fffbee2f61ae21e4bef3f792b3c7b772de7ec4df20d", "ref_doc_id": "379c133f-b178-41b8-b255-09fb40527f5b"}, "a2186272-28e4-4876-9cab-21a157fedbf7": {"doc_hash": "ae1fc2abf7e689435bffaaa5b5cf7308624702a2f37b91982ccf0ae5f100fbc3", "ref_doc_id": "379c133f-b178-41b8-b255-09fb40527f5b"}, "9f2c5c93-2e83-4da1-8f8a-f3debbc13a1e": {"doc_hash": "72df2af1889aad08af37f1b5d19880e83e221704f5957cd0fd582c06b9a434e3", "ref_doc_id": "379c133f-b178-41b8-b255-09fb40527f5b"}, "2cb7a4b8-a71d-4315-9ca3-03d58900a664": {"doc_hash": "7439f7d48bed0eb67064d90b6195399881f8bd0fd4e5428db4ad170973ce9510", "ref_doc_id": "379c133f-b178-41b8-b255-09fb40527f5b"}, "b9d09769-019c-43b2-985a-054385bb1c24": {"doc_hash": "bf479a53883235ea161eeb5caa4eee5e3198ffe7c9bb31121fbea1fdeae6ffbf", "ref_doc_id": "379c133f-b178-41b8-b255-09fb40527f5b"}, "456fd25f-8b30-46be-bc17-1756b1e0b9e8": {"doc_hash": "67438a79ee13e062b9c7e876a03ba3df5a808b3fa877440cdc50dd8b00933254", "ref_doc_id": "379c133f-b178-41b8-b255-09fb40527f5b"}, "dbae576a-5efc-4ec3-a083-0741a8f722ed": {"doc_hash": "da1c53b0770a24eaa33311b7f098ad7b3f4aeee8862af0c3881c6bd136d61210", "ref_doc_id": "379c133f-b178-41b8-b255-09fb40527f5b"}, "63d6c4ab-20af-4de3-94e0-18cfd4dcbfa7": {"doc_hash": "45d9eedcf30ab54c58ff609b0daf2416fdc7a6a34f0b7aa3fa8d343501bd0f3b", "ref_doc_id": "379c133f-b178-41b8-b255-09fb40527f5b"}, "9f3644b9-ce8b-4b54-9f9d-eb060f0b3e58": {"doc_hash": "9613e61157ece5950eef56cafde356d4a8fccf04eb6ca58038ede6ed79e77e65", "ref_doc_id": "379c133f-b178-41b8-b255-09fb40527f5b"}, "345050e3-f5a5-44de-9e89-d7627b5c89d9": {"doc_hash": "87a3b5cb6ec8f1335185133784d2dc54383eac719d6298db4f91803a8e01c4e3", "ref_doc_id": "379c133f-b178-41b8-b255-09fb40527f5b"}, "877bee80-fb5b-4b2c-ac7e-fb4abd7e3fdb": {"doc_hash": "366b7b053282920c6844e35630c3bd8a1556510901dbc294e890325f08760cd1", "ref_doc_id": "379c133f-b178-41b8-b255-09fb40527f5b"}, "51675636-1f9f-4468-823a-b8373cf1cc89": {"doc_hash": "a12e82f660804c3bf93edd9254a98d8201ba792075bc7e57c6fb62b8ff3b3535", "ref_doc_id": "379c133f-b178-41b8-b255-09fb40527f5b"}, "f6976ce7-6b05-4ca6-a3a2-bbb6d535b322": {"doc_hash": "4c85ad3e91381790dd0e32d2237380d6326775641cacb2ca97b9549873bc29e8", "ref_doc_id": "379c133f-b178-41b8-b255-09fb40527f5b"}, "6044c37d-73e6-4d95-acd9-d294fe0b5967": {"doc_hash": "959e6b364d85d8419b30504f8a1f41338490ad3101b0842fc1283b1c9ae85e21", "ref_doc_id": "379c133f-b178-41b8-b255-09fb40527f5b"}, "96ede007-8026-44a3-8f92-1c7445e282df": {"doc_hash": "7ecc30f63aa9aa7e3e27ac80977fd78f0fa86af5dcf585eb83e60d12ab23496f", "ref_doc_id": "379c133f-b178-41b8-b255-09fb40527f5b"}, "773f6ba9-efb2-4139-b602-945ac5d85c77": {"doc_hash": "644d9bedb704477907d1220f855c6c209a7e57c5911e228ba909e509859749d2", "ref_doc_id": "379c133f-b178-41b8-b255-09fb40527f5b"}, "f746793b-4175-4b08-811a-d0cd1f17c7a6": {"doc_hash": "06165093e3fb905f4cfd50e6aa58b4c30ff005724a3a8cd293f0362f45792b22", "ref_doc_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9"}, "527c9790-8f88-452c-8075-a1d50d4abb37": {"doc_hash": "26c3e70155bcd7f31b4ededa33a2f58a5e4da621ad3ca32275ef98cf4d84c782", "ref_doc_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9"}, "6f2ee4d8-c38c-4590-a8f6-29632711875e": {"doc_hash": "aa692e5f11c0726f441931594a6ada314629536443cfa165f8875d5b08d1b7d8", "ref_doc_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9"}, "8adabc5c-e492-45b8-8ffc-e9f0d273529a": {"doc_hash": "7aef2f08b24838e83ecb352a7f77fc575448c412cbedb169e1eb639c04ce9033", "ref_doc_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9"}, "51cf68e4-6de2-485d-ae67-2922cc51cafa": {"doc_hash": "f0733db3ebdf05b55a4c8ff41af9c2d0d4770e1e018ed62e9b201d7cae425cc6", "ref_doc_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9"}, "d0516e21-8c2c-4b8a-b038-55de634e68f3": {"doc_hash": "3cf0088d9d7e958a66bac22b6a54a1f8ccf54137082a5d6a416e94bfb2650c51", "ref_doc_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9"}, "4c9581b0-6633-4c0a-a3de-8f6bb89eeef6": {"doc_hash": "e65c00b2166679add8e51e3eea52f96489d334c643bdf6c93e79522bf3c1cc7c", "ref_doc_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9"}, "79de0656-c33e-48fe-b16e-4b4a5dc56f40": {"doc_hash": "36e48bb08adc6533f57b315c543dea432970b6a374e53ef267888270b015081c", "ref_doc_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9"}, "813b382a-3fa6-46bf-b827-f0c91fe6c876": {"doc_hash": "6a38acc9b361d899503a9bf6ea1686ad12b00ed5fbada06194c8a84f851c8375", "ref_doc_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9"}, "eef0880a-fdf4-4a0b-af16-09f9162865da": {"doc_hash": "6e4d2d9bb880aa0e318c951681d3b27ed1dc5f81a1d5a30cd26efbce7558a688", "ref_doc_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9"}, "3c12baca-0b1d-479f-99d4-f688ec2c4f38": {"doc_hash": "29de573c9e881e0dd5d6e6af6f7798576949f06e7157b2abd0e292d1f59de935", "ref_doc_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9"}, "255a4ca4-5a39-4ba2-a18a-63e427abc4b4": {"doc_hash": "7882e698147efe31e8ed0463161f3e7ae22d4a4e8ad20386c572ad01626a517b", "ref_doc_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9"}, "95bffacd-5b23-4d05-82d3-481a002b8a28": {"doc_hash": "9d6e4859be11f8eeba9ca7a2d9f5d5b7c00e7cb6121c702f8efda664843ef823", "ref_doc_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9"}, "d556a2c5-2206-4561-953d-3abf5da4651e": {"doc_hash": "878ec6916392b092b3c526552b6b28030e3a1ddc50e71c322baa6701523e0528", "ref_doc_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9"}, "a7859ec4-1226-40ad-bb0b-54244b5ff11a": {"doc_hash": "41dc2b8364a2fef75d67123d2065009e732c957d00247738fda3885a93d84c89", "ref_doc_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9"}, "56314dd6-1061-4ddb-a636-db0206f57be9": {"doc_hash": "6db4abf47525d7b9ce41dcd9b9b9ed851ebc193f09f961db027bad9e1ab30dda", "ref_doc_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e"}, "6fba6239-e970-42ad-85e5-77522f5a20bc": {"doc_hash": "c88f9e3bb05ab42be0eba10e631ef22125c1c7d567d7d5ea9d3b63535adc899e", "ref_doc_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e"}, "be1f73fa-af05-413f-b982-cba94adc557d": {"doc_hash": "1f1732267eacae10f4f9d5280e7c54f92486b34827194de7dfea48cc97c021d4", "ref_doc_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e"}, "d40b99d0-b88d-42bc-9114-127e80e37eec": {"doc_hash": "bceb7915db098aa2c9ca18125d996364a3b09073dae77fde73f338b87c25ada5", "ref_doc_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e"}, "68ff5801-50e7-428f-803a-b533a9e3e46f": {"doc_hash": "3603c126b40fa9cf8d299f8a5ff12d8bc7f5d591fe814bfaf31dd655671dad98", "ref_doc_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e"}, "ee82a7a9-c60b-4806-b7d5-c098de5d78b2": {"doc_hash": "8d269d52236a329743623838274635a30da91f1b7d462bc8b002a320f649189e", "ref_doc_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e"}, "3ccbab52-6288-4576-95a0-a19a7abd4d1f": {"doc_hash": "1cb1ae4a41de58e5167624b830b81e6c8a2b570e0f43b1063dc280d3ddbcdbe3", "ref_doc_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e"}, "9a0e28e3-c119-4cba-b3ae-9acbd1cd82bc": {"doc_hash": "42374d99eadb559d855871b1ea8eefbd0ec093aefc5257c7d30c15a0d3249cc7", "ref_doc_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e"}, "fd766762-ca70-408f-827d-7285e8780576": {"doc_hash": "5ce9665220c0a0598a5235cf1ad760453ded4daa18fcdf63a6173a893b30435f", "ref_doc_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e"}, "ba52c01b-71d4-4afb-a5e8-a332aa3388d7": {"doc_hash": "74ee507ce33dc127cee90744c83ec6363563683e23932acb0b4b2e9670888a36", "ref_doc_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e"}, "1c68bf0c-3a3e-4374-90a2-5c3320e9da1d": {"doc_hash": "a695c344614da3a003731cc9a730cf38580b925af4a52160a26e1e1b8c652aec", "ref_doc_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e"}, "9bec683a-b82d-44d7-aa0a-26e69d186443": {"doc_hash": "148e8af1d8057588b521f67a4879479d24871340877dd5fd42f9859067c533f1", "ref_doc_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e"}, "e224cf01-5cb6-4c6f-8335-c7cf0b51ba6a": {"doc_hash": "fd2feb72e3c0ae9178cfa968c51b15d62003350b72a52eac8adecbf1cbca7f4c", "ref_doc_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e"}, "7cc6140d-66be-4eed-aafa-16fa334f286b": {"doc_hash": "a557143c7a3875bbd6c3227dac329c526aafbfd52b5c09f17b13ab91297b58e6", "ref_doc_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e"}, "13cd0e23-2706-4e65-abcc-caf9d0e3ffa6": {"doc_hash": "acd336c4b79df0d3422512a1b3979bf7226ffed39c954f3f117be62d3ba4c3a4", "ref_doc_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e"}, "e071bcdd-702a-4eb1-8c5e-c9d979f31d76": {"doc_hash": "6811ce42fdbcc845ec8652e211be9e97e60cab62697bbfc3c5e8a46432e3bc52", "ref_doc_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e"}, "670ea53d-d1e9-4bf4-9675-6cde9da55787": {"doc_hash": "fedf62451ccf197071d4ccc04adf50993a40f8285f682a31ebe7f63d4e6e6e30", "ref_doc_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e"}, "6781525e-8480-44c5-9174-6341b0e015ac": {"doc_hash": "91ff6a8e904f2a38072f291ba86faa54fcb9c454fb77ff07b4936fada24fbedb", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "e6791fd4-90fe-4ef5-af23-4c3fe135e934": {"doc_hash": "f6296f86ccaad98f68567aebb7644e6efe9daba2a44e22cc604c174a7d375f7e", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "28a3eaf2-be00-4db7-a230-61f7b0c18454": {"doc_hash": "4e33c0530accaa7bef19c155cb0480ebbad13a0b0428f2d210939a17ef2b7662", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "f79e72eb-6945-4965-b735-42e762c84ee8": {"doc_hash": "ae924c9ebd7ac46cc5b3d394ca7402fffa0d379dad3de8327c3ee72aa1e94e19", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "443a33d5-fe83-4c2c-a637-11e220d5e8b9": {"doc_hash": "164bfb687450ad81b3b8dc5f1fef1cbe78194f0435a55f69fd9651c39fa275c0", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "21a69d4d-32a2-4343-a878-df7303d60ff1": {"doc_hash": "f19976f81b1de4747fcd6defa1f45da56b855a06998877ef23c9a2f92053ebf4", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "2db2b4d6-7a81-4a9c-b6fc-22e1f92717ad": {"doc_hash": "7931451091893bae5340478d87a69a13ff8716ea842855f8a5b99c726857c986", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "3c9fdaa2-1d76-4b85-ac8d-74d2f42f5802": {"doc_hash": "09c26cf57b01b20f77f5dd05d357de4372e69312ca8c07a408dd075760de3ce8", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "b9475402-a72e-478a-ad98-e336327f54e1": {"doc_hash": "36831c9123caf89ac31d01ff632b220c3a4bf1f937cb6394ff8bddf415c07a9e", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "7cceb124-274d-4a1e-8ea8-4d4286625a10": {"doc_hash": "7dcc5c765c8ddd3717dbddfcf2cdd2a3b4a40feb0b5ed5226beaaba48c495232", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "83d857a3-86ea-42b0-b6d2-8173642ba453": {"doc_hash": "23eb5c2d6b5f2c142b96ae071757beb8e2a4717025c87713bdbd8aa0894ee4c8", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "a0db060d-6f3f-4100-8503-51203880b818": {"doc_hash": "d6374d5ff578294df2f6a9589ebafea96f7b1a4b4ebf99d1614d0877f51788fd", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "af8c6e75-5b3e-4ab3-9fb9-79308a312647": {"doc_hash": "a9c204dc66814ead6ced2ddf0a981614a439a71eaaf47d27c87610f714fda7b8", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "fde1bfd7-7ad9-4a3f-868e-ff550405e87c": {"doc_hash": "9bbdeae85d8731cb56a36b45a174f10894c8c157ca4c25c6f7f8c1cc1d35d1eb", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "01fffc19-cc5f-4c5d-a929-1b28af70bca0": {"doc_hash": "9f7f707b26249fdf91af7b04634589ea1d0ada47649566212fde2f6b51d26415", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "63a222a9-9a11-4255-a06c-572c10c91819": {"doc_hash": "72e05e8a89944509dd250ad8361d874ef1c4bf861b640306757fb55cee04439b", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "08b29995-a904-4aa0-afd4-beec83a1cafb": {"doc_hash": "579b5fa6a46337f4a94cd76ee67ac9c140f119f13585bbb5d5ae40abadeb403c", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "a67b01e4-3050-4be7-a07d-306bc59c2b32": {"doc_hash": "c38f6f3c4db1801ce05bcd0f613c645e6ae80c383b6159245187c83f9d170a34", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "e9cc89ed-3da3-46f1-82b7-ccdf3bb4def6": {"doc_hash": "a35e0dcc410a63077697aa22c0cb9626d041282e486f27cfbe5bb2bae49c5b8d", "ref_doc_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f"}, "b4b9faf5-0f15-4da3-b0af-9070ab8c918f": {"doc_hash": "7216cdca633d749eba10c9253c71831cc74e3506f04fa82cc431687e69e89406", "ref_doc_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb"}, "dab3435e-7d10-4def-a8ba-4287f2b7802f": {"doc_hash": "3b5d145bafe603a75ea805a3f43b361689d3eb43baa60598ae18ee6d8fd35728", "ref_doc_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb"}, "ba1329eb-dd87-47d3-acb2-f06627b0a070": {"doc_hash": "7c911f032c74cc1e9fbb324d601d995087365416c10d891cd2e6981678df122b", "ref_doc_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb"}, "072e28c3-53dd-4b61-bab1-4c56b8f158b2": {"doc_hash": "1325ca5853219d3af12d7707fb87ec9a35e6b64381bb299dd031debe0471627d", "ref_doc_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb"}, "750432d5-3963-4160-aca2-504a84d29755": {"doc_hash": "f2d56244aa51be290fea69fb119523fe5ecfe8aa17229e69b136b3d90df65264", "ref_doc_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb"}, "858e6d4f-4cb9-452a-bc08-5e275ca9cb74": {"doc_hash": "ea30809098b866b10ea09739c25430ec255186d0924daf940a13b22f4d58221a", "ref_doc_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb"}, "38ff3acd-f6e8-4583-be08-942c089b333f": {"doc_hash": "74b355a95aabb04e50de1b40bd46e717a7731146c528175c61f9fa4096358f1f", "ref_doc_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb"}, "13a375ac-50af-4d49-a8ca-ff69e6942061": {"doc_hash": "9136721b7e6fe2703c0a742a9a63dc5913ec66b0772147f1c0639b5b355ef7e2", "ref_doc_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb"}, "859b5f2e-8876-4f04-8d41-d74b3515be3b": {"doc_hash": "ac0b5480829cfce0cc5b25795016bb2bab859de8c30025ea7930c8b7575cab24", "ref_doc_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb"}, "5759d082-e98b-40f2-9d57-5a4bf1c97e75": {"doc_hash": "f915cc6ead576f4f5b9a014db5047b3e29b6a72d624ac0f1c030c1d33d9fa0fd", "ref_doc_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb"}, "2deb1e91-700c-4e43-a86e-eb461dda5c30": {"doc_hash": "4d8957996a9ec060e8a2c63e0871cb0970c9ecaf92029c496142b4839823005c", "ref_doc_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb"}, "95277ebd-ef74-49c4-8d5d-d0e3b85dd531": {"doc_hash": "fc5111b021761c6cc7ead7903034143e5d2f03a17dc7a606d435867b688a3c64", "ref_doc_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb"}, "ee7f0fd2-eb8b-4a40-a3c4-62067cf8181a": {"doc_hash": "07bfaa38c49780b0490d2da20d6be94aa812f1834dd2ff8f1fe18f354845f28d", "ref_doc_id": "85737b80-6c4a-445e-b173-8c9277e49204"}, "20d922a1-47d7-4c52-ac2f-4796b063d8f9": {"doc_hash": "427f3e035e186b596b96d4680ce0c6942ac3ea850daa7da726af291af9517ff4", "ref_doc_id": "85737b80-6c4a-445e-b173-8c9277e49204"}, "a7078776-4551-4944-ba1e-8b1ec24cd9e9": {"doc_hash": "7ca2b3f0f6a75466e71fcf21b3de730a15fd50cd8ec442c028e49105e3622a48", "ref_doc_id": "85737b80-6c4a-445e-b173-8c9277e49204"}, "17f82a1d-2aee-4478-ac8f-013105a4b701": {"doc_hash": "9889811f66551a2f5abd1d6795206a032a217d0adcf607963dec10fcdb26d5b8", "ref_doc_id": "85737b80-6c4a-445e-b173-8c9277e49204"}, "f177db70-122b-4602-9ab4-cd7f3ece95d5": {"doc_hash": "b33c93df1b576dbe00dfb417a04dbf68799367cda14b060b03db445e397e398b", "ref_doc_id": "85737b80-6c4a-445e-b173-8c9277e49204"}, "6189ab3c-eb2e-46a8-9ef4-a2c873357e56": {"doc_hash": "59ed19c40a9611124eea6280367d7f9979a48fb37f17148557f5f20a13e6f0e0", "ref_doc_id": "85737b80-6c4a-445e-b173-8c9277e49204"}, "8247b101-90cc-4661-8f72-78a888d708c7": {"doc_hash": "82ced87bf2e394755bb6773dbae2a6eb6d9833ca4c4ce5ce40079d2a148a5c67", "ref_doc_id": "85737b80-6c4a-445e-b173-8c9277e49204"}, "562f8e26-f9be-4c6f-a83a-bfe6195546ec": {"doc_hash": "e675e630612a35d43c66ac099204dcbef2ac5361cbb02079b6a8cb5ffb3647c5", "ref_doc_id": "85737b80-6c4a-445e-b173-8c9277e49204"}, "7949cd4f-c5e9-437d-84f4-462bc1211f3f": {"doc_hash": "85309c14c5e4bfcb9460b25ed101a71a4edcae18551d4e84337d0d211e7e2860", "ref_doc_id": "85737b80-6c4a-445e-b173-8c9277e49204"}, "050b11a9-35b3-4a5b-abfe-6a90536f68c5": {"doc_hash": "55bbf8572b546c998a2354d4d2932323320127d4b91c244febce0b35aa9a3f04", "ref_doc_id": "85737b80-6c4a-445e-b173-8c9277e49204"}, "f38bc9b7-f176-4299-980d-e2ec65b1083f": {"doc_hash": "66410bb9ad51b5d679e3f9d7c02a26259b6638b13a87c54c745552fcb130d58b", "ref_doc_id": "85737b80-6c4a-445e-b173-8c9277e49204"}, "4605c950-9913-4c4f-a6be-a97671cf5cdb": {"doc_hash": "b407420d92864ff0f61222da26d6eba3cd539b97572c95340a49f876100e0d57", "ref_doc_id": "413315cd-b11e-4878-a2b9-e653548db0f6"}, "8f751593-1b85-4d9c-a8bb-0e4176fbeac7": {"doc_hash": "ad2d89189703632c27d98192b450bf991a09cf3c85484084f9397e76a6c86468", "ref_doc_id": "413315cd-b11e-4878-a2b9-e653548db0f6"}, "b2c1f884-b681-4218-b1a5-d4badc00bc61": {"doc_hash": "b4732816c2aba181dba7fea899067dcc357089a9b29805ef1776d4dc1a05c5bc", "ref_doc_id": "413315cd-b11e-4878-a2b9-e653548db0f6"}, "acd201aa-2168-4178-9c9d-6368e54bcecb": {"doc_hash": "9dd33c414c513e9487149fb9bfdf14fb62bc1c69467d55b60e96f6132e70dc8b", "ref_doc_id": "413315cd-b11e-4878-a2b9-e653548db0f6"}, "69d9c997-ccc9-4670-a6b2-6099224c2b49": {"doc_hash": "a6b426494cbc40575054723b9fcabe211f1a49882c809629c1b2060b7227fc0e", "ref_doc_id": "413315cd-b11e-4878-a2b9-e653548db0f6"}, "a7087c5e-dd79-404a-ac42-ec59a44b3e30": {"doc_hash": "90b1fcbbfbdd6b6554664aa9904fb2616767843c408a79474e93e4fc8dfc2cfa", "ref_doc_id": "413315cd-b11e-4878-a2b9-e653548db0f6"}, "4a546e0f-a684-43d6-9dfd-c7489a85d753": {"doc_hash": "bcdf38604bf1be384db078b24197804da4e63d92c6ac3cf23b78595a1e54044b", "ref_doc_id": "413315cd-b11e-4878-a2b9-e653548db0f6"}, "5077c8de-7305-4184-a9fc-ee4f5cbfa31c": {"doc_hash": "dfc2d7ddf104c063f7a28b01bffb849a0cdf0d830b35b4bc75229183a61c7259", "ref_doc_id": "413315cd-b11e-4878-a2b9-e653548db0f6"}, "c7108d42-1b03-488f-9b45-42e9b2339bde": {"doc_hash": "736ffa6607660150b0291e95a5a67cd408ae83dd87746cc51ebdbc20aa14b8cc", "ref_doc_id": "413315cd-b11e-4878-a2b9-e653548db0f6"}, "05dbb448-bf7d-40ff-854b-4cc2b0cc172a": {"doc_hash": "4e2b5273b054d28cb74f33ec7553220bc0fc8d0e80b60105b4e154eae7301e6b", "ref_doc_id": "413315cd-b11e-4878-a2b9-e653548db0f6"}, "5874d62b-383b-4ccc-9117-d2a5e44c32de": {"doc_hash": "fa02c2c31dafc13bcdc45d45109737a055ddb1712571a08f4cb72999bccc9632", "ref_doc_id": "413315cd-b11e-4878-a2b9-e653548db0f6"}, "04131aff-4e42-4283-94a1-09140859b23e": {"doc_hash": "4346224ff96329df4216e2fc2467a616a68621ea5f61f72ca44098abf4c85c35", "ref_doc_id": "413315cd-b11e-4878-a2b9-e653548db0f6"}, "1d4ce580-c085-4728-bd9f-7b321d548077": {"doc_hash": "c12cbac0df625d18238b1c2d5f9e4f954477c938ef26294eaf8fcb1c6e045f05", "ref_doc_id": "a9256579-366e-4b1d-821b-7e410b884184"}, "6359578d-c194-43bb-a730-686f0f0680a5": {"doc_hash": "b048a7ed323c5eb1cbe5a139499724d8df0794bc001c0b6c53080624b2f7c250", "ref_doc_id": "a9256579-366e-4b1d-821b-7e410b884184"}, "3063fa75-f221-41bc-b87f-86db7e9a3ca0": {"doc_hash": "15cd05de8ad136d0987ed38a0581b25737aa5473bba3afb8b6e4ef1a4c53ea8d", "ref_doc_id": "a9256579-366e-4b1d-821b-7e410b884184"}, "398cdb4a-588c-405d-a78d-3007f1e610cd": {"doc_hash": "134d928b811883de871b63bc2add8bdedacce780fec305d979cae94efc9bb347", "ref_doc_id": "a9256579-366e-4b1d-821b-7e410b884184"}, "621f8d8f-325c-4cc5-8faf-0ca60d8ac2c9": {"doc_hash": "2f67570d9ad21d1466eba309e7c3b1d24dd5e35abc2099480ec782c679f78a9f", "ref_doc_id": "a9256579-366e-4b1d-821b-7e410b884184"}, "0c87fcee-c971-45a2-b1e6-c0e689eb9d50": {"doc_hash": "86b481775b4dfc9cbcd0dfd50c57f62673ff68aadcd48c6e0bb2574dac5b4df1", "ref_doc_id": "a9256579-366e-4b1d-821b-7e410b884184"}, "3301bf78-27a9-4cfd-a84f-14739569d00b": {"doc_hash": "af1190f75bd7b586af9307ebac303d4307e38474947be8734edaeee6be438016", "ref_doc_id": "a9256579-366e-4b1d-821b-7e410b884184"}, "aea7399d-1604-4f71-a6c2-e5ed90e864f2": {"doc_hash": "e38e58f1a7ec5573589f573f846aec758721b5ce77c7f3e8fda04a87acae216d", "ref_doc_id": "f13e8ae9-34c4-472f-9fd5-2bd7c44a33f8"}, "3668a3e5-f6cb-4f29-abb7-0cefdb73436b": {"doc_hash": "8df2e1c6a5b0d0089dde2c0f733e8eae5d269190307765779167573806ef39f2", "ref_doc_id": "f13e8ae9-34c4-472f-9fd5-2bd7c44a33f8"}, "6405d710-c524-4550-aa6d-e590008be9f9": {"doc_hash": "7cfaeb9153f5d5ec552d32271a32326f219152904214a4852482b4b7cbb5ee16", "ref_doc_id": "f13e8ae9-34c4-472f-9fd5-2bd7c44a33f8"}, "095c0460-a0b6-435c-9893-82878fd88e64": {"doc_hash": "fa546294b2648c0f0fb0e5bb937bd707c1e99b8184eac3e49ae665c9676a00d9", "ref_doc_id": "f13e8ae9-34c4-472f-9fd5-2bd7c44a33f8"}, "1bcbeda0-c2e2-44cb-ab6b-c835e19884e8": {"doc_hash": "e13b456e283f50efb0d0f778c7298cb6b8cde2e06c6979c068221046c30bd261", "ref_doc_id": "f13e8ae9-34c4-472f-9fd5-2bd7c44a33f8"}, "64f21f32-de35-476e-add8-69af4ecce98c": {"doc_hash": "80405be03e5880b596ae04b70fe786093b9c265213d30e2c7dfe0cbfd8dcde79", "ref_doc_id": "f13e8ae9-34c4-472f-9fd5-2bd7c44a33f8"}, "8f789e43-ad43-4f41-a2f9-b0bed5dc4ad9": {"doc_hash": "6f8d48b30061b9c32d2ea90e00a16d08754e7fd1443c5f4b76c97e114316573d", "ref_doc_id": "f13e8ae9-34c4-472f-9fd5-2bd7c44a33f8"}, "e7213d1b-3a36-484a-9fbc-ea4155b6664b": {"doc_hash": "8d96e912159bd97663328ab3368755af4cc551f209b4ef5fa934cb5de0fac83b", "ref_doc_id": "f13e8ae9-34c4-472f-9fd5-2bd7c44a33f8"}, "f25a8684-701e-4bf8-94cf-c03306e2618e": {"doc_hash": "7ce4d693b90fe933d9060611f8354035231da66c34394f59acaee258afbd7f6c", "ref_doc_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea"}, "f4ed1c5c-539d-4b3d-a13d-9d4fc5ba828e": {"doc_hash": "40b05fbfec89793b92fd810575ed6c48a54ac641e16cdacfb39c688d79e5f698", "ref_doc_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea"}, "61113d88-0609-496e-91d0-575c2547835f": {"doc_hash": "df3dcdd9dc50148f782044c75f45fafd70cd6164f5e1f225ad7b9b209c5cd1b9", "ref_doc_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea"}, "83bf573e-f907-4ba2-aada-1a32bfd179ef": {"doc_hash": "6ee4b2d7fd3160c2c4ab39d55281a51bd53f4ed2f6b23be621a729802052c568", "ref_doc_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea"}, "98b2f0f3-f3fb-43db-a496-1ec409796ee8": {"doc_hash": "cc14a6d2a841420ce760626e618b22987eb39b243dfa7c6e1c2e72e8a243595f", "ref_doc_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea"}, "18c69308-84f7-4dc8-92ce-8aba47ec6874": {"doc_hash": "d8acccfc7518e7a53f3b923a27b4aff4308382239861ccbafad8437ca2fad97e", "ref_doc_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea"}, "4540d62f-d7cc-4feb-a2c6-fede1456f207": {"doc_hash": "d3d285353f521e76cd836a48dd7045f678d125501c1dcc742d25116c781964df", "ref_doc_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea"}, "5bd16d89-190c-49f4-b8d6-a55b730f5f85": {"doc_hash": "c07993f07e797ae8bef8e06ac97cf5439c1ad78c767b2dcd2501210d05bb29e1", "ref_doc_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea"}, "ecdfb7f9-71db-4368-99de-b40bd2642a44": {"doc_hash": "e82aa4dd8cc6b1d56413b82488b9d391c2693cfc95e61b1b59cebf091dace50f", "ref_doc_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea"}, "9f8c5dd6-26fb-481c-831e-711684b41a30": {"doc_hash": "389ee720a4bc4dbccabb26a69d1f5eb56379a589b7b40c0e2cea669e68c7f928", "ref_doc_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea"}, "7ca79a94-2bc0-4855-bbb0-c31be0cfab29": {"doc_hash": "acc83984f96b74edf5acc009d77d80de32223d30998c7f8fca8445d3dc72f945", "ref_doc_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea"}, "92d3c07e-4996-4422-b0da-f53f5e19ac8e": {"doc_hash": "d3ad8de3a971dd0a9d50ec12511294d2562aa78bad172cc0023f070442b5a416", "ref_doc_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea"}, "3b23392f-ef78-4af5-b649-cad427e33eec": {"doc_hash": "9e11c5ad61f1d8c840927b8e9c4910b4827b943d83d148ae3be3e57ec06deb1e", "ref_doc_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea"}, "941e6e63-0642-4911-93c3-1a436c8d6f75": {"doc_hash": "30d8105994bd4f9570cb93580154b53504924048b01c9537afd881f02aec2b65", "ref_doc_id": "831b1971-2e28-49f8-814b-0a40a7f215fa"}, "e40c9e7d-d78a-46f6-8b38-b6f0cac92220": {"doc_hash": "e0c418b44bfd183798d82876adb3663bb5d491f6ada097f37b381d21ff897d81", "ref_doc_id": "831b1971-2e28-49f8-814b-0a40a7f215fa"}, "244a53aa-238d-4837-8a03-9702012af209": {"doc_hash": "5bc7eecb8cc2eee7707bdf2ac8d2cadb3b5f1a0ad872df8c00dc25aa8d3c062e", "ref_doc_id": "831b1971-2e28-49f8-814b-0a40a7f215fa"}, "d8c99bef-a6dc-458c-9f15-a7129365f7c9": {"doc_hash": "05abc2436fd32c24fe3dd4248aa162a5a9f4e5ccda34a903564fa3a7b5dd3d9a", "ref_doc_id": "831b1971-2e28-49f8-814b-0a40a7f215fa"}, "998f049d-b5d3-4bd5-80d4-5f2814e83138": {"doc_hash": "35c90b3f76a66b6904cec808da592d302c03e686c9bbda07fa20e9fa8ac31521", "ref_doc_id": "831b1971-2e28-49f8-814b-0a40a7f215fa"}, "80e443ac-edbf-4a93-8662-05614bf6a340": {"doc_hash": "cb599b712bc16b2a65a4d3b063195f250d3e9af5d55a64e26e27e72a42127949", "ref_doc_id": "831b1971-2e28-49f8-814b-0a40a7f215fa"}, "7e2c1c46-67c2-4fc0-98cd-9c9fce6549a4": {"doc_hash": "71e8fc27d214bb8af9414722abf6949817e4586f47c613f3a97f493012d4ced2", "ref_doc_id": "831b1971-2e28-49f8-814b-0a40a7f215fa"}, "89a3eb54-3214-4003-adc7-5995d9c40349": {"doc_hash": "806849044295f709a79f95790de012adb246234c57b11852213a0172dc54ae27", "ref_doc_id": "831b1971-2e28-49f8-814b-0a40a7f215fa"}, "180498c1-ec3e-49ed-b945-0914ef8c282a": {"doc_hash": "2a4b23984e09e92a6f0a3a3da84596a17206e15332328ee44c1c411f714ebf13", "ref_doc_id": "831b1971-2e28-49f8-814b-0a40a7f215fa"}, "00bec5fd-2fda-407f-af14-2cd7fe9e18ef": {"doc_hash": "b0d60346b4b36917bc8c9b29b92dfd6afc64c6f2ca0ec4a403d25c461ab1748b", "ref_doc_id": "831b1971-2e28-49f8-814b-0a40a7f215fa"}, "80b5b7dc-f200-4b30-8526-224b6537b1e5": {"doc_hash": "634a5cab0cc138fd8e62c0b2226a9bdc1eab142521e1baa713fbd7c9defc81ff", "ref_doc_id": "831b1971-2e28-49f8-814b-0a40a7f215fa"}, "fb70aaa1-0326-41cd-9992-8b98cc90d3d7": {"doc_hash": "c7dff94c052d28340d28634940c177f3cdc4fe991d83abc2fc3e280287d90638", "ref_doc_id": "831b1971-2e28-49f8-814b-0a40a7f215fa"}, "87c2b0f5-8ae0-4338-b26f-a1ac43d16bcb": {"doc_hash": "44d5855f7294ac007ae03c804e8d037f44ec6a947166b30a2a4e85f45dfb15a0", "ref_doc_id": "831b1971-2e28-49f8-814b-0a40a7f215fa"}, "76561b4e-24e4-49d9-a25f-1655f512412e": {"doc_hash": "e987e45372129cc04e10b430a11703b9fc953e456f84e9ede669210e417d6ddb", "ref_doc_id": "831b1971-2e28-49f8-814b-0a40a7f215fa"}, "e28e121b-2c44-48c9-8eba-90d578892858": {"doc_hash": "dba274dc1f9433bd6a903526574782edeb819036884623975080a2391b134a9f", "ref_doc_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595"}, "9edb9de7-6ee9-421f-9e81-fb3d60d13fd5": {"doc_hash": "36adfd96403486653ca29e8aed2b8ede6cc45bf7c526f946871746219bf090e1", "ref_doc_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595"}, "3a4e4613-5acb-4380-9bde-c86134bcf165": {"doc_hash": "bc4ba28b97290aff33d221860f90e62ccf3cb937bb009656237e879fe04e71c3", "ref_doc_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595"}, "12c32e20-ea7c-40df-b99f-b9cdd50428ed": {"doc_hash": "a594d77697d07c7771822152d73a6c024359fcdf7e3a6760a5cf49574a7c31bd", "ref_doc_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595"}, "9abd727d-3ef9-4096-aa23-0c4fbed05e98": {"doc_hash": "468db5a670d33c1ff852c7e31be40fac52e5adba0b2bc547f2d4a4caade04df0", "ref_doc_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595"}, "6b304b51-214e-4e96-9ae5-bfd8b35cde6b": {"doc_hash": "cd0a8aa378241ae7cab9a081fc0ed234a1e4f31ba3bc66883fbff5b083d9d66d", "ref_doc_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595"}, "923f20c4-8403-47b8-ad81-87bcc3e811a4": {"doc_hash": "638cd89e662bf9a77787d4fdc6ea70f6ede83b5b6ebe40478045fa628fd3bbc5", "ref_doc_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595"}, "c9f15394-3e6a-4dfc-a217-ae33b09c0db9": {"doc_hash": "dba50f957ad9f740d24666271b22bc58c790278d1c4bf300a42361a640575457", "ref_doc_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595"}, "1b9780a4-31e2-48cd-afba-128f1e61d040": {"doc_hash": "c66cc4f3261952ed962cd4187f094a38ced5acd6d2e9b818aea19cdc1362ba31", "ref_doc_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595"}, "3705a049-24a2-4419-8593-395783c26699": {"doc_hash": "f72c783455dd8f55fcafd1ce4b8e7e51637b615ee9bb09bea686b2fc29db08e5", "ref_doc_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595"}, "5ff668f4-b456-40cf-9343-12ff2ae87b15": {"doc_hash": "2e23c7a5163621b4cc34c31ddec160dbcb2f51f669be5983aa857e7a02d89944", "ref_doc_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595"}, "e5470b4e-a719-4c21-ada0-b6e39c62cac9": {"doc_hash": "518b7d24f7af801c6801d64e7892b4d934b370bd339f77cca4fad9cd23d9b2b3", "ref_doc_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595"}, "0df911a4-79c9-4fb7-8323-3cc195a4aaf0": {"doc_hash": "e1757b735606f37d9069650760ece8927a800e3ccea0ee8ffcf8745bbb7cd2d3", "ref_doc_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2"}, "c00dc7c3-d38f-4011-8a3a-810e1409300a": {"doc_hash": "b0fd2f0d0a9bc949eccaded954a7bb32a22f388185d34d2266321250e8affde0", "ref_doc_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2"}, "236923d0-f07d-4f86-af8b-c3e106eda8de": {"doc_hash": "8e0183b98c22fbb8ef4ca49484c6ee639e20a4a185a40d724e3d86d03e740ff2", "ref_doc_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2"}, "ab098901-752f-4dc0-ac9d-8027175549ac": {"doc_hash": "1718ddaef682b74957d988b13d75ca9e7318efc7eb76d2607fdc18e4ee385d5f", "ref_doc_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2"}, "d21c6dc7-34ff-4315-9adf-36d7707170fd": {"doc_hash": "6cf029ca7d981dd30fd7cf2c74786bbfdd8a7855faa2460c39f29399c754d0a1", "ref_doc_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2"}, "b44ef511-8e2d-43b0-8bd2-f2a3029f4eba": {"doc_hash": "23a7f57c35f98cb34d519153d44469411cc00e7ee4cf9eda49fd45446573cd55", "ref_doc_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2"}, "ea51b84f-10cd-4948-b5ea-e5bd7f8f7608": {"doc_hash": "c1e787c457157f78e536e92d901f596d4eaaf112d59cddf6dfeb97dcd8f6e2b9", "ref_doc_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2"}, "25a0fcef-3040-44c3-86fa-b0bbfdbbcf34": {"doc_hash": "90f7955a90e677123f4a641b2d2276113bbbc2193a3d6f20288d8107ea0ad55e", "ref_doc_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2"}, "a211fd5c-8abc-45da-8e14-505855b4539f": {"doc_hash": "fad493d2bb9b107eabf21af8e91198c013b45294e974ae72463285ff110a5cab", "ref_doc_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2"}, "957eeeb9-ab77-446d-964a-d5839b53adf7": {"doc_hash": "aca12f4a04cccb78fa38cdff6232bc63a2693bcd74d64d0141cc221f83d1108a", "ref_doc_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2"}, "3081212b-b031-4faf-90cf-3ac781ab69f6": {"doc_hash": "aa5a07a45c4bf025e5fa203c13b2b240d919db239db78165c3d54c664ab15bd5", "ref_doc_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2"}, "d2768a8d-1fe1-4c3f-86c8-333f21e2080d": {"doc_hash": "aa28262bf01fa86d16e461f24ba8781a0c8aaecc31e1dadaa6bd81f2d75ecf8b", "ref_doc_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2"}, "5dd64e0e-f03e-4080-b0b6-087d1d58f192": {"doc_hash": "c85cdba00eaf9957a96032642afb90cb026124c9171a23b3d936f94a4d4581e7", "ref_doc_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2"}, "5d782522-bd9f-4b8e-b157-3ffc10792f55": {"doc_hash": "3ed484b4fbc586ecbf3c14c122b07fabfa5dfed998d480d32f2fac592612ee4c", "ref_doc_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3"}, "5c76348b-a7fd-49a6-a3af-aee53fb94002": {"doc_hash": "769e05d53a744d48b16de435f754e4328dc9646da9f0fdf605cc4ceed78aa92f", "ref_doc_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3"}, "5d519cfc-bf1c-4e06-a673-6dc744e8035b": {"doc_hash": "8d04ae33765b7c433c2875f26882493086b5aac50c77d2be0fb4828a51ad2c16", "ref_doc_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3"}, "9b9a34ee-890d-4680-96c6-08291072e510": {"doc_hash": "b9576dd3bcfbefe746d733bf707adecf23766c69887471d1eb50ff86212a7adf", "ref_doc_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3"}, "a0913454-32c5-4aaa-9f79-7b51c472ab48": {"doc_hash": "7842935a7d39c2972045de8e323142b1997a42511cc4471b0307452d3c113f81", "ref_doc_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3"}, "986bf8e8-4913-45bc-bdf3-c06dd56449c2": {"doc_hash": "88f96961198c7d5e95cd8571960716b59d8f84a4fc37d62730cd19aa45721cb1", "ref_doc_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3"}, "46060bcc-6f1d-44ce-a6f7-1918fc4f1fed": {"doc_hash": "11ebefeef83346d4c5ffbf743ef57c4918439bebde2ef207160a145131081752", "ref_doc_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3"}, "48c35703-9920-42c5-b53a-7a097340cac9": {"doc_hash": "fbd7f2c4ce6d39fb349a7d3da96714bbdc86c24ea53e6ea42fd2f9493eae48bc", "ref_doc_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3"}, "97709ca3-af29-48ca-9747-e35511cbd4de": {"doc_hash": "847f0a91cbc9dbba5234bba2498707cde646d4070759b767992f27f0062a7f31", "ref_doc_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3"}, "805402fb-4247-4a68-904f-542031e4c451": {"doc_hash": "d123867d506158dc0f28b23f634665eb4cceed87b6a0b2d12df71b5f2b56275b", "ref_doc_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3"}, "4af015e1-7c98-4d3c-8c15-4f2b6b080213": {"doc_hash": "ae6803f1591ff529fd4d77dbcc8e4562a0f137ed5061da6d273aace3bd7f34f3", "ref_doc_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3"}, "6ff29eca-6e3f-4a30-9db3-76e56ead421c": {"doc_hash": "14b2769b87c3c8158f9299bb1e90b46a98f33f755dc95acc2062064be091e0bf", "ref_doc_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3"}, "56b85528-65a0-42d8-be31-64dcd6523a70": {"doc_hash": "f21abec561024f8eac19b14b87a8ff56a927006330ceed25c21b0c28803eec1f", "ref_doc_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3"}, "e0ac23c6-63b2-4280-8b14-b87114224ee4": {"doc_hash": "eca4e0f52b0f0d02c73b24840d1285f265a2fbe52c36b920a03f467ee1efa1de", "ref_doc_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3"}, "8998f858-59f9-4878-b26a-519dfc30d18a": {"doc_hash": "80c157689e0b5debc9d5e20515192022677fecb70eafe45a8f0b62e044589c01", "ref_doc_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3"}, "323c6721-b9d4-4ff2-9359-ed912adde66f": {"doc_hash": "2b3ef16da055edecf1f2d6a9f358508a8d1e7a414da27773638d80f33a58b659", "ref_doc_id": "2d6d4703-f15a-41d4-9b36-b455938824f6"}, "14ad5045-be95-4ff8-bbfc-125da14a28d9": {"doc_hash": "bb262682255a683d7a9baa83b526d54d580c9cf46544dddfede8175cd6671b4c", "ref_doc_id": "2d6d4703-f15a-41d4-9b36-b455938824f6"}, "cc638e13-ca57-41b3-b18f-85c23a16001a": {"doc_hash": "9fcdcd6e81346fec19870abc3f250cb5cc87ff636e95f2a01a983cd5ec6904a3", "ref_doc_id": "2d6d4703-f15a-41d4-9b36-b455938824f6"}, "cfe1d74a-d20f-4001-a222-5568f3b77b28": {"doc_hash": "dfe025ea4f85a9d030a8e7e5755c9a07f9b83bb1076f0b078482a31131cc48ae", "ref_doc_id": "2d6d4703-f15a-41d4-9b36-b455938824f6"}, "e10ba924-2c64-46ad-9d62-61cefae5022a": {"doc_hash": "ad6138fffe632784048384aa5423a98b508724cb9fcf6097c3afb580021e2db8", "ref_doc_id": "2d6d4703-f15a-41d4-9b36-b455938824f6"}, "b6e95a98-e8e5-4fd4-867a-4cc5a95e7fb7": {"doc_hash": "7fe7bfe66d1dd80872712fe5d5fb75f1a1bb9c8bda57ea02768638ef477c2ac8", "ref_doc_id": "2d6d4703-f15a-41d4-9b36-b455938824f6"}, "e3af3c46-4db9-49da-95a6-8e417c96f1ca": {"doc_hash": "615cfcd7cf9a47468ec5a9fd040c463bb6601428bd86edc58329b2152fff9689", "ref_doc_id": "2d6d4703-f15a-41d4-9b36-b455938824f6"}, "2a628b0a-6d21-47fc-8a0b-7f7ab999c8bc": {"doc_hash": "6a8a7734ee0bcc815d9bbd7e96daf9f0bb6a6dda2847d176993ffd3ffc26c96b", "ref_doc_id": "2d6d4703-f15a-41d4-9b36-b455938824f6"}, "3a6cf700-64c7-4d3e-a15a-8283746cbb3e": {"doc_hash": "b41f5eba854fccdcd855bb3a4f15aae4d35a9c106e9deb325c8781533ff66881", "ref_doc_id": "2d6d4703-f15a-41d4-9b36-b455938824f6"}, "6092d867-37f3-4da3-83e9-024adacf4dc3": {"doc_hash": "6c8f4a8627e888d3cb7939a73e0e0b0f3c1491b74061abe0f750b4185f0e51b0", "ref_doc_id": "2d6d4703-f15a-41d4-9b36-b455938824f6"}, "50220c6d-4184-4db7-8317-c54f2530f230": {"doc_hash": "7c46fe5a0231e702dad75c5c72767f594b2e0ae0c5046a1c2389a0f8806e98f3", "ref_doc_id": "2d6d4703-f15a-41d4-9b36-b455938824f6"}, "0e6f3598-1f75-4c43-8198-3406a01623e6": {"doc_hash": "fb52afc3fbad5b0e216dca1fa74db98483fdfb2396c6a20a6db2555eaa770ca1", "ref_doc_id": "2d6d4703-f15a-41d4-9b36-b455938824f6"}, "76be207d-bb54-4c0e-a819-2ff07f84f5a5": {"doc_hash": "2fa75c3696bd844cc322730bf658fd293c487819f24289469a126955444a4bc5", "ref_doc_id": "2d6d4703-f15a-41d4-9b36-b455938824f6"}, "883c8cba-2a6d-48b4-98ac-02211a140eae": {"doc_hash": "19a688210aac0bfc5ffb038bceb077be83edc5b9d43742e144455fb6d3962afb", "ref_doc_id": "82569dcb-23d6-4577-baed-6323c76765ea"}, "f3618613-2d30-4a24-a6dd-3b21bc2c69fa": {"doc_hash": "4e419210353d3ee20605357a184abebc39292382f763893ab02636eda32ebc4f", "ref_doc_id": "82569dcb-23d6-4577-baed-6323c76765ea"}, "a77fa51f-9413-43e1-a600-67f0b804f708": {"doc_hash": "69ac91abc25403eae09e6ff8a755952e98443e334febcd75b32a11a205334201", "ref_doc_id": "82569dcb-23d6-4577-baed-6323c76765ea"}, "b864fc11-4aa6-4152-a231-4106015bd133": {"doc_hash": "7aabf4e3adce17f76f8807b0f49e91d8bfe8bf4e13a0336cc485ab01b3b4e8b2", "ref_doc_id": "82569dcb-23d6-4577-baed-6323c76765ea"}, "28b19a65-bbcd-46ea-9a28-997dcfb1cb92": {"doc_hash": "1a588242980a16c33d0e8035b3f8180eca04b3e2034d827c207bb8533f894721", "ref_doc_id": "82569dcb-23d6-4577-baed-6323c76765ea"}, "26d047f5-8d47-4472-904d-ede7a2ea58d5": {"doc_hash": "f43f048ba571d8a68c7bb5a00566bc058f090ee0e1ff3eb19408c19a2af93144", "ref_doc_id": "82569dcb-23d6-4577-baed-6323c76765ea"}, "8e53ef68-3986-4eef-a7e6-17f4dd76f2d5": {"doc_hash": "bbef0b2ada41c520b2de0db2bbfdeb3f7d90d841c64fc4a2e0d6d0b3ba5b5233", "ref_doc_id": "82569dcb-23d6-4577-baed-6323c76765ea"}, "fd1738d3-7246-46bf-8432-f8407638bd16": {"doc_hash": "e50a55e8f59b060ca108b72621e0ddf2a1b4f7d312f903f46eec25f6b6319985", "ref_doc_id": "82569dcb-23d6-4577-baed-6323c76765ea"}, "ccd06bf4-eb4b-4ca0-ae9f-a9335b625d6a": {"doc_hash": "f4dee5eae212e47d00bbd53a0e7b49a0a7996c5c4a8bcbe636c9e4ed577f6b92", "ref_doc_id": "82569dcb-23d6-4577-baed-6323c76765ea"}, "89c05dfe-cb96-4016-adfb-82c9fea8ee1b": {"doc_hash": "b460878c54e5b1962fa6733bbc25957a67d828ac6957880fa96b194e2cd20c42", "ref_doc_id": "82569dcb-23d6-4577-baed-6323c76765ea"}, "6b34a56a-227c-4d94-bcf9-697b67721093": {"doc_hash": "e22cd7d332cef592cd066640125412b630bf7d64f85611b3398a8d4c0a2f3be8", "ref_doc_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1"}, "dc9dfecc-eff1-4e2f-8b32-cced235246c2": {"doc_hash": "71ca244837e31b3dfa82927db92b23e555ff61b2744efc958738e5a6b76ba9a6", "ref_doc_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1"}, "0b3f3a06-4d14-4dcd-821d-7fb2d00097e6": {"doc_hash": "3f5162d6d79cbea27166bfffc6891864680f2bb64d324ba430fe9af20b656b07", "ref_doc_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1"}, "836f43f4-551b-4bb5-8cff-34115d7bf0ef": {"doc_hash": "b31394bda3b213081553d3dfcb777e68e4eb347852f03456e3873159ef9f6aee", "ref_doc_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1"}, "7813656b-3df4-4491-bdf9-ac57c909e464": {"doc_hash": "4b77e53ab8c15f8dd3b8c9d101d80583f6fa66d4ec76caef48598e74042a3928", "ref_doc_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1"}, "c1d36aa7-e929-4a23-9d01-012fa2492d00": {"doc_hash": "a1727f64079e0962e098649096541a43caf9da38a3e41d79feb048f0fd9974f6", "ref_doc_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1"}, "e121a8c4-2990-4a29-9807-184d42608a7e": {"doc_hash": "c60611c085ac945d096fbdaad663eb12d87b08fa5e5c2abbe8dbec90fd6b5a44", "ref_doc_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1"}, "0bc6f66d-2959-4dd7-a7ff-dbbb22d4eb17": {"doc_hash": "31f5d32c48bb1a5816072b76e9dfa6e89f06a29ed14d988d80b43a4f7e9b187f", "ref_doc_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1"}, "abdb4513-33cb-40a9-9228-81a8e59980ac": {"doc_hash": "83d2dcfb193d288ce659547a0d3180d073e925fe33c902196de607f996e1fcdd", "ref_doc_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1"}, "647e1f2c-ba7c-48d7-b0d4-433ccb43fbdd": {"doc_hash": "165fc01862293dc33284b14f1c1d1f22ba6be7d1fdd19b50268903eb5a1814f7", "ref_doc_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1"}, "f6115c24-2088-48a8-92b5-6af7164c0388": {"doc_hash": "2dcad4510a7bcb540c8f3ab5ac7b4dfc0714a8c9915833b2b797954d951e3fb7", "ref_doc_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1"}, "5ad0faae-8a59-4ba0-9243-7958eb21e5d9": {"doc_hash": "e7c7279fa19acc7c9a69e36ac0639f4f1f9457e400c89110bc7d06cc7ac9d0fa", "ref_doc_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1"}, "d5344355-c807-4bf5-b930-9d279d987236": {"doc_hash": "d5b5b81043ee13251b73c9fa9f18b6477365e26ce76f5be77906e0d2d703b333", "ref_doc_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1"}, "5fdfe646-5a52-4849-a24a-02257c53f74a": {"doc_hash": "266b362516909b99ed12878223271dce655a228210887d5ba5d9da8844ccc580", "ref_doc_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1"}, "63d9a34c-5359-4681-9f9c-35ab18e34f66": {"doc_hash": "75552c277c3e1e9e3499793faf25018448790e6853e34f84e1dd4116a1c57f06", "ref_doc_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1"}, "04d7f7e6-d8df-4114-acd0-c50b96066c62": {"doc_hash": "2be60053a2d248ad0a60b03a1e0304b1bd4c8d05efc726e8edf6bf7f3542c3f3", "ref_doc_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad"}, "2ebcaa7f-a71c-4581-9fa5-bde810bbb3aa": {"doc_hash": "9d0a4976530121db07ba05f3fd51c8dc643f20b921f8a647845ae311c70edf06", "ref_doc_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad"}, "c984d430-e376-4720-93fb-30b330251bba": {"doc_hash": "8cc9bda14c08614b3a07e585b759c0019a782b57283dd2c5f7b760b00a4b8cf1", "ref_doc_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad"}, "acab9c82-c85d-486b-a13d-0198006b8995": {"doc_hash": "f18eb92c120aaa9ea80eddef92e111454c32c543ebaac1d630de363d00badde8", "ref_doc_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad"}, "c0617e13-734e-47ae-970d-c4a9a26223ac": {"doc_hash": "3695125021a6a47d7603e66adc229c361a1eede73aba08dba6e3d395da29594e", "ref_doc_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad"}, "d0470b26-6bee-4172-9cfe-3097f5dacc6e": {"doc_hash": "098010ce55a94328c9b2cd09450d6d1ea040f3a9e1e20fd499a79c6aa5f662e5", "ref_doc_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad"}, "99fb4455-e3aa-46b5-ad00-69ac772d0f3c": {"doc_hash": "e07ca809a5980c2a83b862207846e52990ef17cf81f27d4fd75b0f8e2240f2e8", "ref_doc_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad"}, "746269ca-7e0e-4c59-8d14-3b0e79ea2427": {"doc_hash": "5a4f0d5ca58ab6e49f535872b32b92bb5f6f98f4b05bf538365c4fdd1033f9e3", "ref_doc_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad"}, "5c8a50ed-b952-4be8-9d5c-b7068eb79d15": {"doc_hash": "724e666667346913b1c85d8c89cc255348b7621ccd58fb30a619cc83fe2c5946", "ref_doc_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad"}, "1de9859f-5d33-495c-9ff5-ce4279c1dcb1": {"doc_hash": "79b99f6609753e342b4ab3215c80895a95d479ba17a291e52cbdd1845d8920be", "ref_doc_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad"}, "aca874eb-0210-4f88-abc9-4a489da741c5": {"doc_hash": "78f2cffed1a3fbd0b4a099162787c533481790d80cb45c83172a6ea0b3ece3a2", "ref_doc_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad"}, "35a5af8e-e596-461f-982c-4d2312a039a1": {"doc_hash": "7d6d0c9203a5154f32bf7fdb05f0b36cd75e8acf24e71aa3af69994573f5b26f", "ref_doc_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad"}, "36bba7f4-2410-430d-ba22-7c76bdc3f638": {"doc_hash": "ee2b31048b442873e72bab2c12847c9ed0944f64ef5b6575b4252b9e65897d4a", "ref_doc_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad"}, "f6751e9e-ef5c-46f6-8461-306bd2961104": {"doc_hash": "64aa9fef1dee392f8225c1fafa269a32d854cea2091b63bcd7b2c7c0b90225c7", "ref_doc_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad"}, "68f95953-57bf-4dfd-80f5-07998e1417a1": {"doc_hash": "44556ac9c4e92e0f3c12da139c1242be74c137d26d00f7aae8d91eef02cf6a95", "ref_doc_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad"}, "f4005cc8-bbe0-49d9-91ef-768a68541133": {"doc_hash": "a76aeab1974240bb6f25c6a175e61b6b3eae311947853283ac8272031c8fb459", "ref_doc_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad"}, "43f50844-054b-492f-942d-acb3350fbefc": {"doc_hash": "e3408ef76ef3c93780dd30227ad2995c05ebe2382bd56a57dde35984fa945fd7", "ref_doc_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2"}, "f621bc79-a3c3-4de2-a439-712b4c6aac99": {"doc_hash": "e9353f5ebb98a5328f4b7982a91a0a2a62d4668531ae6d8a2635ba0d605d8d9b", "ref_doc_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2"}, "6ced4a45-235e-4e17-9756-76a019b810c7": {"doc_hash": "3c6de005e0514ddbea4abf27ae99f8b5a3ced2c15b250c361ec16512edb0a83d", "ref_doc_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2"}, "f5ed7ba3-b428-48aa-945c-49f33ff9fc3e": {"doc_hash": "a699ef150532126228d326aa19c9b3381c1b2557a76efa0952dc70c651936d97", "ref_doc_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2"}, "9666f0af-601c-4cbc-9e4d-e0deaee4004b": {"doc_hash": "6c559af02bb2656ff73d00507e1304b0f041fae9c98933803ff1c5ab87498f6d", "ref_doc_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2"}, "e362da55-6dee-435a-81eb-add758acb95b": {"doc_hash": "aa9651be5fd895bbd97716a872d1293d02d74b72539b179523de31736b5e9bd8", "ref_doc_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2"}, "3eb9743b-8b07-481e-b59d-e88bb1bdc2bf": {"doc_hash": "88759c7a17d5903311debf9665f305819a633e60f2921ec8185acaba3d10c95a", "ref_doc_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2"}, "d77696b6-e1ff-442a-b61d-4aad50045aff": {"doc_hash": "3861311d7243071d004890eef98058ddd3dc469ffc0c7f62566f5158393dc922", "ref_doc_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2"}, "c5242756-de26-47c5-94bc-44dde66b0561": {"doc_hash": "51ab1f06545d5d3f3cf8f74357410885e2ecc4d1bba834480245b36b18c11614", "ref_doc_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2"}, "18248ca3-ba28-40fd-876f-8e3ec4212582": {"doc_hash": "1a1fb2acfb9c9a7c21e38af8869901fea02b32df61ffe8fd8e043aa127cb6603", "ref_doc_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2"}, "e9fb5076-864e-47c0-b894-055604a57a49": {"doc_hash": "de2e8137a8b617b3cf6ba4bdb1f35d071c728adb5d2ade50538e4b07fdb84713", "ref_doc_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2"}, "f0563b1b-1269-44ab-9e7a-9e5fca4458b5": {"doc_hash": "24356b771773552db38829d16c082b72ff0b5f3e81fbfa8010dab51150edc1ea", "ref_doc_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2"}, "7193e7dc-58cc-4716-b2fd-2d1fc87a641d": {"doc_hash": "c8647b1ee32b495864db3fb9914bab8ab243f6cbc5b252927225ea0464a156c5", "ref_doc_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2"}, "95504dfc-80ff-4330-a72c-2bced8191393": {"doc_hash": "431e28cabcd97d32a8cd5ba6b72360565dd66d9bc52acdeb7126a1a8d85b6b83", "ref_doc_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2"}, "f1340b64-5420-4948-9ad9-6393298eabd8": {"doc_hash": "271d1aa28a45ce2c43b2871274612b87e203372fd32494c8d5de2cccff40523e", "ref_doc_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e"}, "8621edb8-0760-4cdf-9a56-a18908ae090f": {"doc_hash": "ee4e6b3efe30947ba3f039d4002fb0700f253889a3bb641efb82716380c29efe", "ref_doc_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e"}, "4cd265f5-b681-4b82-895f-f89cb2be19cc": {"doc_hash": "0e54b508888b7ebb02b1e7e993ff7e2e916965b0f70306749ca178a2a2f27e1f", "ref_doc_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e"}, "3e5c8e49-4007-4aca-98a2-09a288c1a90a": {"doc_hash": "6417dd1253b56aa8dc42023d2b0ac595dacac86f26104485fa3db2b5286b3281", "ref_doc_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e"}, "8d2d620d-e4ca-46a5-b9a8-9af7c3393959": {"doc_hash": "7e86bbf18661640dc96259596e97565b56b4d80585cc6829e6931162e367d89d", "ref_doc_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e"}, "969a327c-df6d-43bb-8655-5daf7039e75f": {"doc_hash": "e2d6a9ffe8e54e78c7f1a678604661e401917bdac2f6a2bb7e08677bea175699", "ref_doc_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e"}, "24a7bc2c-bd50-4ca3-9db3-2c1f6e9cd3bb": {"doc_hash": "98588cf9fc083a06b01d343d7e791677444f8ba7a6233a1f71af33705a56e864", "ref_doc_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e"}, "26931cf2-0cf2-484b-a76a-9a2f2c43ba8e": {"doc_hash": "902b245dbf09bc61ff1a4601a89c81a28db108bcd867a33dd5a1004de8f582f8", "ref_doc_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e"}, "d4091503-39ec-4971-9906-62c72ce7b27d": {"doc_hash": "bcd243c0c7d3ceffc7caa8ddaafc4e08fa181c18eb7ccc9e23e0792ce2d77b69", "ref_doc_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e"}, "87e0774b-9acb-44e2-bab1-c7ad6265dea7": {"doc_hash": "a79558b8c99c1a035c85dc005f961a2268f0c89ebc75843863e2aa1fc233d80f", "ref_doc_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e"}, "2ecf66ed-0607-4e23-ae9c-dfbab7b1cd3e": {"doc_hash": "408c62036577df709eb7293c5d55ade711e92e41cc7f3ec5497f4d0cb35a349c", "ref_doc_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e"}, "971f1641-8cfd-4d7d-9794-6ef485058d15": {"doc_hash": "292a6c9124fb872eaf0c8ad8b81a926183326df83813fffd275f55f70a43a1bc", "ref_doc_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e"}, "50c5b40d-57da-4c64-9744-c6e2650dfe23": {"doc_hash": "4405cd5cc187a6fcc9f5ec6d8fc5950b28867dba1f36cb925a245409006edd10", "ref_doc_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e"}, "e00b94e0-7373-4313-9913-dad5d7b9ac6c": {"doc_hash": "aa3fba98e209a7b597facaae936a50bb0957f38d91b8531a4826c6167b6ea1c2", "ref_doc_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e"}, "f2b734de-2263-42da-9e8a-dbef54b8285b": {"doc_hash": "87446e0b30c2fffc5def3781f93f97b83fc4ff5e4cedb26ffd8986b06e3bbb1b", "ref_doc_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e"}, "816c37a6-ad7d-4e5a-8c47-f2ba4a2b551a": {"doc_hash": "34afc950e63c551b2e429978cdce94b9d30d0f4927b140ee32b907c5715d3d45", "ref_doc_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e"}, "bbaaae45-f6aa-4e2b-a980-1bbec6ef76ff": {"doc_hash": "bc213584f40cf761c70a82c678416ed3380398ca97b7d90c69a5694e7ed73a13", "ref_doc_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e"}, "bdb0f1b2-e9fd-4595-b93e-ef551b0c2aed": {"doc_hash": "46b567f52a3da4cbdba0a50789449dc5999c60beac4e9b4aaea0f698685f6fd7", "ref_doc_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e"}, "3733d4a4-aa62-4509-b31c-3421108ccea1": {"doc_hash": "56bcffb43519cbdfd7d78e191255b3925b4168ea4e59bb1808ac7e88c269db9b", "ref_doc_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643"}, "1b90c6b6-2bef-4321-ae32-ce7239f3a15b": {"doc_hash": "52a5ad6e50052c5a9d7ef1a2e7950b900123b2ebf3e7cc5c3d963076803a9763", "ref_doc_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643"}, "332609b6-6cc3-4580-ae50-def58c52b9b8": {"doc_hash": "66051016c3e2aeddd7bcab3e648f585a606b77bf9ef35abe2fd5dcaaaf62864e", "ref_doc_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643"}, "8740f887-2e78-4cba-acbe-a8229f0792d7": {"doc_hash": "a083e6a4231acb3b11b7361f154337d0ebde34482f7446d07182273f4e795e4b", "ref_doc_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643"}, "4ba668ab-0283-43a8-b628-40ef6a79be8d": {"doc_hash": "fd298bfafb98dfd39f829919e1315d9b1426af0d805a9657be1c5741fd32eab0", "ref_doc_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643"}, "035a0640-6c10-4cf9-8dee-3b792537ff60": {"doc_hash": "6197ce59871cf95efc8ec492fb0b54cb0dba4bd725a5336d0144b1de0bee427f", "ref_doc_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643"}, "773717ef-e388-4755-acc0-78a530477037": {"doc_hash": "ea6a2aa2c9fd4decbd36725b984597afb01ad86ea6f179cf369330b8147f6964", "ref_doc_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643"}, "aa3f9ded-8b34-4766-9077-4cfc27e09e01": {"doc_hash": "b748ff73d1a777f57f50afcd83d654ed862f7249c6eb938aeaea01c824f45f8a", "ref_doc_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643"}, "5948ad4b-b6eb-41e7-b7a9-05c54feeaae0": {"doc_hash": "27e13167378505bb38e3d8ab56f96fd63cfe8ddf6d37590cf4982a5ecc36f14f", "ref_doc_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643"}, "0ad3c605-bfe0-47d8-82a9-c7592fdeb73f": {"doc_hash": "fb2a50c6de40ae37dbafe7b8d698960c63f03829e828c726d790258e94d6361c", "ref_doc_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643"}, "b8971594-96e9-41ad-92b0-12d2798e8912": {"doc_hash": "e2a0db999263cd9d07b6181c83e1a70dbcae176b75e07d5ac52cf8eca4577b13", "ref_doc_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643"}, "58e60fc0-4848-46f2-b261-d6b7be4f5d22": {"doc_hash": "6b0f0d203e63b78074f6e7e91e318d04d5968cc91d6b6418bf33bed19a21d3cc", "ref_doc_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc"}, "dc157a10-f735-4b29-87cb-1b2cc43e1bfd": {"doc_hash": "78bdebd348e84e310c087703582329e8789a5bb7cf2e4bdaa71b7b09ae55ecb6", "ref_doc_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc"}, "798accf5-d179-4622-b47e-629bcc3b03fe": {"doc_hash": "14440ee6ded749371af54e8636b64f2ec378730a631bbf865c8cb822a90c2371", "ref_doc_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc"}, "fec9dc9a-e377-4634-8427-1ef0015614c6": {"doc_hash": "7a62d950c3979d925a98b0b9d913ab6972a131dc1ddbe5122acfc3f49340c0ad", "ref_doc_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc"}, "149fbc9c-44ea-4be7-baf4-d05f5c32fe89": {"doc_hash": "6b5db090fd6435cc826e530c868771ea59e27bc547d48fb1be726dcffd2fe3e2", "ref_doc_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc"}, "154961d0-1fc1-4844-91f9-7dfb43fd2355": {"doc_hash": "bd5ec21bffe02fe6e2a5a287fdac628db5820075f4ef5a980d23e1720e549ddd", "ref_doc_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc"}, "addcdd2f-35e5-4430-bab0-4e38db2f9491": {"doc_hash": "3e374e1e56052b0717c8907692ffe91f22d71d237ff6602d2fe69b2cc41f9642", "ref_doc_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc"}, "607037a1-d640-4113-b469-aa0b97094622": {"doc_hash": "da06b65eb3643b75174c6bdfb1c642c514bad63632e7fc0877b608a7ee1eab3d", "ref_doc_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc"}, "48131f00-78b7-4d03-ad3d-13b4867dd447": {"doc_hash": "63884f35448541d6e1d5aae708a08a2878adc27df9d35633f2bf6d85671ee20e", "ref_doc_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc"}, "5f1bd0a5-55c3-47a3-8b84-8373111c4d2b": {"doc_hash": "94d7dc20b2ee31def7e4bdbbf9031c4a861141620e06127306792218f3f360b6", "ref_doc_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc"}, "da64bd59-0a49-4f94-9061-9280d3ea395b": {"doc_hash": "da073d2300e5fcf1187298c493f277ce342ce1eaf13f494809eeac59404bea12", "ref_doc_id": "a26995a6-144e-4b45-aafd-f1ffbdd56f4e"}, "b7e875f8-361b-4ec1-be90-2abbf7948880": {"doc_hash": "33f348f253204a26cff151ad63636596b95427032ed3cce93a73e437850e3df6", "ref_doc_id": "a26995a6-144e-4b45-aafd-f1ffbdd56f4e"}, "e645f476-287f-40fb-95dd-7881c1ba5b41": {"doc_hash": "edd8c1a5ca1d3c48a1a2f8debd10812acba992a304886787da9a8e0b27228029", "ref_doc_id": "a26995a6-144e-4b45-aafd-f1ffbdd56f4e"}, "4b92dc60-fbd1-471b-ae70-b01c5a5ca422": {"doc_hash": "d15a4ba0eb663a2cdcef80d3edfcdc4e771b484552ecedf16e6af356a4f04f88", "ref_doc_id": "a26995a6-144e-4b45-aafd-f1ffbdd56f4e"}, "606ea042-2e61-48a6-b06b-96cc5fe9d1cb": {"doc_hash": "1202563c2203a1500f63887bc665e256e45c9043ef851e491824fd55a18b23ff", "ref_doc_id": "a26995a6-144e-4b45-aafd-f1ffbdd56f4e"}, "de5853d4-7893-459b-ba8f-abaddd1a366a": {"doc_hash": "4c8e8b9ad7f243e04f0bea5a18769de1035228ce119c02a72d50ebf93ba03d90", "ref_doc_id": "a26995a6-144e-4b45-aafd-f1ffbdd56f4e"}, "7f7c10cb-efc6-4d57-9ab9-fc3f8441e878": {"doc_hash": "5354bd823a285a96ba0afdb0a8e4ac038ea360eae73f0f6815ff831521e46dfa", "ref_doc_id": "a26995a6-144e-4b45-aafd-f1ffbdd56f4e"}, "d95561c7-0a05-4e06-b029-dcf27e7ae531": {"doc_hash": "eb732c36612bed450e974292f39b1da41210bde688aab8ff512463c16c6bede1", "ref_doc_id": "2ee4cf9f-afe0-41ab-b5d2-70d8df27e172"}, "df61960a-d283-41c7-83cf-9f77e76c22e5": {"doc_hash": "900896e3dc8600714cf7597cf7e6cf57ad47724e2a16540b0aab3ed660c03a35", "ref_doc_id": "2ee4cf9f-afe0-41ab-b5d2-70d8df27e172"}, "533a6168-a76a-4394-bcef-56bee79e0846": {"doc_hash": "af2d2fb7c5ff2a1f831fb4bb3d7bdad88fc55be1134e8838d5bbd41aad06f19c", "ref_doc_id": "2ee4cf9f-afe0-41ab-b5d2-70d8df27e172"}, "5ca58590-93f3-40cf-8938-a97260d9fec0": {"doc_hash": "2af25ffabdf2ebae9a5e192e674f6500489b185e5ae7d2c39cc9e9449b75e767", "ref_doc_id": "2ee4cf9f-afe0-41ab-b5d2-70d8df27e172"}, "c2e2929c-217f-4299-adf8-af6986055bde": {"doc_hash": "8dbe57f00321b91f4b30eb9a5752854be2d8e74034c7166316ca6ccaa7c9b49f", "ref_doc_id": "2ee4cf9f-afe0-41ab-b5d2-70d8df27e172"}, "dbc05bff-934b-48b3-8b25-d1fa3578cc56": {"doc_hash": "953f27c8f9fd70049ad497b5e516ecdd9d1b4d1823eed6825895c17e519e5ece", "ref_doc_id": "2ee4cf9f-afe0-41ab-b5d2-70d8df27e172"}, "1986e4e9-c92c-4b9e-8cc6-2e37de112e6e": {"doc_hash": "5465d32968c7cf5c045f6c30d5e892b7952ec5e599197a12a2e03a4735891ead", "ref_doc_id": "2ee4cf9f-afe0-41ab-b5d2-70d8df27e172"}, "a0e71c7b-d6a6-41aa-bdd4-5d365ebe763c": {"doc_hash": "577dcf8b7d97ac4f534bcd32998a1927effeaefc58f7c45bda676c856a9d5091", "ref_doc_id": "c8b90a72-370e-4b48-9eec-86788c3a9950"}, "1a841a6d-cb85-481e-b1e9-66577646687c": {"doc_hash": "dda71ca04056974f80fe6e7841225efa8a192dae9d456ad5c37a544f673318b2", "ref_doc_id": "c8b90a72-370e-4b48-9eec-86788c3a9950"}, "2c008bf4-b108-4755-9b26-818fb5d18143": {"doc_hash": "33d3eb23233c7466bb478f2c61fb6705ec8b9709b21a64c1379a9bf61fd9fcdf", "ref_doc_id": "c8b90a72-370e-4b48-9eec-86788c3a9950"}, "365a14c3-41ab-4589-af83-4b37ce61a6da": {"doc_hash": "bf2f68f561f55d44c572392743ebbfcfc955534d113a245ec0acb843e47eff52", "ref_doc_id": "c8b90a72-370e-4b48-9eec-86788c3a9950"}, "86a656b6-a94a-41d4-93a1-8396a68209ea": {"doc_hash": "aa1bbad51d3dd5a42def53a64c332b9688bf043a5ac2e5fd4d065402810065fd", "ref_doc_id": "c8b90a72-370e-4b48-9eec-86788c3a9950"}, "8135295e-4471-4a6d-be68-867d936baf9d": {"doc_hash": "ad8ab1e04154445da550c9f6748ffcfe8ef5c32902eaa029ab6842ad2c565dc2", "ref_doc_id": "c8b90a72-370e-4b48-9eec-86788c3a9950"}, "8be263b6-84cb-4081-b994-c841e7a1e699": {"doc_hash": "62918d0d7623204c004b77e6e0fce7a2a819d5f7df4b1c4d9a10e989df8d04db", "ref_doc_id": "c8b90a72-370e-4b48-9eec-86788c3a9950"}, "4d73fb3e-8e6a-4abc-bdb3-103b4a4b59f4": {"doc_hash": "8cb34106a6b8493ee5be65a79c56d09e8a30cad96d82b506b5083cad378b55fc", "ref_doc_id": "c8b90a72-370e-4b48-9eec-86788c3a9950"}, "4dd0b39d-6ab0-41a9-b2e7-adb70c60cdad": {"doc_hash": "8c8480f9b8190f3679bdbe9a797673161e2f6d531e4d6a1eb91fc9800978164f", "ref_doc_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257"}, "7790ce7d-a65b-479a-b893-c371443490f0": {"doc_hash": "49ed90cbc3f7cdac1619ca3e3e43835fa390a868f8d770bc86094c2ce05cc840", "ref_doc_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257"}, "f03c37e6-8d89-4b22-8722-cf4d7785a36c": {"doc_hash": "14eb290fd8fe394f7e11cb8fa9990a6d38cd5a1f715bd6617c858c2301dcdeb3", "ref_doc_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257"}, "8c3de202-5afa-46ca-9552-6f870f41d8f5": {"doc_hash": "3e0d8c27daa2879839621dc8b7617ec685b04631882fec50d1f8e4a26a57ef69", "ref_doc_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257"}, "060808b7-54c3-414a-85d5-f18be4174fbe": {"doc_hash": "e99cf35d64020677ee9f9fe9b72164b31e31798ab75087670595a058df24a8b2", "ref_doc_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257"}, "04848a13-e3aa-4da8-8f1a-5e8b05500948": {"doc_hash": "0ecb95e5a551fcbfb41c9056c639aa8f8fa10e37b62499beff27f83e43dfd322", "ref_doc_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257"}, "ec98a1e6-59e8-4bff-b8f0-2b80e8d1006c": {"doc_hash": "07fabebe7a95b699939ec53fb01513879cb6dcb90e197f23f9dbbb4ce8a92907", "ref_doc_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257"}, "18d58403-5c14-4f16-b452-c39658460db9": {"doc_hash": "8e725c5d43d9e067163cec66f3a54752dd59a993d4dc984c190bab2e12a4a1dc", "ref_doc_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257"}, "26acad6b-675f-4242-a7bd-9644e0e45bb7": {"doc_hash": "07298041631b44a8bc5a2a2ab74ea5c1e7df751068f435c5b60724b22fa976af", "ref_doc_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257"}, "516f2eca-6080-4df5-8687-8f767fe916ba": {"doc_hash": "68cd6afb0f3eb5e7cf1326b3fb8973dba277e0be98da6e55dc1151d061b6f50c", "ref_doc_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257"}, "e6f35f87-5756-4c04-8ef0-2bf5f4a96673": {"doc_hash": "154638861787457cf9aebdd170bda3bf0cec78bdca0a635832534d3a73ac6e37", "ref_doc_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257"}, "ef8e40ea-e9ae-462e-b2d1-faa9f8108d43": {"doc_hash": "87ee0581f31c22318b210aa6fd4fcbdd0636d2bbbd2b74f530290842d189adb0", "ref_doc_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257"}, "16baf198-51f0-4086-8cdf-eacf38e61052": {"doc_hash": "633531ea400d8768d3c0a0ab86defc943fea03afbae257af20e11a535d996945", "ref_doc_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257"}, "31f30d0c-dcbb-4175-979c-e30b2caa9838": {"doc_hash": "6362c4dbde8cffeb124ee3da3988a49e416f7a45cb711762065758dc632ccb8d", "ref_doc_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49"}, "40cb7b9e-57cf-42d8-8b0b-f1db386b382a": {"doc_hash": "d189a4fe5a165e589a467c6be652dc92f6079c9964562e02b587602ba3a6d5dd", "ref_doc_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49"}, "64157c44-a4eb-49d8-83a5-823ef37c5ed1": {"doc_hash": "027ec475e5bbd4f6acbf0e8dca46f9e7a68b090ba1679ce2e7e50b9bd7cc5607", "ref_doc_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49"}, "4e4a1f1f-f72b-475f-a5c4-742b97b8fcf1": {"doc_hash": "24f72967149a136a85131b8167acd73798d3a7e5fa02749cbd492d519f364a23", "ref_doc_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49"}, "ea97f991-589f-4986-84fb-f960599fda6d": {"doc_hash": "4eff5f05a945299f3440869e9b00f54dcaaf16548e06d08a0d1415e8fb2539df", "ref_doc_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49"}, "7306e74d-155e-4d86-bcd4-1c9a8e96953c": {"doc_hash": "65eb8d0597d91ca9dee700631d9b0d312c93d4bd65dcc9bb9cff687832b232bb", "ref_doc_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49"}, "a9263a29-2d49-45df-a5bf-4e8363562db4": {"doc_hash": "16a386f5c9df443ab1c91dcdc6628cd16b4dcdb3af4a44098d40a60b6d89f9b8", "ref_doc_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49"}, "89dcb929-a85d-4f6b-9608-3ed30a1040ab": {"doc_hash": "85d3d0b72e04f231d12ca47e315c8c6197eadf6b53279fa1736ead8d17647495", "ref_doc_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49"}, "9647d7ea-2ed5-4e99-b505-65554b8276b4": {"doc_hash": "df3dbd65ba5260df9a3504dda4d2fafd69bd413924666e96c36b4a0d39ec7a48", "ref_doc_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49"}, "9d833549-9152-4cc9-87f1-2ac2d37006e6": {"doc_hash": "cff3d7d5e82ee8d4dd41251f8e728ab44381b898a6b1733f9b6accb4eeee6539", "ref_doc_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49"}}, "docstore/data": {"f94d1948-ee6c-464e-b5bb-c14f88e3ca0c": {"__data__": {"id_": "f94d1948-ee6c-464e-b5bb-c14f88e3ca0c", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f06dacdd-bb77-4a42-896a-75a3736b2b39", "node_type": "1", "metadata": {}, "hash": "a3f9601ae773f692618ad8d284a8eb4c76762cb7036029e263573b27b01bffd5", "class_name": "RelatedNodeInfo"}}, "text": "Join our upcoming webinar \u00e2\u0080\u009cDeriving Business Value from LLMs and RAGs.\u00e2\u0080\u009d\n\n[Register\nnow](https://www.superannotate.com/webinar?utm_source=alert_bar&utm_medium=banner&utm_campaign=website_alert_bar)\n\n[![superannotate logo](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/6127731d30dc5270fa629b99_logoDark.svg)![](https://assets-\nglobal.website-\nfiles.com/612770618d97595db63a9470/612dea788938e85714b0d752_Logo_Footer.svg)](/)\n\nPlatform\n\n[\u00ee\u00a4\u009aFineTuneCreate top-quality training data across all data\ntypes.](/annotation-tool)\n\n[LLM![](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/llms)[Image![](https://assets-\nglobal.website-\nfiles.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/image-\nannotation-tool)[Video!", "start_char_idx": 0, "end_char_idx": 825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f06dacdd-bb77-4a42-896a-75a3736b2b39": {"__data__": {"id_": "f06dacdd-bb77-4a42-896a-75a3736b2b39", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f94d1948-ee6c-464e-b5bb-c14f88e3ca0c", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "8bb1d0cd499db38799f407140a913d86d09149e710883ba27c2e875e73cb1a11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4382cf6-99dd-4b2b-b332-212af5327c13", "node_type": "1", "metadata": {}, "hash": "31ed93c8250299653c07ff0c788ed88a127993ddfc83c864db7f2e0ab61bda4a", "class_name": "RelatedNodeInfo"}}, "text": "[](https://assets-\nglobal.website-\nfiles.com/612770618d97595db63a9470/612dea788938e85714b0d752_Logo_Footer.svg)](/)\n\nPlatform\n\n[\u00ee\u00a4\u009aFineTuneCreate top-quality training data across all data\ntypes.](/annotation-tool)\n\n[LLM![](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/llms)[Image![](https://assets-\nglobal.website-\nfiles.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/image-\nannotation-tool)[Video![](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/video-\nannotation)[Text![](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/text-\nannotation)[Audio!", "start_char_idx": 330, "end_char_idx": 1105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4382cf6-99dd-4b2b-b332-212af5327c13": {"__data__": {"id_": "d4382cf6-99dd-4b2b-b332-212af5327c13", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f06dacdd-bb77-4a42-896a-75a3736b2b39", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "53c68ed8b745181350ba5b57e409a8dd8fc12ffe10e92459f77649d8e995cc3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "741f7e47-9921-4edd-9cd0-7ff58e7f9c67", "node_type": "1", "metadata": {}, "hash": "39285adfc56fdb965f0758a944d5b75b4159336f8efa82a482d147cbe02ac514", "class_name": "RelatedNodeInfo"}}, "text": "[](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/llms)[Image![](https://assets-\nglobal.website-\nfiles.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/image-\nannotation-tool)[Video![](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/video-\nannotation)[Text![](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/text-\nannotation)[Audio![](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/audio-\nannotation)\n\n[\u00ee\u00a4\u0098ExploreManage, version, and debug your data and create more accurate\ndatasets faster.", "start_char_idx": 550, "end_char_idx": 1330, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "741f7e47-9921-4edd-9cd0-7ff58e7f9c67": {"__data__": {"id_": "741f7e47-9921-4edd-9cd0-7ff58e7f9c67", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4382cf6-99dd-4b2b-b332-212af5327c13", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "7fd59e624730f40b630443527958d6f1551bfd01c7d0c174fc0831718af24078", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23dcb314-9ad3-4007-8243-af7bf1df0e9a", "node_type": "1", "metadata": {}, "hash": "592a1161116ca8ff9b644ba5e08a3999cf606585f36683570a21b652916018b8", "class_name": "RelatedNodeInfo"}}, "text": "[](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/video-\nannotation)[Text![](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/text-\nannotation)[Audio![](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/audio-\nannotation)\n\n[\u00ee\u00a4\u0098ExploreManage, version, and debug your data and create more accurate\ndatasets faster.](/data-curation)[\u00ee\u00a4\u0099OrchestrateCreate CI/CD AI pipelines\nusing our built-in neural networks, Python SDK, webhooks, and advanced\norchestration.](/orchestrate)[\u00ee\u00a4\u0084WForceAccess a global marketplace of 400+\nvetted annotation service teams.", "start_char_idx": 825, "end_char_idx": 1566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23dcb314-9ad3-4007-8243-af7bf1df0e9a": {"__data__": {"id_": "23dcb314-9ad3-4007-8243-af7bf1df0e9a", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "741f7e47-9921-4edd-9cd0-7ff58e7f9c67", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "55dd7a559a869741a954593ca973e00a57d53dd3a95006d0ea8bd31ec7db88ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0dd1f595-0a51-49a1-99fb-63ea3d1ae3b7", "node_type": "1", "metadata": {}, "hash": "987483f2417e71ccd5bbf587a1a5c695eaa9af922bc25fbe9616dd895bc9c6bc", "class_name": "RelatedNodeInfo"}}, "text": "[](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/text-\nannotation)[Audio![](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/64b566d7be24c4d018794a47_arrow_nav_icon.svg)](/audio-\nannotation)\n\n[\u00ee\u00a4\u0098ExploreManage, version, and debug your data and create more accurate\ndatasets faster.](/data-curation)[\u00ee\u00a4\u0099OrchestrateCreate CI/CD AI pipelines\nusing our built-in neural networks, Python SDK, webhooks, and advanced\norchestration.](/orchestrate)[\u00ee\u00a4\u0084WForceAccess a global marketplace of 400+\nvetted annotation service teams.](/annotation-services)\n\nLLMs & GenAI\n\n[\u00ee\u00a4 LLM & GenAI Software](/llms)[\u00ee\u00a4\u0084LLM Expert Workforce](/llms#hire-\nexperts)[\u00ee\u00a4\u009bFree Playground](/llms-genai-playground)\n\nSolutions\n\nIndustries and use cases\n\n[\u00ee\u00a4 LLMs & GenAI](/llms-\ngenai)[\u00ee\u00a4\u0082Agriculture](/agriculture)[\u00ee\u00a4\u008aHealthcare](/healthcare)[\u00ee\u00a4\u008bInsurance](/insurance)[\u00ee\u00a4\u0095Sports](/sports)[\u00ee\u00a4\u0086Autonomous\ndriving](/autonomous-driving)[\u00ee\u00a4\u0093Robotics](/robotics)[\u00ee\u00a4\u0081Aerial\nimagery](/aerial-imagery)[\u00ee\u00a4\u008fNLP](/nlp)[\u00ee\u00a4\u0094Security and\nsurveillance](/security)\n\n\u00e2\u0080\u00a6 and many more\n\nCase studies\n\n[\u00ee\u00a4\u0087Hinge Health](https://www.", "start_char_idx": 965, "end_char_idx": 2125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0dd1f595-0a51-49a1-99fb-63ea3d1ae3b7": {"__data__": {"id_": "0dd1f595-0a51-49a1-99fb-63ea3d1ae3b7", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23dcb314-9ad3-4007-8243-af7bf1df0e9a", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "8725a256b4888e2480612ffbdb7ccac7e2bf5f4e958c7d24d7b6477784c5ca7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54373d6a-6875-464b-8364-d3581dfe5519", "node_type": "1", "metadata": {}, "hash": "d30696ea7314ae5b2e732eb8ecf079c3baa1ef81119bd3a9e92de21cd15181d0", "class_name": "RelatedNodeInfo"}}, "text": "superannotate.com/blog/hinge-health-case-\nstudy)[\u00ee\u00a4\u0087OneCup AI](https://www.superannotate.com/blog/onecup-case-\nstudy)[\u00ee\u00a4\u0087Percepto](https://www.superannotate.com/blog/how-superannotate-\nhelped-percepto-cut-the-time-to-complete-annotation-projects)[\u00ee\u00a4\u0087Orsi\nAcademy](https://www.superannotate.com/blog/orsi-case-study)\n\nResources\n\nCompany\n\n[\u00ee\u00a4\u0087Blog](/blog)[\u00ee\u00a4\u0090Podcast](/podcast)[\u00ee\u00a4\u0096Webinar](/webinar)[\u00ee\u00a4\u0088Careers](/careers)[\u00ee\u00a4\u0080About\nus](/company)\n\nPlatform\n\n[\u00ee\u00a4\u0089Documentation](https://doc.superannotate.com/docs)[\u00ee\u00a4\u008eWhat\u00e2\u0080\u0099s\nnew](https://www.superannotate.com/blog-category/product)[\u00ee\u00a4\u0097Python\nSDK](https://doc.superannotate.com/docs/python-sdk)[\u00ee\u00a4\u008cIntegrations and\nSecurity](/security-at-superannotate)\n\n[Pricing](/pricing)\n\n[Sign\nIn](https://auth.superannotate.com/login?__hstc=17958374.", "start_char_idx": 2125, "end_char_idx": 2909, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54373d6a-6875-464b-8364-d3581dfe5519": {"__data__": {"id_": "54373d6a-6875-464b-8364-d3581dfe5519", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0dd1f595-0a51-49a1-99fb-63ea3d1ae3b7", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "062fa8642d089009877fc9c3d1afbbb230cf92e0e2f8e5a1d63e9a18b61396c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "384b183a-eb02-4243-83a8-330f6a6eadc2", "node_type": "1", "metadata": {}, "hash": "532a158520c4a8d110744cdde25b1b6cc37ce0136a53084b664896cddf80bb61", "class_name": "RelatedNodeInfo"}}, "text": "superannotate.com/blog/orsi-case-study)\n\nResources\n\nCompany\n\n[\u00ee\u00a4\u0087Blog](/blog)[\u00ee\u00a4\u0090Podcast](/podcast)[\u00ee\u00a4\u0096Webinar](/webinar)[\u00ee\u00a4\u0088Careers](/careers)[\u00ee\u00a4\u0080About\nus](/company)\n\nPlatform\n\n[\u00ee\u00a4\u0089Documentation](https://doc.superannotate.com/docs)[\u00ee\u00a4\u008eWhat\u00e2\u0080\u0099s\nnew](https://www.superannotate.com/blog-category/product)[\u00ee\u00a4\u0097Python\nSDK](https://doc.superannotate.com/docs/python-sdk)[\u00ee\u00a4\u008cIntegrations and\nSecurity](/security-at-superannotate)\n\n[Pricing](/pricing)\n\n[Sign\nIn](https://auth.superannotate.com/login?__hstc=17958374.8b8562dd440376db6b7733af5a05b8a4.1672215330596.1672215330596.1672215330596.1&__hssc=17958374.7.1672215330597&__hsfp=357064021)[Request\nDemo](/request-demo)\n\n[LLM](/blog-category/llm)\n\n# Fine-tuning large language models (LLMs) in 2024\n\nFebruary 5,", "start_char_idx": 2401, "end_char_idx": 3156, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "384b183a-eb02-4243-83a8-330f6a6eadc2": {"__data__": {"id_": "384b183a-eb02-4243-83a8-330f6a6eadc2", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54373d6a-6875-464b-8364-d3581dfe5519", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "1e824cdd675cb7b105e27243aaaecc2af7ee8f00ce23854d769e89f4c3099d67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c92f851-0657-4251-9eaa-e7646d2ce529", "node_type": "1", "metadata": {}, "hash": "c99a5623c45372100fde51c688d0f3f798d551a3faad27609466080d4baf18d8", "class_name": "RelatedNodeInfo"}}, "text": "com/docs)[\u00ee\u00a4\u008eWhat\u00e2\u0080\u0099s\nnew](https://www.superannotate.com/blog-category/product)[\u00ee\u00a4\u0097Python\nSDK](https://doc.superannotate.com/docs/python-sdk)[\u00ee\u00a4\u008cIntegrations and\nSecurity](/security-at-superannotate)\n\n[Pricing](/pricing)\n\n[Sign\nIn](https://auth.superannotate.com/login?__hstc=17958374.8b8562dd440376db6b7733af5a05b8a4.1672215330596.1672215330596.1672215330596.1&__hssc=17958374.7.1672215330597&__hsfp=357064021)[Request\nDemo](/request-demo)\n\n[LLM](/blog-category/llm)\n\n# Fine-tuning large language models (LLMs) in 2024\n\nFebruary 5, 2024\n\n\u00e2\u0080\u00a2\n\n12 min\n\n![llm fine tuning](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/653775b2bdff592188a789dd_large-language-\nmodels-llm-fine-tuning.webp)\n\n[!", "start_char_idx": 2624, "end_char_idx": 3334, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c92f851-0657-4251-9eaa-e7646d2ce529": {"__data__": {"id_": "0c92f851-0657-4251-9eaa-e7646d2ce529", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "384b183a-eb02-4243-83a8-330f6a6eadc2", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "bc416026870d660aea44038a5a6dd49c23619fce2c3113d276b87f755afd5ca8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee171899-adb4-4797-8477-328097365f8a", "node_type": "1", "metadata": {}, "hash": "b2f3d3701b9047d5a31fea5654ee8e69362365922681b8aa7f691beb5bfb6c7c", "class_name": "RelatedNodeInfo"}}, "text": "superannotate.com/login?__hstc=17958374.8b8562dd440376db6b7733af5a05b8a4.1672215330596.1672215330596.1672215330596.1&__hssc=17958374.7.1672215330597&__hsfp=357064021)[Request\nDemo](/request-demo)\n\n[LLM](/blog-category/llm)\n\n# Fine-tuning large language models (LLMs) in 2024\n\nFebruary 5, 2024\n\n\u00e2\u0080\u00a2\n\n12 min\n\n![llm fine tuning](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/653775b2bdff592188a789dd_large-language-\nmodels-llm-fine-tuning.webp)\n\n[![](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/617674c3fb077ab741e5847c_ReadMore-\nArrow.svg)Back to Blog](/blog)\n\n#### Contents\n\nTable of content Item\n\nIt\u00e2\u0080\u0099s no secret that [large language models\n(LLMs)](https://www.superannotate.com/blog/llm-overview) are evolving at a\nwild speed and are turning heads in the [generative\nAI](https://www.superannotate.com/blog/generative-ai-explained) industry.", "start_char_idx": 2869, "end_char_idx": 3753, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee171899-adb4-4797-8477-328097365f8a": {"__data__": {"id_": "ee171899-adb4-4797-8477-328097365f8a", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c92f851-0657-4251-9eaa-e7646d2ce529", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "b7ee5970f781aebfe7db91eddee55042751763a66658f53ac4d161f0cd33b532", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b024f6ac-6450-4c36-9e43-1f3bf589c73f", "node_type": "1", "metadata": {}, "hash": "7d2edbbf5769620e2ca726ac0ed9c4381b247042d08afb9b94fd54bd611629c1", "class_name": "RelatedNodeInfo"}}, "text": "2024\n\n\u00e2\u0080\u00a2\n\n12 min\n\n![llm fine tuning](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/653775b2bdff592188a789dd_large-language-\nmodels-llm-fine-tuning.webp)\n\n[![](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/617674c3fb077ab741e5847c_ReadMore-\nArrow.svg)Back to Blog](/blog)\n\n#### Contents\n\nTable of content Item\n\nIt\u00e2\u0080\u0099s no secret that [large language models\n(LLMs)](https://www.superannotate.com/blog/llm-overview) are evolving at a\nwild speed and are turning heads in the [generative\nAI](https://www.superannotate.com/blog/generative-ai-explained) industry.\nEnterprises aren't just intrigued; they're obsessed with LLMs, looking for\nways to integrate this technology into their operations. Billions of dollars\nhave been poured into LLM research and development recently. Industry leaders\nand tech enthusiasts are showing a growing appetite to deepen their\nunderstanding of LLMs. While the LLM frontier keeps expanding more and more,\nstaying informed is critical.", "start_char_idx": 3157, "end_char_idx": 4158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b024f6ac-6450-4c36-9e43-1f3bf589c73f": {"__data__": {"id_": "b024f6ac-6450-4c36-9e43-1f3bf589c73f", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee171899-adb4-4797-8477-328097365f8a", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "e6dcf382566177dfab1de7d4b5c9c52bea1b282308965ffb7754170dcf84180d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23cd2a1a-9902-4670-ab32-2d5be4a62ee2", "node_type": "1", "metadata": {}, "hash": "8b1cf50e2536d8b478a36e1eec76d6ae41f2da848b53a31fa8685ba232e4a8aa", "class_name": "RelatedNodeInfo"}}, "text": "Enterprises aren't just intrigued; they're obsessed with LLMs, looking for\nways to integrate this technology into their operations. Billions of dollars\nhave been poured into LLM research and development recently. Industry leaders\nand tech enthusiasts are showing a growing appetite to deepen their\nunderstanding of LLMs. While the LLM frontier keeps expanding more and more,\nstaying informed is critical. The value LLMs may add to your business depends\non your knowledge and intuition around this technology.\n\nA large language model life cycle has several key steps, and today we're going\nto cover one of the juiciest and most intensive parts of this cycle - the\nfine-tuning process. This is a laborious, heavy, but rewarding task that's\ninvolved in many language model training processes.\n\n## Large language model lifecycle\n\nBefore going over LLM fine-tuning, it's important to understand the LLM\nlifecycle and how it works.\n\n![llm project lifecycle](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7a47ae46886939395ad02_llm-project-\nlifecycle.webp)\n\n1\\. **Vision & scope:** First, you should define the project's vision.", "start_char_idx": 3754, "end_char_idx": 4897, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23cd2a1a-9902-4670-ab32-2d5be4a62ee2": {"__data__": {"id_": "23cd2a1a-9902-4670-ab32-2d5be4a62ee2", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b024f6ac-6450-4c36-9e43-1f3bf589c73f", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "254ce87b549c7a8574eec7c90184dfbdaeb27a162ee7e6a3370f91ce44cec33d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b4f227a-b568-4023-a30d-db8de1128926", "node_type": "1", "metadata": {}, "hash": "34ec98a03383041f90303a8e2e8168780b2db9033e448fb7df81a6db35e968b6", "class_name": "RelatedNodeInfo"}}, "text": "The value LLMs may add to your business depends\non your knowledge and intuition around this technology.\n\nA large language model life cycle has several key steps, and today we're going\nto cover one of the juiciest and most intensive parts of this cycle - the\nfine-tuning process. This is a laborious, heavy, but rewarding task that's\ninvolved in many language model training processes.\n\n## Large language model lifecycle\n\nBefore going over LLM fine-tuning, it's important to understand the LLM\nlifecycle and how it works.\n\n![llm project lifecycle](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7a47ae46886939395ad02_llm-project-\nlifecycle.webp)\n\n1\\. **Vision & scope:** First, you should define the project's vision.\nDetermine if your LLM will be a more universal tool or target a specific task\nlike named entity recognition. Clear objectives save time and resources.\n\n2 **. Model selection:** Choose between training a model from scratch or\nmodifying an existing one. In many cases, adapting a pre-existing model is\nefficient, but some instances require fine-tuning with a new model.\n\n3\\.", "start_char_idx": 4159, "end_char_idx": 5270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b4f227a-b568-4023-a30d-db8de1128926": {"__data__": {"id_": "9b4f227a-b568-4023-a30d-db8de1128926", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23cd2a1a-9902-4670-ab32-2d5be4a62ee2", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "b5bacdf4b43f1d5089a30b5507d6d1ad7e390a4fb717d8f775e0fece57ee19a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06903398-8cd2-4835-a060-aecab6577375", "node_type": "1", "metadata": {}, "hash": "7856ca36646114fe3ed278452959fd1ce93227ccab141cd51d8b7c557c87a9f1", "class_name": "RelatedNodeInfo"}}, "text": "## Large language model lifecycle\n\nBefore going over LLM fine-tuning, it's important to understand the LLM\nlifecycle and how it works.\n\n![llm project lifecycle](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7a47ae46886939395ad02_llm-project-\nlifecycle.webp)\n\n1\\. **Vision & scope:** First, you should define the project's vision.\nDetermine if your LLM will be a more universal tool or target a specific task\nlike named entity recognition. Clear objectives save time and resources.\n\n2 **. Model selection:** Choose between training a model from scratch or\nmodifying an existing one. In many cases, adapting a pre-existing model is\nefficient, but some instances require fine-tuning with a new model.\n\n3\\. **Model 's performance and adjustment:** After preparing your model, you\nneed to assess its performance. If it\u00e2\u0080\u0099s unsatisfactory, try [prompt\nengineering](https://www.superannotate.com/blog/llm-prompting-tricks) or\nfurther fine-tuning. We'll focus on this part. Ensure the model's outputs are\nin sync with human preferences.\n\n4\\. **Evaluation & iteration:** Conduct evaluations regularly using metrics\nand benchmarks.", "start_char_idx": 4545, "end_char_idx": 5689, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06903398-8cd2-4835-a060-aecab6577375": {"__data__": {"id_": "06903398-8cd2-4835-a060-aecab6577375", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b4f227a-b568-4023-a30d-db8de1128926", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "0ccbea578918760783fbe0468c3b125e255a2645d16fc66c88441b70a81c357c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45895980-0f75-448b-9d50-8d3e93145a68", "node_type": "1", "metadata": {}, "hash": "2dfd6b7b4ae034bd062b6ac5c84c43198e93c8fcad6655f6f7021ff5b8f7eb86", "class_name": "RelatedNodeInfo"}}, "text": "**Vision & scope:** First, you should define the project's vision.\nDetermine if your LLM will be a more universal tool or target a specific task\nlike named entity recognition. Clear objectives save time and resources.\n\n2 **. Model selection:** Choose between training a model from scratch or\nmodifying an existing one. In many cases, adapting a pre-existing model is\nefficient, but some instances require fine-tuning with a new model.\n\n3\\. **Model 's performance and adjustment:** After preparing your model, you\nneed to assess its performance. If it\u00e2\u0080\u0099s unsatisfactory, try [prompt\nengineering](https://www.superannotate.com/blog/llm-prompting-tricks) or\nfurther fine-tuning. We'll focus on this part. Ensure the model's outputs are\nin sync with human preferences.\n\n4\\. **Evaluation & iteration:** Conduct evaluations regularly using metrics\nand benchmarks. Iterate between prompt engineering, fine-tuning, and\nevaluation until you reach the desired outcomes.\n\n5\\. **Deployment:** Once the model performs as expected, deploy it. Optimize\nfor computational efficiency and user experience at this juncture.\n\n## What is LLM fine-tuning?", "start_char_idx": 4831, "end_char_idx": 5965, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45895980-0f75-448b-9d50-8d3e93145a68": {"__data__": {"id_": "45895980-0f75-448b-9d50-8d3e93145a68", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06903398-8cd2-4835-a060-aecab6577375", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "67ec3b5ef6e9837aad268bdb7245b4c466df3ebfd34719a3eeac54bf74c01e98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de7ec9b3-91c3-4123-aec6-4bb68b397612", "node_type": "1", "metadata": {}, "hash": "52ccf2f69ad6f2f6833e48dae6fa2ef26d6f36280b791ba40675ecfa52e77caa", "class_name": "RelatedNodeInfo"}}, "text": "In many cases, adapting a pre-existing model is\nefficient, but some instances require fine-tuning with a new model.\n\n3\\. **Model 's performance and adjustment:** After preparing your model, you\nneed to assess its performance. If it\u00e2\u0080\u0099s unsatisfactory, try [prompt\nengineering](https://www.superannotate.com/blog/llm-prompting-tricks) or\nfurther fine-tuning. We'll focus on this part. Ensure the model's outputs are\nin sync with human preferences.\n\n4\\. **Evaluation & iteration:** Conduct evaluations regularly using metrics\nand benchmarks. Iterate between prompt engineering, fine-tuning, and\nevaluation until you reach the desired outcomes.\n\n5\\. **Deployment:** Once the model performs as expected, deploy it. Optimize\nfor computational efficiency and user experience at this juncture.\n\n## What is LLM fine-tuning?\n\nLarge language model (LLM) fine-tuning is the process of taking pre-trained\nmodels and further training them on smaller, specific datasets to refine their\ncapabilities and improve performance in a particular task or domain. Fine-\ntuning is about turning general-purpose models and turning them into\nspecialized models.", "start_char_idx": 5150, "end_char_idx": 6285, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de7ec9b3-91c3-4123-aec6-4bb68b397612": {"__data__": {"id_": "de7ec9b3-91c3-4123-aec6-4bb68b397612", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45895980-0f75-448b-9d50-8d3e93145a68", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "4522d045a49f4161fe91bfd6c3618ecec60b9e8b7ee891e556cd07cbdae7f24d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "876b4dd4-25ec-476b-b32e-abad8f02fb33", "node_type": "1", "metadata": {}, "hash": "4c4376348263854eef7c32a4e44c30089a2714740bc00d983504e4f31d9692a4", "class_name": "RelatedNodeInfo"}}, "text": "We'll focus on this part. Ensure the model's outputs are\nin sync with human preferences.\n\n4\\. **Evaluation & iteration:** Conduct evaluations regularly using metrics\nand benchmarks. Iterate between prompt engineering, fine-tuning, and\nevaluation until you reach the desired outcomes.\n\n5\\. **Deployment:** Once the model performs as expected, deploy it. Optimize\nfor computational efficiency and user experience at this juncture.\n\n## What is LLM fine-tuning?\n\nLarge language model (LLM) fine-tuning is the process of taking pre-trained\nmodels and further training them on smaller, specific datasets to refine their\ncapabilities and improve performance in a particular task or domain. Fine-\ntuning is about turning general-purpose models and turning them into\nspecialized models. It bridges the gap between generic pre-trained models and\nthe unique requirements of specific applications, ensuring that the language\nmodel aligns closely with human expectations. Think of OpenAI's GPT-3, a\nstate-of-the-art large language model designed for a broad range of [natural\nlanguage processing (NLP)](https://www.superannotate.com/blog/what-is-natural-\nlanguage-processing) tasks. Suppose a healthcare organization wants to use\nGPT-3 to assist doctors in generating patient reports from textual notes.", "start_char_idx": 5508, "end_char_idx": 6798, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "876b4dd4-25ec-476b-b32e-abad8f02fb33": {"__data__": {"id_": "876b4dd4-25ec-476b-b32e-abad8f02fb33", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de7ec9b3-91c3-4123-aec6-4bb68b397612", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "6c08f89b7968d05ef433f8b1b9da314b580744975db53773cfa0c0816ca6990a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e9cdc58-b77a-4b8c-8e83-f7a63cbf56b8", "node_type": "1", "metadata": {}, "hash": "256a6ec827cf8b0de3f6841d1fbcfc308b0769fb6996f940696908412114d4b9", "class_name": "RelatedNodeInfo"}}, "text": "Optimize\nfor computational efficiency and user experience at this juncture.\n\n## What is LLM fine-tuning?\n\nLarge language model (LLM) fine-tuning is the process of taking pre-trained\nmodels and further training them on smaller, specific datasets to refine their\ncapabilities and improve performance in a particular task or domain. Fine-\ntuning is about turning general-purpose models and turning them into\nspecialized models. It bridges the gap between generic pre-trained models and\nthe unique requirements of specific applications, ensuring that the language\nmodel aligns closely with human expectations. Think of OpenAI's GPT-3, a\nstate-of-the-art large language model designed for a broad range of [natural\nlanguage processing (NLP)](https://www.superannotate.com/blog/what-is-natural-\nlanguage-processing) tasks. Suppose a healthcare organization wants to use\nGPT-3 to assist doctors in generating patient reports from textual notes.\nWhile GPT-3 can understand and create general text, it might not be optimized\nfor intricate medical terms and specific healthcare jargon.\n\nTo enhance its performance for this specialized role, the organization fine-\ntunes GPT-3 on a dataset filled with medical reports and patient notes.", "start_char_idx": 5861, "end_char_idx": 7086, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e9cdc58-b77a-4b8c-8e83-f7a63cbf56b8": {"__data__": {"id_": "3e9cdc58-b77a-4b8c-8e83-f7a63cbf56b8", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "876b4dd4-25ec-476b-b32e-abad8f02fb33", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "24038efd1087f57bd671df80d91e637dc5a171d42b494058044efd48d2330916", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ab2448a-3142-4d3d-abac-ffc5a31e9448", "node_type": "1", "metadata": {}, "hash": "4c8e931466cb3895f2ebc87151ff97b7f56f0a5e2a4d1c9a2d2d0ab68ae9b63d", "class_name": "RelatedNodeInfo"}}, "text": "Fine-\ntuning is about turning general-purpose models and turning them into\nspecialized models. It bridges the gap between generic pre-trained models and\nthe unique requirements of specific applications, ensuring that the language\nmodel aligns closely with human expectations. Think of OpenAI's GPT-3, a\nstate-of-the-art large language model designed for a broad range of [natural\nlanguage processing (NLP)](https://www.superannotate.com/blog/what-is-natural-\nlanguage-processing) tasks. Suppose a healthcare organization wants to use\nGPT-3 to assist doctors in generating patient reports from textual notes.\nWhile GPT-3 can understand and create general text, it might not be optimized\nfor intricate medical terms and specific healthcare jargon.\n\nTo enhance its performance for this specialized role, the organization fine-\ntunes GPT-3 on a dataset filled with medical reports and patient notes. It\nmight use tools like [SuperAnnotate's LLM custom\neditor](https://www.superannotate.com/llms) to build its own model with the\ndesired interface. Through this process, the model becomes more familiar with\nmedical terminologies, the nuances of clinical language, and typical report\nstructures.", "start_char_idx": 6191, "end_char_idx": 7380, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ab2448a-3142-4d3d-abac-ffc5a31e9448": {"__data__": {"id_": "1ab2448a-3142-4d3d-abac-ffc5a31e9448", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e9cdc58-b77a-4b8c-8e83-f7a63cbf56b8", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "556887598123fd706336a0f3e712494f6008b7feb4613ce35a9d3744a41a61fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64af5183-d123-45cb-81d2-d70b27da9ee5", "node_type": "1", "metadata": {}, "hash": "5129bea3d204db4fb00d1615ed4d8d98b22b4cc9ea5fe5836d259e9632edbe97", "class_name": "RelatedNodeInfo"}}, "text": "Suppose a healthcare organization wants to use\nGPT-3 to assist doctors in generating patient reports from textual notes.\nWhile GPT-3 can understand and create general text, it might not be optimized\nfor intricate medical terms and specific healthcare jargon.\n\nTo enhance its performance for this specialized role, the organization fine-\ntunes GPT-3 on a dataset filled with medical reports and patient notes. It\nmight use tools like [SuperAnnotate's LLM custom\neditor](https://www.superannotate.com/llms) to build its own model with the\ndesired interface. Through this process, the model becomes more familiar with\nmedical terminologies, the nuances of clinical language, and typical report\nstructures. After fine-tuning, GPT-3 is primed to assist doctors in generating\naccurate and coherent patient reports, demonstrating its adaptability for\nspecific tasks.\n\n![what does fine tuning do for the model](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7a5e7f00d1466ff618b26_what-does-fine-\ntuning-do-for-the-model.webp)\n\nThis sounds great to have in every large language model, but remember that\neverything comes with a cost. We'll discuss that in more detail soon.", "start_char_idx": 6678, "end_char_idx": 7863, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64af5183-d123-45cb-81d2-d70b27da9ee5": {"__data__": {"id_": "64af5183-d123-45cb-81d2-d70b27da9ee5", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ab2448a-3142-4d3d-abac-ffc5a31e9448", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "8cedf186b7a075d459cc2e2ed43379e22c5dbe0065e727765d2283f69fcfe7bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62fe50b0-c220-4d5a-98a3-b7e0dad4e09b", "node_type": "1", "metadata": {}, "hash": "2988f8287c5921d8eef51032bdfd605a159207a6be5751948c3e505764a1b839", "class_name": "RelatedNodeInfo"}}, "text": "It\nmight use tools like [SuperAnnotate's LLM custom\neditor](https://www.superannotate.com/llms) to build its own model with the\ndesired interface. Through this process, the model becomes more familiar with\nmedical terminologies, the nuances of clinical language, and typical report\nstructures. After fine-tuning, GPT-3 is primed to assist doctors in generating\naccurate and coherent patient reports, demonstrating its adaptability for\nspecific tasks.\n\n![what does fine tuning do for the model](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7a5e7f00d1466ff618b26_what-does-fine-\ntuning-do-for-the-model.webp)\n\nThis sounds great to have in every large language model, but remember that\neverything comes with a cost. We'll discuss that in more detail soon.\n\n## When to use fine-tuning\n\nOur article about large language models touches upon topics like in-context\nlearning and zero/one/few shot inference. Here\u00e2\u0080\u0099s a\u00c2 quick recap:\n\n **In-context learning** is a method for improving the prompt through specific\ntask examples within the prompt, offering the LLM a blueprint of what it needs\nto accomplish.", "start_char_idx": 7087, "end_char_idx": 8209, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62fe50b0-c220-4d5a-98a3-b7e0dad4e09b": {"__data__": {"id_": "62fe50b0-c220-4d5a-98a3-b7e0dad4e09b", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64af5183-d123-45cb-81d2-d70b27da9ee5", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "b4e1e4a1a40bcf4c31eaa0dd5a58de7303c29f5f298abcf9155b6c77f51bf097", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "100555c8-f5f0-4d37-8fc2-4ad818e00ba0", "node_type": "1", "metadata": {}, "hash": "30894099d26b91bb69c1eca8fa5cb29ea616e1097f819bd773c5e36258c3713b", "class_name": "RelatedNodeInfo"}}, "text": "![what does fine tuning do for the model](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7a5e7f00d1466ff618b26_what-does-fine-\ntuning-do-for-the-model.webp)\n\nThis sounds great to have in every large language model, but remember that\neverything comes with a cost. We'll discuss that in more detail soon.\n\n## When to use fine-tuning\n\nOur article about large language models touches upon topics like in-context\nlearning and zero/one/few shot inference. Here\u00e2\u0080\u0099s a\u00c2 quick recap:\n\n **In-context learning** is a method for improving the prompt through specific\ntask examples within the prompt, offering the LLM a blueprint of what it needs\nto accomplish.\n\n **Zero-shot inference** incorporates your input data in the prompt without\nextra examples. If zero-shot inference doesn't yield the desired results,\n**'**[ **one-shot**](https://www.superannotate.com/blog/one-shot-annotation)\n**' or 'few-shot inference'** can be used. These tactics involve adding one or\nmultiple completed examples within the prompt, helping smaller LLMs perform\nbetter.\n\n!", "start_char_idx": 7539, "end_char_idx": 8603, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "100555c8-f5f0-4d37-8fc2-4ad818e00ba0": {"__data__": {"id_": "100555c8-f5f0-4d37-8fc2-4ad818e00ba0", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62fe50b0-c220-4d5a-98a3-b7e0dad4e09b", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "9a8b657e6b6486d564d10d7e4a417d055f5d0af1f407be72ec4135037029ce72", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "810a954e-8b1f-4f6b-a84d-e80e88a3188a", "node_type": "1", "metadata": {}, "hash": "5db89d7cdcdaf1f9482efed7f0df4c51d602047c793107eb36d2e6deb9dbe777", "class_name": "RelatedNodeInfo"}}, "text": "We'll discuss that in more detail soon.\n\n## When to use fine-tuning\n\nOur article about large language models touches upon topics like in-context\nlearning and zero/one/few shot inference. Here\u00e2\u0080\u0099s a\u00c2 quick recap:\n\n **In-context learning** is a method for improving the prompt through specific\ntask examples within the prompt, offering the LLM a blueprint of what it needs\nto accomplish.\n\n **Zero-shot inference** incorporates your input data in the prompt without\nextra examples. If zero-shot inference doesn't yield the desired results,\n**'**[ **one-shot**](https://www.superannotate.com/blog/one-shot-annotation)\n**' or 'few-shot inference'** can be used. These tactics involve adding one or\nmultiple completed examples within the prompt, helping smaller LLMs perform\nbetter.\n\n![in context learning](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7a642af158d41a94768de_in-context-\nlearning.webp)\n\nThese are techniques used directly in the user prompt and aim to optimize the\nmodel's output and better fit it to the user's preferences. The problem is\nthat they don\u00e2\u0080\u0099t always work, especially for smaller LLMs. Here's an example\nof how in-context learning may fail.", "start_char_idx": 7824, "end_char_idx": 9011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "810a954e-8b1f-4f6b-a84d-e80e88a3188a": {"__data__": {"id_": "810a954e-8b1f-4f6b-a84d-e80e88a3188a", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "100555c8-f5f0-4d37-8fc2-4ad818e00ba0", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "ef5424d6b5d04bb11ef8b7f24b2d42f3fc7ce3fa616d009dab2839aecfee50aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ade6fdf-a752-45b9-8550-13a82a7812d1", "node_type": "1", "metadata": {}, "hash": "edce1623e72f6dd613a0c2e7aae9699db5d951a8e35ba676f31065e5f8a27bb7", "class_name": "RelatedNodeInfo"}}, "text": "**Zero-shot inference** incorporates your input data in the prompt without\nextra examples. If zero-shot inference doesn't yield the desired results,\n**'**[ **one-shot**](https://www.superannotate.com/blog/one-shot-annotation)\n**' or 'few-shot inference'** can be used. These tactics involve adding one or\nmultiple completed examples within the prompt, helping smaller LLMs perform\nbetter.\n\n![in context learning](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7a642af158d41a94768de_in-context-\nlearning.webp)\n\nThese are techniques used directly in the user prompt and aim to optimize the\nmodel's output and better fit it to the user's preferences. The problem is\nthat they don\u00e2\u0080\u0099t always work, especially for smaller LLMs. Here's an example\nof how in-context learning may fail.\n\nOther than that, any examples you include in your prompt take up valuable\nspace in the context window, reducing the space you have to include additional\nhelpful information. And here, finally, comes fine-tuning. Unlike the pre-\ntraining phase, with vast amounts of unstructured text data, fine-tuning is a\nsupervised learning process.", "start_char_idx": 8212, "end_char_idx": 9347, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ade6fdf-a752-45b9-8550-13a82a7812d1": {"__data__": {"id_": "8ade6fdf-a752-45b9-8550-13a82a7812d1", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "810a954e-8b1f-4f6b-a84d-e80e88a3188a", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "41e35d778ea4c4d9c2092268a6f6e725a3913b62c6775345f87731309579d091", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c0a244d-acc3-4094-910e-5988ec67274b", "node_type": "1", "metadata": {}, "hash": "ae6b273f6d6846629c31eb69e3478cff7edb16a2e56ede70717b854b229c2b01", "class_name": "RelatedNodeInfo"}}, "text": "![in context learning](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7a642af158d41a94768de_in-context-\nlearning.webp)\n\nThese are techniques used directly in the user prompt and aim to optimize the\nmodel's output and better fit it to the user's preferences. The problem is\nthat they don\u00e2\u0080\u0099t always work, especially for smaller LLMs. Here's an example\nof how in-context learning may fail.\n\nOther than that, any examples you include in your prompt take up valuable\nspace in the context window, reducing the space you have to include additional\nhelpful information. And here, finally, comes fine-tuning. Unlike the pre-\ntraining phase, with vast amounts of unstructured text data, fine-tuning is a\nsupervised learning process. This means that you use a dataset of labeled\nexamples to update the weights of LLM. These labeled examples are usually\nprompt-response pairs, resulting in a better completion of specific tasks.\n\n## Supervised fine-tuning (SFT)\n\nSupervised fine-tuning means updating a pre-trained language model using\nlabeled data to do a specific task. The data used has been checked earlier.\nThis is different from unsupervised methods, where data isn't checked.", "start_char_idx": 8602, "end_char_idx": 9795, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c0a244d-acc3-4094-910e-5988ec67274b": {"__data__": {"id_": "5c0a244d-acc3-4094-910e-5988ec67274b", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ade6fdf-a752-45b9-8550-13a82a7812d1", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "489f428b516eeb5a7701f0c5e76168bf80a9823d574bc6b9d42e1ab7a70eab03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8e7bd3a-eb79-470b-9c7f-9f18924f7180", "node_type": "1", "metadata": {}, "hash": "5ad1c8a976d77d4db439c2891e59a0dbebb51cbb49b304d8ad656db23887f6b1", "class_name": "RelatedNodeInfo"}}, "text": "Here's an example\nof how in-context learning may fail.\n\nOther than that, any examples you include in your prompt take up valuable\nspace in the context window, reducing the space you have to include additional\nhelpful information. And here, finally, comes fine-tuning. Unlike the pre-\ntraining phase, with vast amounts of unstructured text data, fine-tuning is a\nsupervised learning process. This means that you use a dataset of labeled\nexamples to update the weights of LLM. These labeled examples are usually\nprompt-response pairs, resulting in a better completion of specific tasks.\n\n## Supervised fine-tuning (SFT)\n\nSupervised fine-tuning means updating a pre-trained language model using\nlabeled data to do a specific task. The data used has been checked earlier.\nThis is different from unsupervised methods, where data isn't checked.\nUsually, the initial training of the language model is unsupervised, but fine-\ntuning is supervised.\n\n### How is fine-tuning performed?\n\nLet's get into more details of fine-tuning in LLMs. For preparing the training\ndata, there are many open-source datasets that offer insights into user\nbehaviors and preferences, even if they aren't directly formatted as\ninstructional data.", "start_char_idx": 8957, "end_char_idx": 10172, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8e7bd3a-eb79-470b-9c7f-9f18924f7180": {"__data__": {"id_": "b8e7bd3a-eb79-470b-9c7f-9f18924f7180", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c0a244d-acc3-4094-910e-5988ec67274b", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "9b84e3acc0e689d6d1903d54c095a1a9669ba2feb78de1632c9ed26b4dcf004b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72bfe78f-4633-41de-85db-72a526402837", "node_type": "1", "metadata": {}, "hash": "051f6f68de70a19c9110581da9a29f08d8bfe41f46fa72dfead5a9c4cf4082ee", "class_name": "RelatedNodeInfo"}}, "text": "This means that you use a dataset of labeled\nexamples to update the weights of LLM. These labeled examples are usually\nprompt-response pairs, resulting in a better completion of specific tasks.\n\n## Supervised fine-tuning (SFT)\n\nSupervised fine-tuning means updating a pre-trained language model using\nlabeled data to do a specific task. The data used has been checked earlier.\nThis is different from unsupervised methods, where data isn't checked.\nUsually, the initial training of the language model is unsupervised, but fine-\ntuning is supervised.\n\n### How is fine-tuning performed?\n\nLet's get into more details of fine-tuning in LLMs. For preparing the training\ndata, there are many open-source datasets that offer insights into user\nbehaviors and preferences, even if they aren't directly formatted as\ninstructional data. For example, we can take the large data set of Amazon\nproduct reviews and turn them into instruction prompt datasets for fine-\ntuning. Prompt template libraries include many templates for different tasks\nand different datasets.\n\n!", "start_char_idx": 9348, "end_char_idx": 10403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72bfe78f-4633-41de-85db-72a526402837": {"__data__": {"id_": "72bfe78f-4633-41de-85db-72a526402837", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8e7bd3a-eb79-470b-9c7f-9f18924f7180", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "ff740548fabfde9eef0e213290b64542b350f23cdc5bdc01652d5f9803170c8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87ddb007-8915-412d-95c1-dd312c4f6945", "node_type": "1", "metadata": {}, "hash": "3b5820fe3963fce26d8b22973bbacb1f2ebb8db8f71c6f9e3774bc7646d351ad", "class_name": "RelatedNodeInfo"}}, "text": "## Supervised fine-tuning (SFT)\n\nSupervised fine-tuning means updating a pre-trained language model using\nlabeled data to do a specific task. The data used has been checked earlier.\nThis is different from unsupervised methods, where data isn't checked.\nUsually, the initial training of the language model is unsupervised, but fine-\ntuning is supervised.\n\n### How is fine-tuning performed?\n\nLet's get into more details of fine-tuning in LLMs. For preparing the training\ndata, there are many open-source datasets that offer insights into user\nbehaviors and preferences, even if they aren't directly formatted as\ninstructional data. For example, we can take the large data set of Amazon\nproduct reviews and turn them into instruction prompt datasets for fine-\ntuning. Prompt template libraries include many templates for different tasks\nand different datasets.\n\n![how is fine tuning performed](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7a67b97500a346be929a1_how-is-fine-\ntuning-performed.webp)\n\nOnce your instruction data set is ready, as with standard supervised learning,\nyou divide the data set into training validation and test splits.", "start_char_idx": 9543, "end_char_idx": 10706, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87ddb007-8915-412d-95c1-dd312c4f6945": {"__data__": {"id_": "87ddb007-8915-412d-95c1-dd312c4f6945", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72bfe78f-4633-41de-85db-72a526402837", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "d5ef95a586d1e4db0843d6f05c6953d202d9e89d6480c06891ec1d42f0f7c37d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30c1fe9b-ad2d-4d93-8eae-d7a3c64c44b0", "node_type": "1", "metadata": {}, "hash": "b5ef030903ec2ff107bd451fdfba30a0e19994bf4faa972b78ff4ecd082f0efa", "class_name": "RelatedNodeInfo"}}, "text": "### How is fine-tuning performed?\n\nLet's get into more details of fine-tuning in LLMs. For preparing the training\ndata, there are many open-source datasets that offer insights into user\nbehaviors and preferences, even if they aren't directly formatted as\ninstructional data. For example, we can take the large data set of Amazon\nproduct reviews and turn them into instruction prompt datasets for fine-\ntuning. Prompt template libraries include many templates for different tasks\nand different datasets.\n\n![how is fine tuning performed](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7a67b97500a346be929a1_how-is-fine-\ntuning-performed.webp)\n\nOnce your instruction data set is ready, as with standard supervised learning,\nyou divide the data set into training validation and test splits. During fine-\ntuning, you select prompts from your training data set and pass them to the\nLLM, which then generates completions.\n\nDuring the fine-tuning phase, when the model is exposed to a newly labeled\ndataset specific to the target task, it calculates the error or difference\nbetween its predictions and the actual labels. The model then uses this error\nto adjust its weights, typically via an optimization algorithm like gradient\ndescent.", "start_char_idx": 9898, "end_char_idx": 11149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30c1fe9b-ad2d-4d93-8eae-d7a3c64c44b0": {"__data__": {"id_": "30c1fe9b-ad2d-4d93-8eae-d7a3c64c44b0", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87ddb007-8915-412d-95c1-dd312c4f6945", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "06a362b634d27fdc4de1837c5e78f47505a665b5e05d07c23315ec9028277c66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "718cc305-9a79-47d7-b02b-d02771a7187f", "node_type": "1", "metadata": {}, "hash": "c67b3b70a8ba95caf2a1e9b027aa78b949c2273514b6d1321dbceaa11e5f6959", "class_name": "RelatedNodeInfo"}}, "text": "Prompt template libraries include many templates for different tasks\nand different datasets.\n\n![how is fine tuning performed](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7a67b97500a346be929a1_how-is-fine-\ntuning-performed.webp)\n\nOnce your instruction data set is ready, as with standard supervised learning,\nyou divide the data set into training validation and test splits. During fine-\ntuning, you select prompts from your training data set and pass them to the\nLLM, which then generates completions.\n\nDuring the fine-tuning phase, when the model is exposed to a newly labeled\ndataset specific to the target task, it calculates the error or difference\nbetween its predictions and the actual labels. The model then uses this error\nto adjust its weights, typically via an optimization algorithm like gradient\ndescent. The magnitude and direction of weight adjustments depend on the\ngradients, which indicate how much each weight contributed to the error.\nWeights that are more responsible for the error are adjusted more, while those\nless responsible are adjusted less.\n\nOver multiple iterations (or epochs) of the dataset, the model continues to\nadjust its weights, honing in on a configuration that minimizes the error for\nthe specific task.", "start_char_idx": 10308, "end_char_idx": 11575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "718cc305-9a79-47d7-b02b-d02771a7187f": {"__data__": {"id_": "718cc305-9a79-47d7-b02b-d02771a7187f", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30c1fe9b-ad2d-4d93-8eae-d7a3c64c44b0", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "b9df16be74324a10dfe55ce8d705675594062309fbe601558330c688e82556fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8399cdd6-cbd5-4218-83d9-40d3473ac522", "node_type": "1", "metadata": {}, "hash": "2a3a10ce24d00cbef7c4880cacc95078d85c4281d88e292cd355c1c0f54c386d", "class_name": "RelatedNodeInfo"}}, "text": "During fine-\ntuning, you select prompts from your training data set and pass them to the\nLLM, which then generates completions.\n\nDuring the fine-tuning phase, when the model is exposed to a newly labeled\ndataset specific to the target task, it calculates the error or difference\nbetween its predictions and the actual labels. The model then uses this error\nto adjust its weights, typically via an optimization algorithm like gradient\ndescent. The magnitude and direction of weight adjustments depend on the\ngradients, which indicate how much each weight contributed to the error.\nWeights that are more responsible for the error are adjusted more, while those\nless responsible are adjusted less.\n\nOver multiple iterations (or epochs) of the dataset, the model continues to\nadjust its weights, honing in on a configuration that minimizes the error for\nthe specific task. The aim is to adapt the previously learned general\nknowledge to the nuances and specific patterns present in the new dataset,\nthereby making the model more specialized and effective for the target task.\n\nDuring this process, the model is updated with the labeled data. It changes\nbased on the difference between its guesses and the actual answers. This helps\nthe model learn details found in the labeled data. By doing this, the model\nimproves at the task for which it's fine-tuned.", "start_char_idx": 10707, "end_char_idx": 12058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8399cdd6-cbd5-4218-83d9-40d3473ac522": {"__data__": {"id_": "8399cdd6-cbd5-4218-83d9-40d3473ac522", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "718cc305-9a79-47d7-b02b-d02771a7187f", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "8ccc21ae7f28f191151b83b505058a761b78c01dd2275fe78f0bc6d4ecf74b47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4751216-0a29-400f-9a72-4cdf18a9c02a", "node_type": "1", "metadata": {}, "hash": "7a316e9bcb0a361565a159ef2c6f8ba3cdf506392ec074e9b68b08be7dbcf5e3", "class_name": "RelatedNodeInfo"}}, "text": "The magnitude and direction of weight adjustments depend on the\ngradients, which indicate how much each weight contributed to the error.\nWeights that are more responsible for the error are adjusted more, while those\nless responsible are adjusted less.\n\nOver multiple iterations (or epochs) of the dataset, the model continues to\nadjust its weights, honing in on a configuration that minimizes the error for\nthe specific task. The aim is to adapt the previously learned general\nknowledge to the nuances and specific patterns present in the new dataset,\nthereby making the model more specialized and effective for the target task.\n\nDuring this process, the model is updated with the labeled data. It changes\nbased on the difference between its guesses and the actual answers. This helps\nthe model learn details found in the labeled data. By doing this, the model\nimproves at the task for which it's fine-tuned.\n\nLet's take an example to picture this better; if you ask a pre-trained\nmodel,\"Why is the sky blue?\" it might reply, \"Because of the way the\natmosphere scatters sunlight.\" This answer is simple and direct. However, the\nanswer might be too brief for a chatbot for a science educational platform. It\nmay need more scientific detail or context based on your guidelines.", "start_char_idx": 11150, "end_char_idx": 12425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4751216-0a29-400f-9a72-4cdf18a9c02a": {"__data__": {"id_": "a4751216-0a29-400f-9a72-4cdf18a9c02a", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8399cdd6-cbd5-4218-83d9-40d3473ac522", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "44a660d10d2ab565ef8e7e5d3ec1751953ada72bd51b9b92d7cb2d8cd2bddd82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff16ff62-ab9b-4e68-8bed-5ac504cf92e8", "node_type": "1", "metadata": {}, "hash": "fe9b8b122bf810fb173ff54a38de56bb1fb6643156f526dc9f6be42a443d2ac9", "class_name": "RelatedNodeInfo"}}, "text": "The aim is to adapt the previously learned general\nknowledge to the nuances and specific patterns present in the new dataset,\nthereby making the model more specialized and effective for the target task.\n\nDuring this process, the model is updated with the labeled data. It changes\nbased on the difference between its guesses and the actual answers. This helps\nthe model learn details found in the labeled data. By doing this, the model\nimproves at the task for which it's fine-tuned.\n\nLet's take an example to picture this better; if you ask a pre-trained\nmodel,\"Why is the sky blue?\" it might reply, \"Because of the way the\natmosphere scatters sunlight.\" This answer is simple and direct. However, the\nanswer might be too brief for a chatbot for a science educational platform. It\nmay need more scientific detail or context based on your guidelines. This is\nwhere supervised fine-tuning helps.\n\n![base model vs fine tuned model](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7a6bd12a6f621f37ebdfa_base-model-vs-\nfine-tuned-model.webp)\n\nAfter fine-tuning, the model can give a more in-depth response to scientific\nquestions.", "start_char_idx": 11576, "end_char_idx": 12722, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff16ff62-ab9b-4e68-8bed-5ac504cf92e8": {"__data__": {"id_": "ff16ff62-ab9b-4e68-8bed-5ac504cf92e8", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4751216-0a29-400f-9a72-4cdf18a9c02a", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "9811254dcf9f6be4a1c2acacb9fda0bb4a2ea3f3ee00fa5bab381e38e3461193", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d34612ce-ab70-4e93-a1b4-e67e1df12df4", "node_type": "1", "metadata": {}, "hash": "ed9ffa795e03835fb2459bbb7f59aa3e78d1c34c3eff9bc9175c0741da8130bf", "class_name": "RelatedNodeInfo"}}, "text": "By doing this, the model\nimproves at the task for which it's fine-tuned.\n\nLet's take an example to picture this better; if you ask a pre-trained\nmodel,\"Why is the sky blue?\" it might reply, \"Because of the way the\natmosphere scatters sunlight.\" This answer is simple and direct. However, the\nanswer might be too brief for a chatbot for a science educational platform. It\nmay need more scientific detail or context based on your guidelines. This is\nwhere supervised fine-tuning helps.\n\n![base model vs fine tuned model](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7a6bd12a6f621f37ebdfa_base-model-vs-\nfine-tuned-model.webp)\n\nAfter fine-tuning, the model can give a more in-depth response to scientific\nquestions. After fine-tuning, when asked, \"Why is the sky blue?\", the model\nmight provide a more detailed explanation like:\n\n\"The sky appears blue because of a phenomenon called Rayleigh scattering. As\nsunlight enters Earth's atmosphere, it consists of different colors, each with\nits own wavelength. Blue light has a shorter wavelength and is scattered in\nall directions by the gases and particles in the atmosphere.", "start_char_idx": 11986, "end_char_idx": 13129, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d34612ce-ab70-4e93-a1b4-e67e1df12df4": {"__data__": {"id_": "d34612ce-ab70-4e93-a1b4-e67e1df12df4", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff16ff62-ab9b-4e68-8bed-5ac504cf92e8", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "e4c8a8806927caa031d6120887d9ea30c227aee7cb6d9110baa3163898f68f62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30d290b0-8efc-473d-aa86-2281ee58825d", "node_type": "1", "metadata": {}, "hash": "98647cd002f23013840cb734cb8a0ed675969649a580c4df9f1fb11868698f7f", "class_name": "RelatedNodeInfo"}}, "text": "It\nmay need more scientific detail or context based on your guidelines. This is\nwhere supervised fine-tuning helps.\n\n![base model vs fine tuned model](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7a6bd12a6f621f37ebdfa_base-model-vs-\nfine-tuned-model.webp)\n\nAfter fine-tuning, the model can give a more in-depth response to scientific\nquestions. After fine-tuning, when asked, \"Why is the sky blue?\", the model\nmight provide a more detailed explanation like:\n\n\"The sky appears blue because of a phenomenon called Rayleigh scattering. As\nsunlight enters Earth's atmosphere, it consists of different colors, each with\nits own wavelength. Blue light has a shorter wavelength and is scattered in\nall directions by the gases and particles in the atmosphere. This scattering\ncauses the direct sunlight to appear white, but the sky itself to take on a\nblue hue.\" This enriched response is comprehensive and suitable for a science\neducational platform.\n\n## Fine-tuning methods\n\nLLM fine-tuning is a supervised learning process where you use a dataset of\nlabeled examples to update the weights of LLM and make the model improve its\nability for specific tasks.", "start_char_idx": 12354, "end_char_idx": 13527, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30d290b0-8efc-473d-aa86-2281ee58825d": {"__data__": {"id_": "30d290b0-8efc-473d-aa86-2281ee58825d", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d34612ce-ab70-4e93-a1b4-e67e1df12df4", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "f4367c3aca49209f9d68efc20b1b1ac923e5f468cff92e0998a8c336e8d112bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e89e5570-8955-4c3d-966a-0248986cd9d3", "node_type": "1", "metadata": {}, "hash": "b61e01b6fcd55acacfcb7fdcea48b084934476f631e2c0990fca07be11df46d6", "class_name": "RelatedNodeInfo"}}, "text": "After fine-tuning, when asked, \"Why is the sky blue?\", the model\nmight provide a more detailed explanation like:\n\n\"The sky appears blue because of a phenomenon called Rayleigh scattering. As\nsunlight enters Earth's atmosphere, it consists of different colors, each with\nits own wavelength. Blue light has a shorter wavelength and is scattered in\nall directions by the gases and particles in the atmosphere. This scattering\ncauses the direct sunlight to appear white, but the sky itself to take on a\nblue hue.\" This enriched response is comprehensive and suitable for a science\neducational platform.\n\n## Fine-tuning methods\n\nLLM fine-tuning is a supervised learning process where you use a dataset of\nlabeled examples to update the weights of LLM and make the model improve its\nability for specific tasks. Let's explore some of the notable fine-tuning\nmethods.\n\n### Instruction fine-tuning\n\nOne strategy used to improve a model's performance on various tasks is\ninstruction fine-tuning. It's about training the machine learning model using\nexamples that demonstrate how the model should respond to the query. The\ndataset you use for fine-tuning large language models has to serve the purpose\nof your instruction. For example, suppose you fine-tune your model to improve\nits summarization skills.", "start_char_idx": 12723, "end_char_idx": 14017, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e89e5570-8955-4c3d-966a-0248986cd9d3": {"__data__": {"id_": "e89e5570-8955-4c3d-966a-0248986cd9d3", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30d290b0-8efc-473d-aa86-2281ee58825d", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "a3753e4e8093a77035949cb7d541b754e3d301dd3ec1945c6a081579418cd853", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "842e2e3f-7318-4b95-b722-bf7051d9da91", "node_type": "1", "metadata": {}, "hash": "c806eddd2cc78e671623f96c9bcf3c82486abb651f3a72ee771a7f204e8aa3b6", "class_name": "RelatedNodeInfo"}}, "text": "This scattering\ncauses the direct sunlight to appear white, but the sky itself to take on a\nblue hue.\" This enriched response is comprehensive and suitable for a science\neducational platform.\n\n## Fine-tuning methods\n\nLLM fine-tuning is a supervised learning process where you use a dataset of\nlabeled examples to update the weights of LLM and make the model improve its\nability for specific tasks. Let's explore some of the notable fine-tuning\nmethods.\n\n### Instruction fine-tuning\n\nOne strategy used to improve a model's performance on various tasks is\ninstruction fine-tuning. It's about training the machine learning model using\nexamples that demonstrate how the model should respond to the query. The\ndataset you use for fine-tuning large language models has to serve the purpose\nof your instruction. For example, suppose you fine-tune your model to improve\nits summarization skills. In that case, you should build up a dataset of\nexamples that begin with the instruction to summarize, followed by text or a\nsimilar phrase. In the case of translation, you should include instructions\nlike \u00e2\u0080\u009ctranslate this text.\u00e2\u0080\u009d These prompt completion pairs allow your model\nto \"think\" in a new niche way and serve the given specific task.\n\n!", "start_char_idx": 13130, "end_char_idx": 14364, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "842e2e3f-7318-4b95-b722-bf7051d9da91": {"__data__": {"id_": "842e2e3f-7318-4b95-b722-bf7051d9da91", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e89e5570-8955-4c3d-966a-0248986cd9d3", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "a54177783f73691d68b681534d24e7c94530e5d0cb4e5b8d188f9618bae86d4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb243c37-4061-43f1-9ce5-9577d4946318", "node_type": "1", "metadata": {}, "hash": "477b1fb8bbef9db786e3f036fcae1684ffb7f4e3206bb0270523a12c2aa08ad2", "class_name": "RelatedNodeInfo"}}, "text": "Let's explore some of the notable fine-tuning\nmethods.\n\n### Instruction fine-tuning\n\nOne strategy used to improve a model's performance on various tasks is\ninstruction fine-tuning. It's about training the machine learning model using\nexamples that demonstrate how the model should respond to the query. The\ndataset you use for fine-tuning large language models has to serve the purpose\nof your instruction. For example, suppose you fine-tune your model to improve\nits summarization skills. In that case, you should build up a dataset of\nexamples that begin with the instruction to summarize, followed by text or a\nsimilar phrase. In the case of translation, you should include instructions\nlike \u00e2\u0080\u009ctranslate this text.\u00e2\u0080\u009d These prompt completion pairs allow your model\nto \"think\" in a new niche way and serve the given specific task.\n\n![using prompts to fine tune llms with instruction](https://assets-\nglobal.website-\nfiles.com/614c82ed388d53640613982e/65b7a73f7ee95b6c9e261863_using-prompts-to-\nfine-tune-llms-with-instruction.webp)\n\n### Full fine-tuning\n\nInstruction fine-tuning, where all of the model's weights are updated, is\nknown as full fine-tuning.", "start_char_idx": 13528, "end_char_idx": 14686, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb243c37-4061-43f1-9ce5-9577d4946318": {"__data__": {"id_": "bb243c37-4061-43f1-9ce5-9577d4946318", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "842e2e3f-7318-4b95-b722-bf7051d9da91", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "fe28831e15f229b6956985b21254d0504c5684ccdee3db783fdd33a53a9eb994", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "455ce9a8-9201-41cb-9b0d-3254dbae2895", "node_type": "1", "metadata": {}, "hash": "f448c1edffb640cf71e8eeb1be6d059c60ac573b5fb9c05cce2b65a3096afc9a", "class_name": "RelatedNodeInfo"}}, "text": "For example, suppose you fine-tune your model to improve\nits summarization skills. In that case, you should build up a dataset of\nexamples that begin with the instruction to summarize, followed by text or a\nsimilar phrase. In the case of translation, you should include instructions\nlike \u00e2\u0080\u009ctranslate this text.\u00e2\u0080\u009d These prompt completion pairs allow your model\nto \"think\" in a new niche way and serve the given specific task.\n\n![using prompts to fine tune llms with instruction](https://assets-\nglobal.website-\nfiles.com/614c82ed388d53640613982e/65b7a73f7ee95b6c9e261863_using-prompts-to-\nfine-tune-llms-with-instruction.webp)\n\n### Full fine-tuning\n\nInstruction fine-tuning, where all of the model's weights are updated, is\nknown as full fine-tuning. The process results in a new version of the model\nwith updated weights. It is important to note that just like pre-training,\nfull fine-tuning requires enough memory and compute budget to store and\nprocess all the gradients, optimizers, and other components being updated\nduring training.\n\n### Parameter-efficient fine-tuning\n\nTraining a language model is a computationally intensive task.", "start_char_idx": 13935, "end_char_idx": 15075, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "455ce9a8-9201-41cb-9b0d-3254dbae2895": {"__data__": {"id_": "455ce9a8-9201-41cb-9b0d-3254dbae2895", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb243c37-4061-43f1-9ce5-9577d4946318", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "4c06679462790015939f90015fadad6169c3841b55b748b29b19853f0bfa2961", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f914ba8-86fe-4391-8260-71cff2a54df7", "node_type": "1", "metadata": {}, "hash": "93e45105e48a28480ceff09955ccf9acc9061b4c49b55ff7867233fa4298c2c3", "class_name": "RelatedNodeInfo"}}, "text": "![using prompts to fine tune llms with instruction](https://assets-\nglobal.website-\nfiles.com/614c82ed388d53640613982e/65b7a73f7ee95b6c9e261863_using-prompts-to-\nfine-tune-llms-with-instruction.webp)\n\n### Full fine-tuning\n\nInstruction fine-tuning, where all of the model's weights are updated, is\nknown as full fine-tuning. The process results in a new version of the model\nwith updated weights. It is important to note that just like pre-training,\nfull fine-tuning requires enough memory and compute budget to store and\nprocess all the gradients, optimizers, and other components being updated\nduring training.\n\n### Parameter-efficient fine-tuning\n\nTraining a language model is a computationally intensive task. For a full LLM\nfine-tuning, you need memory not only to store the model, but also the\nparameters that are necessary for the training process. Your computer might be\nable to handle the model weights, but allocating memory for optimizing states,\ngradients, and forward activations during the training process is a\nchallenging task. Simple hardware cannot handle this amount of hurdle. This is\nwhere PEFT is crucial.", "start_char_idx": 14363, "end_char_idx": 15489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f914ba8-86fe-4391-8260-71cff2a54df7": {"__data__": {"id_": "2f914ba8-86fe-4391-8260-71cff2a54df7", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "455ce9a8-9201-41cb-9b0d-3254dbae2895", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "20d9ed58917eefc8048099babb43648502e028c5a500216fdd0a3ab3db3e2166", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a36ca3a-34f2-45b1-9def-e832893dc387", "node_type": "1", "metadata": {}, "hash": "34c57cbc03ff099964eb817f28d28a1534386438bd822d8f92b01b4abe2275aa", "class_name": "RelatedNodeInfo"}}, "text": "The process results in a new version of the model\nwith updated weights. It is important to note that just like pre-training,\nfull fine-tuning requires enough memory and compute budget to store and\nprocess all the gradients, optimizers, and other components being updated\nduring training.\n\n### Parameter-efficient fine-tuning\n\nTraining a language model is a computationally intensive task. For a full LLM\nfine-tuning, you need memory not only to store the model, but also the\nparameters that are necessary for the training process. Your computer might be\nable to handle the model weights, but allocating memory for optimizing states,\ngradients, and forward activations during the training process is a\nchallenging task. Simple hardware cannot handle this amount of hurdle. This is\nwhere PEFT is crucial. While full LLM fine-tuning updates every model's weight\nduring the supervised learning process, **PEFT methods only update a small set\nof parameters**. This transfer learning technique chooses specific model\ncomponents and \"freezes\" the rest of the parameters. The result is logically\nhaving a much smaller number of parameters than in the original model (in some\ncases, just 15-20% of the original weights; LoRA can reduce the number of\ntrainable parameters by 10,000 times).", "start_char_idx": 14687, "end_char_idx": 15966, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a36ca3a-34f2-45b1-9def-e832893dc387": {"__data__": {"id_": "8a36ca3a-34f2-45b1-9def-e832893dc387", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f914ba8-86fe-4391-8260-71cff2a54df7", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "cb203cf9e858d47be0c25b0a2a1d1966e4c894cb0243f31fe82d93645b724cbd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83fb0a1a-4a22-48f1-940e-4213ad6a4ab5", "node_type": "1", "metadata": {}, "hash": "1c6782086a5c40c555aeaf493dc4cbbd2697c9a9e187e83b09a68834d954a14a", "class_name": "RelatedNodeInfo"}}, "text": "For a full LLM\nfine-tuning, you need memory not only to store the model, but also the\nparameters that are necessary for the training process. Your computer might be\nable to handle the model weights, but allocating memory for optimizing states,\ngradients, and forward activations during the training process is a\nchallenging task. Simple hardware cannot handle this amount of hurdle. This is\nwhere PEFT is crucial. While full LLM fine-tuning updates every model's weight\nduring the supervised learning process, **PEFT methods only update a small set\nof parameters**. This transfer learning technique chooses specific model\ncomponents and \"freezes\" the rest of the parameters. The result is logically\nhaving a much smaller number of parameters than in the original model (in some\ncases, just 15-20% of the original weights; LoRA can reduce the number of\ntrainable parameters by 10,000 times). This makes memory requirements much\nmore manageable. Not only that, but PEFT is also dealing with catastrophic\nforgetting. Since it's not touching the original LLM, the model does not\nforget the previously learned information. Full fine-tuning results in a new\nversion of the model for every task you train on.", "start_char_idx": 15076, "end_char_idx": 16277, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83fb0a1a-4a22-48f1-940e-4213ad6a4ab5": {"__data__": {"id_": "83fb0a1a-4a22-48f1-940e-4213ad6a4ab5", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a36ca3a-34f2-45b1-9def-e832893dc387", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "04618021b18670ef96598157cf816b453b80a0a7042fe308018d66299ef89cb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3523fb4e-129f-49f4-a394-9daa7e810301", "node_type": "1", "metadata": {}, "hash": "22768f67a089cae126734f342302f9057e61c3538315229bc765dbcc07b24969", "class_name": "RelatedNodeInfo"}}, "text": "Simple hardware cannot handle this amount of hurdle. This is\nwhere PEFT is crucial. While full LLM fine-tuning updates every model's weight\nduring the supervised learning process, **PEFT methods only update a small set\nof parameters**. This transfer learning technique chooses specific model\ncomponents and \"freezes\" the rest of the parameters. The result is logically\nhaving a much smaller number of parameters than in the original model (in some\ncases, just 15-20% of the original weights; LoRA can reduce the number of\ntrainable parameters by 10,000 times). This makes memory requirements much\nmore manageable. Not only that, but PEFT is also dealing with catastrophic\nforgetting. Since it's not touching the original LLM, the model does not\nforget the previously learned information. Full fine-tuning results in a new\nversion of the model for every task you train on. Each of these is the same\nsize as the original model, so it can create an expensive storage problem if\nyou're fine-tuning for multiple tasks.\n\n### Other types of fine-tuning\n\nLet's learn a few more types of learning:\n\n **Transfer learning:** Transfer learning is about taking the model that had\nlearned on general-purpose, massive datasets and training it on distinct,\ntask-specific data.", "start_char_idx": 15406, "end_char_idx": 16666, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3523fb4e-129f-49f4-a394-9daa7e810301": {"__data__": {"id_": "3523fb4e-129f-49f4-a394-9daa7e810301", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83fb0a1a-4a22-48f1-940e-4213ad6a4ab5", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "0c2913e878ba4fa510cc0fa761fa968e9134acb0b01df8f8f0791db5c2220349", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6148fe8-35b4-4408-b743-3a1abccc07a5", "node_type": "1", "metadata": {}, "hash": "2ee8021b248dda6b29074032866d81e142b8188950b7a2b2835d31b87a5aa815", "class_name": "RelatedNodeInfo"}}, "text": "This makes memory requirements much\nmore manageable. Not only that, but PEFT is also dealing with catastrophic\nforgetting. Since it's not touching the original LLM, the model does not\nforget the previously learned information. Full fine-tuning results in a new\nversion of the model for every task you train on. Each of these is the same\nsize as the original model, so it can create an expensive storage problem if\nyou're fine-tuning for multiple tasks.\n\n### Other types of fine-tuning\n\nLet's learn a few more types of learning:\n\n **Transfer learning:** Transfer learning is about taking the model that had\nlearned on general-purpose, massive datasets and training it on distinct,\ntask-specific data. This dataset may include labeled examples related to that\ndomain. Transfer learning is used when there is not enough data or a lack of\ntime to train data; the main advantage of it is that it offers a higher\nlearning rate and accuracy after training. You can take existing LLMs that are\npre-trained on vast amounts of data, like GPT \u00c2\u00be and BERT, and customize them\nfor your own use case.", "start_char_idx": 15967, "end_char_idx": 17053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6148fe8-35b4-4408-b743-3a1abccc07a5": {"__data__": {"id_": "f6148fe8-35b4-4408-b743-3a1abccc07a5", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3523fb4e-129f-49f4-a394-9daa7e810301", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "591f3ce2871fceb81b9cea5925649ce6530ff1e73d76302b3d2de02ee1091078", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01dc11d3-78ae-4e07-93c4-09ea9af5801d", "node_type": "1", "metadata": {}, "hash": "8db5aed8051600d19bab19d4a1141f1561f89c3dd35c3b23b0475c75171bbae8", "class_name": "RelatedNodeInfo"}}, "text": "Full fine-tuning results in a new\nversion of the model for every task you train on. Each of these is the same\nsize as the original model, so it can create an expensive storage problem if\nyou're fine-tuning for multiple tasks.\n\n### Other types of fine-tuning\n\nLet's learn a few more types of learning:\n\n **Transfer learning:** Transfer learning is about taking the model that had\nlearned on general-purpose, massive datasets and training it on distinct,\ntask-specific data. This dataset may include labeled examples related to that\ndomain. Transfer learning is used when there is not enough data or a lack of\ntime to train data; the main advantage of it is that it offers a higher\nlearning rate and accuracy after training. You can take existing LLMs that are\npre-trained on vast amounts of data, like GPT \u00c2\u00be and BERT, and customize them\nfor your own use case.\n\n **Task-specific fine-tuning:** Task-specific fine-tuning is a method where\nthe pre-trained model is fine-tuned on a specific task or domain using a\ndataset designed for that domain. This method requires more data and time than\ntransfer learning but can result in higher performance on the specific task.\n\nFor example, translation using a dataset of examples for that task.\nInterestingly, good results can be achieved with relatively few examples.", "start_char_idx": 16194, "end_char_idx": 17502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01dc11d3-78ae-4e07-93c4-09ea9af5801d": {"__data__": {"id_": "01dc11d3-78ae-4e07-93c4-09ea9af5801d", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6148fe8-35b4-4408-b743-3a1abccc07a5", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "d3148164ec187515355d59d59281b2284508aef2646cb4fc4fd093caf4597381", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f49e87f-4462-460e-94cd-4c73e0f81a38", "node_type": "1", "metadata": {}, "hash": "8042539dfcd1e921aa2ea8aecf8b3ae91c1c8bc8aedf729ec5d7950a864c0336", "class_name": "RelatedNodeInfo"}}, "text": "This dataset may include labeled examples related to that\ndomain. Transfer learning is used when there is not enough data or a lack of\ntime to train data; the main advantage of it is that it offers a higher\nlearning rate and accuracy after training. You can take existing LLMs that are\npre-trained on vast amounts of data, like GPT \u00c2\u00be and BERT, and customize them\nfor your own use case.\n\n **Task-specific fine-tuning:** Task-specific fine-tuning is a method where\nthe pre-trained model is fine-tuned on a specific task or domain using a\ndataset designed for that domain. This method requires more data and time than\ntransfer learning but can result in higher performance on the specific task.\n\nFor example, translation using a dataset of examples for that task.\nInterestingly, good results can be achieved with relatively few examples.\nOften, just a few hundred or thousand examples can result in good performance\ncompared to the billions of pieces of text that the model saw during its pre-\ntraining phase. However, there is a potential downside to fine-tuning on a\nsingle task. The process may lead to a phenomenon called **catastrophic\nforgetting**.\n\nCatastrophic forgetting happens because the full fine-tuning process modifies\nthe weights of the original LLM.", "start_char_idx": 16667, "end_char_idx": 17931, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f49e87f-4462-460e-94cd-4c73e0f81a38": {"__data__": {"id_": "1f49e87f-4462-460e-94cd-4c73e0f81a38", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01dc11d3-78ae-4e07-93c4-09ea9af5801d", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "17fc52655853ecf55f497878b41cf7dfa1cb543266a3d821faa40ba175b4ce42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65cc9163-2a7d-43b3-9c34-a5e1d12b10be", "node_type": "1", "metadata": {}, "hash": "e3398d334be5e7e8345167d96c779e5f259881e0958c0b2b793924f3e6d53ab1", "class_name": "RelatedNodeInfo"}}, "text": "**Task-specific fine-tuning:** Task-specific fine-tuning is a method where\nthe pre-trained model is fine-tuned on a specific task or domain using a\ndataset designed for that domain. This method requires more data and time than\ntransfer learning but can result in higher performance on the specific task.\n\nFor example, translation using a dataset of examples for that task.\nInterestingly, good results can be achieved with relatively few examples.\nOften, just a few hundred or thousand examples can result in good performance\ncompared to the billions of pieces of text that the model saw during its pre-\ntraining phase. However, there is a potential downside to fine-tuning on a\nsingle task. The process may lead to a phenomenon called **catastrophic\nforgetting**.\n\nCatastrophic forgetting happens because the full fine-tuning process modifies\nthe weights of the original LLM. While this leads to great performance on a\nsingle fine-tuning task, it can degrade performance on other tasks. For\nexample, while fine-tuning can improve the ability of a model to perform\ncertain NLP tasks like [sentiment\nanalysis](https://www.superannotate.com/blog/sentiment-analysis-explained) and\nresult in\u00c2 quality completion, the model may forget how to do other tasks.", "start_char_idx": 17056, "end_char_idx": 18307, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65cc9163-2a7d-43b3-9c34-a5e1d12b10be": {"__data__": {"id_": "65cc9163-2a7d-43b3-9c34-a5e1d12b10be", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f49e87f-4462-460e-94cd-4c73e0f81a38", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "daaa0b89229b0fef45420f5b64a852b445c6f6a8ce8faee2eb6c2108094f2e75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbf38aa9-72c3-43e3-861b-62fb00774098", "node_type": "1", "metadata": {}, "hash": "8c95817fb68074c8733c9a61ae60c43c8544867c38881aa7a232871391e495c9", "class_name": "RelatedNodeInfo"}}, "text": "Interestingly, good results can be achieved with relatively few examples.\nOften, just a few hundred or thousand examples can result in good performance\ncompared to the billions of pieces of text that the model saw during its pre-\ntraining phase. However, there is a potential downside to fine-tuning on a\nsingle task. The process may lead to a phenomenon called **catastrophic\nforgetting**.\n\nCatastrophic forgetting happens because the full fine-tuning process modifies\nthe weights of the original LLM. While this leads to great performance on a\nsingle fine-tuning task, it can degrade performance on other tasks. For\nexample, while fine-tuning can improve the ability of a model to perform\ncertain NLP tasks like [sentiment\nanalysis](https://www.superannotate.com/blog/sentiment-analysis-explained) and\nresult in\u00c2 quality completion, the model may forget how to do other tasks.\nThis model knew how to carry out named entity recognition before fine-tuning\ncorrectly identifying.\n\n **Multi-task learning:** Multi-task fine-tuning is an extension of single-\ntask fine-tuning, where the training dataset consists of example inputs and\noutputs for multiple tasks.", "start_char_idx": 17429, "end_char_idx": 18588, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbf38aa9-72c3-43e3-861b-62fb00774098": {"__data__": {"id_": "cbf38aa9-72c3-43e3-861b-62fb00774098", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65cc9163-2a7d-43b3-9c34-a5e1d12b10be", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "c287e3e34deaeb2439b9be54970feafa7f6348d24399d35f03f057547af2cc81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbb64c94-8642-456b-a6f0-8c7dfe44e6fc", "node_type": "1", "metadata": {}, "hash": "8bc5d23ced5834730921bf2920d3f2869607ff7765b3346c46586a06793b6a2a", "class_name": "RelatedNodeInfo"}}, "text": "The process may lead to a phenomenon called **catastrophic\nforgetting**.\n\nCatastrophic forgetting happens because the full fine-tuning process modifies\nthe weights of the original LLM. While this leads to great performance on a\nsingle fine-tuning task, it can degrade performance on other tasks. For\nexample, while fine-tuning can improve the ability of a model to perform\ncertain NLP tasks like [sentiment\nanalysis](https://www.superannotate.com/blog/sentiment-analysis-explained) and\nresult in\u00c2 quality completion, the model may forget how to do other tasks.\nThis model knew how to carry out named entity recognition before fine-tuning\ncorrectly identifying.\n\n **Multi-task learning:** Multi-task fine-tuning is an extension of single-\ntask fine-tuning, where the training dataset consists of example inputs and\noutputs for multiple tasks. Here, the dataset contains examples that instruct\nthe model to carry out a variety of tasks, including summarization, review\nrating, code translation, and entity recognition. You train the model on this\nmixed dataset so that it can improve the performance of the model on all the\ntasks simultaneously, thus avoiding the issue of catastrophic forgetting.", "start_char_idx": 17747, "end_char_idx": 18942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbb64c94-8642-456b-a6f0-8c7dfe44e6fc": {"__data__": {"id_": "cbb64c94-8642-456b-a6f0-8c7dfe44e6fc", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbf38aa9-72c3-43e3-861b-62fb00774098", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "3f078e59f7539467b60ef3b254392a271c2d78e26a4fdb05c04713c70b874974", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49bff0a3-ce68-4285-8d73-6e6fc893f5b9", "node_type": "1", "metadata": {}, "hash": "c318b0ac1aa7c3d8f7d1e3f3e22f21ca65325b41f900f02c5d5dd0263160cba4", "class_name": "RelatedNodeInfo"}}, "text": "For\nexample, while fine-tuning can improve the ability of a model to perform\ncertain NLP tasks like [sentiment\nanalysis](https://www.superannotate.com/blog/sentiment-analysis-explained) and\nresult in\u00c2 quality completion, the model may forget how to do other tasks.\nThis model knew how to carry out named entity recognition before fine-tuning\ncorrectly identifying.\n\n **Multi-task learning:** Multi-task fine-tuning is an extension of single-\ntask fine-tuning, where the training dataset consists of example inputs and\noutputs for multiple tasks. Here, the dataset contains examples that instruct\nthe model to carry out a variety of tasks, including summarization, review\nrating, code translation, and entity recognition. You train the model on this\nmixed dataset so that it can improve the performance of the model on all the\ntasks simultaneously, thus avoiding the issue of catastrophic forgetting. Over\nmany epochs of training, the calculated losses across examples are used to\nupdate the weights of the model, resulting in a fine-tuned model that knows\nhow to be good at many different tasks simultaneously. One drawback of multi-\ntask fine-tuned models is that they require a lot of data. You may need as\nmany as 50-100,000 examples in your training set.", "start_char_idx": 18043, "end_char_idx": 19301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49bff0a3-ce68-4285-8d73-6e6fc893f5b9": {"__data__": {"id_": "49bff0a3-ce68-4285-8d73-6e6fc893f5b9", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbb64c94-8642-456b-a6f0-8c7dfe44e6fc", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "c522ad865a17c127d857afbc1ad66451722ac627faadab18a916d21253f66d38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0dfd3540-93b3-4a0e-a548-0cf68d4623f1", "node_type": "1", "metadata": {}, "hash": "65c811a79e37a59f308c5a7dcebdf6b0fd5d5fb90bdbe16fb794ef14439da771", "class_name": "RelatedNodeInfo"}}, "text": "**Multi-task learning:** Multi-task fine-tuning is an extension of single-\ntask fine-tuning, where the training dataset consists of example inputs and\noutputs for multiple tasks. Here, the dataset contains examples that instruct\nthe model to carry out a variety of tasks, including summarization, review\nrating, code translation, and entity recognition. You train the model on this\nmixed dataset so that it can improve the performance of the model on all the\ntasks simultaneously, thus avoiding the issue of catastrophic forgetting. Over\nmany epochs of training, the calculated losses across examples are used to\nupdate the weights of the model, resulting in a fine-tuned model that knows\nhow to be good at many different tasks simultaneously. One drawback of multi-\ntask fine-tuned models is that they require a lot of data. You may need as\nmany as 50-100,000 examples in your training set. However, assembling this\ndata can be really worthwhile and worth the effort. The resulting models are\noften very capable and suitable for use in situations where good performance\nat many tasks is desirable.\n\n **Sequential fine-tuning:** Sequential fine-tuning is about sequentially\nadapting a pre-trained model on several related tasks.", "start_char_idx": 18410, "end_char_idx": 19638, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0dfd3540-93b3-4a0e-a548-0cf68d4623f1": {"__data__": {"id_": "0dfd3540-93b3-4a0e-a548-0cf68d4623f1", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49bff0a3-ce68-4285-8d73-6e6fc893f5b9", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "f0673c29c742361b10ee9477a4ab6e2e5849bfbce72aa49e021e235b91c24abf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32b9570b-acf7-4fca-bd57-681e32fdd8b1", "node_type": "1", "metadata": {}, "hash": "970812fb31b2f0e316bfdcbb2ef194a804436b5cb15e0bf1e931f767eeed6c92", "class_name": "RelatedNodeInfo"}}, "text": "You train the model on this\nmixed dataset so that it can improve the performance of the model on all the\ntasks simultaneously, thus avoiding the issue of catastrophic forgetting. Over\nmany epochs of training, the calculated losses across examples are used to\nupdate the weights of the model, resulting in a fine-tuned model that knows\nhow to be good at many different tasks simultaneously. One drawback of multi-\ntask fine-tuned models is that they require a lot of data. You may need as\nmany as 50-100,000 examples in your training set. However, assembling this\ndata can be really worthwhile and worth the effort. The resulting models are\noften very capable and suitable for use in situations where good performance\nat many tasks is desirable.\n\n **Sequential fine-tuning:** Sequential fine-tuning is about sequentially\nadapting a pre-trained model on several related tasks. After the initial\ntransfer to a general domain, the LLM might be fine-tuned on a more specific\nsubset. For instance, it can be fine-tuned from general language to medical\nlanguage and then from medical language to pediatric cardiology.", "start_char_idx": 18764, "end_char_idx": 19874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32b9570b-acf7-4fca-bd57-681e32fdd8b1": {"__data__": {"id_": "32b9570b-acf7-4fca-bd57-681e32fdd8b1", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0dfd3540-93b3-4a0e-a548-0cf68d4623f1", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "4019051414692c35f4783fedef0e8f56183b7d9c9aaf5d6780ba38e5f9ad6334", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd7fb78e-9768-4d1b-84fd-40a0caef149f", "node_type": "1", "metadata": {}, "hash": "6bcf82c05ac75e3ef532f73a05a2a1920940af12c98505d239c60974c2b8457a", "class_name": "RelatedNodeInfo"}}, "text": "One drawback of multi-\ntask fine-tuned models is that they require a lot of data. You may need as\nmany as 50-100,000 examples in your training set. However, assembling this\ndata can be really worthwhile and worth the effort. The resulting models are\noften very capable and suitable for use in situations where good performance\nat many tasks is desirable.\n\n **Sequential fine-tuning:** Sequential fine-tuning is about sequentially\nadapting a pre-trained model on several related tasks. After the initial\ntransfer to a general domain, the LLM might be fine-tuned on a more specific\nsubset. For instance, it can be fine-tuned from general language to medical\nlanguage and then from medical language to pediatric cardiology.\n\nNote that there are other fine-tuning examples \u00e2\u0080\u0093 adaptive, behavioral, and\ninstruction, [reinforced fine-\ntuning](https://www.superannotate.com/blog/reinforced-fine-tuning) of large\nlanguage models. These cover some important specific cases for training\nlanguage models.\n\n### Retrieval augmented generation (RAG)\n\n[Retrieval augmented generation (RAG)](https://www.superannotate.com/blog/rag-\nexplained) is a well-known alternative to fine-tuning and is a combination of\nnatural language generation and information retrieval.", "start_char_idx": 19154, "end_char_idx": 20403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd7fb78e-9768-4d1b-84fd-40a0caef149f": {"__data__": {"id_": "fd7fb78e-9768-4d1b-84fd-40a0caef149f", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32b9570b-acf7-4fca-bd57-681e32fdd8b1", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "d622515d0595d5756418f4635a82709d7ffe93611b436a1ed51fdc9431c53568", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eace7e82-9a0f-48b3-baee-bee0b7657b63", "node_type": "1", "metadata": {}, "hash": "964289f3ba9e212b48516129cc245c8fb6f25c4b50dade40c798141cb7fd6a9d", "class_name": "RelatedNodeInfo"}}, "text": "**Sequential fine-tuning:** Sequential fine-tuning is about sequentially\nadapting a pre-trained model on several related tasks. After the initial\ntransfer to a general domain, the LLM might be fine-tuned on a more specific\nsubset. For instance, it can be fine-tuned from general language to medical\nlanguage and then from medical language to pediatric cardiology.\n\nNote that there are other fine-tuning examples \u00e2\u0080\u0093 adaptive, behavioral, and\ninstruction, [reinforced fine-\ntuning](https://www.superannotate.com/blog/reinforced-fine-tuning) of large\nlanguage models. These cover some important specific cases for training\nlanguage models.\n\n### Retrieval augmented generation (RAG)\n\n[Retrieval augmented generation (RAG)](https://www.superannotate.com/blog/rag-\nexplained) is a well-known alternative to fine-tuning and is a combination of\nnatural language generation and information retrieval. RAG ensures that\nlanguage models are grounded by external up-to-date knowledge sources/relevant\ndocuments and provides sources. This technique bridges the gap between\ngeneral-purpose models' vast knowledge and the need for precise, up-to-date\ninformation with rich context. Thus, RAG is an essential technique for\nsituations where facts can evolve over time.", "start_char_idx": 19511, "end_char_idx": 20762, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eace7e82-9a0f-48b3-baee-bee0b7657b63": {"__data__": {"id_": "eace7e82-9a0f-48b3-baee-bee0b7657b63", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd7fb78e-9768-4d1b-84fd-40a0caef149f", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "472d3bfb0cc117b3c40094b1ea6fa9e9060c734bba2ba6cc4da1fb3d5d4b1425", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c79289c8-e668-4e12-9c36-6cf92b14360e", "node_type": "1", "metadata": {}, "hash": "7c32233ae8a1c3b49d9081cc1cd84943ccd38f9a854a96d304f8f902e21e8412", "class_name": "RelatedNodeInfo"}}, "text": "Note that there are other fine-tuning examples \u00e2\u0080\u0093 adaptive, behavioral, and\ninstruction, [reinforced fine-\ntuning](https://www.superannotate.com/blog/reinforced-fine-tuning) of large\nlanguage models. These cover some important specific cases for training\nlanguage models.\n\n### Retrieval augmented generation (RAG)\n\n[Retrieval augmented generation (RAG)](https://www.superannotate.com/blog/rag-\nexplained) is a well-known alternative to fine-tuning and is a combination of\nnatural language generation and information retrieval. RAG ensures that\nlanguage models are grounded by external up-to-date knowledge sources/relevant\ndocuments and provides sources. This technique bridges the gap between\ngeneral-purpose models' vast knowledge and the need for precise, up-to-date\ninformation with rich context. Thus, RAG is an essential technique for\nsituations where facts can evolve over time.\n[Grok](https://www.superannotate.com/blog/grok-ai-elon-musk), the recent\ninvention of xAI, uses RAG techniques to ensure its information is fresh and\ncurrent.\n\n!", "start_char_idx": 19876, "end_char_idx": 20924, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c79289c8-e668-4e12-9c36-6cf92b14360e": {"__data__": {"id_": "c79289c8-e668-4e12-9c36-6cf92b14360e", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eace7e82-9a0f-48b3-baee-bee0b7657b63", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "f548184a29f46a93c7800c438da09bb3f83afa3a70278afeb2c58cdb26bbb8a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc3d174a-81c7-4960-ba16-e1b6c46e2834", "node_type": "1", "metadata": {}, "hash": "550164ee93d0dcc0dc619da9ab094b6a250496b46ba42dcfc23ca74d3eea4ed5", "class_name": "RelatedNodeInfo"}}, "text": "These cover some important specific cases for training\nlanguage models.\n\n### Retrieval augmented generation (RAG)\n\n[Retrieval augmented generation (RAG)](https://www.superannotate.com/blog/rag-\nexplained) is a well-known alternative to fine-tuning and is a combination of\nnatural language generation and information retrieval. RAG ensures that\nlanguage models are grounded by external up-to-date knowledge sources/relevant\ndocuments and provides sources. This technique bridges the gap between\ngeneral-purpose models' vast knowledge and the need for precise, up-to-date\ninformation with rich context. Thus, RAG is an essential technique for\nsituations where facts can evolve over time.\n[Grok](https://www.superannotate.com/blog/grok-ai-elon-musk), the recent\ninvention of xAI, uses RAG techniques to ensure its information is fresh and\ncurrent.\n\n![retrieval augmented generation](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7aa718561628120ca45c1_retrieval-\naugmented-generation.webp)\n\nOne advantage that RAG has over fine-tuning is information management.\nTraditional fine-tuning embeds data into the model's architecture, essentially\n'hardwriting' the knowledge, which prevents easy modification.", "start_char_idx": 20077, "end_char_idx": 21299, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc3d174a-81c7-4960-ba16-e1b6c46e2834": {"__data__": {"id_": "cc3d174a-81c7-4960-ba16-e1b6c46e2834", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c79289c8-e668-4e12-9c36-6cf92b14360e", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "c80a5e1764c875329fb211284e93e731ee428c450e65c5202c14cb3c1a77db1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23a0eef4-12af-4b6c-b0ab-15d963aefa6d", "node_type": "1", "metadata": {}, "hash": "1e6bfd4c2084ce24c619c94dc567116d434b6056f38b39fdb8464f88e306cdd9", "class_name": "RelatedNodeInfo"}}, "text": "This technique bridges the gap between\ngeneral-purpose models' vast knowledge and the need for precise, up-to-date\ninformation with rich context. Thus, RAG is an essential technique for\nsituations where facts can evolve over time.\n[Grok](https://www.superannotate.com/blog/grok-ai-elon-musk), the recent\ninvention of xAI, uses RAG techniques to ensure its information is fresh and\ncurrent.\n\n![retrieval augmented generation](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7aa718561628120ca45c1_retrieval-\naugmented-generation.webp)\n\nOne advantage that RAG has over fine-tuning is information management.\nTraditional fine-tuning embeds data into the model's architecture, essentially\n'hardwriting' the knowledge, which prevents easy modification. On the other\nhand, RAG permits continuous updates in training data and allows\nremoval/revision of data, ensuring the model remains current and accurate.\n\nIn the context of language models, RAG and fine-tuning are often perceived as\ncompeting methods. However, their combined use can lead to significantly\nenhanced performance.", "start_char_idx": 20532, "end_char_idx": 21626, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23a0eef4-12af-4b6c-b0ab-15d963aefa6d": {"__data__": {"id_": "23a0eef4-12af-4b6c-b0ab-15d963aefa6d", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc3d174a-81c7-4960-ba16-e1b6c46e2834", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "fdf0b9a239a9f1cc80496030b07178b12ed85f828e677378a737fdb43d326ef9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9619a267-a0f1-44d7-b79b-6a56d3e8039f", "node_type": "1", "metadata": {}, "hash": "f29f8bdedc5f58f5bdd594cd60273ee6399a58471b2a6403fa514756e700fd63", "class_name": "RelatedNodeInfo"}}, "text": "![retrieval augmented generation](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7aa718561628120ca45c1_retrieval-\naugmented-generation.webp)\n\nOne advantage that RAG has over fine-tuning is information management.\nTraditional fine-tuning embeds data into the model's architecture, essentially\n'hardwriting' the knowledge, which prevents easy modification. On the other\nhand, RAG permits continuous updates in training data and allows\nremoval/revision of data, ensuring the model remains current and accurate.\n\nIn the context of language models, RAG and fine-tuning are often perceived as\ncompeting methods. However, their combined use can lead to significantly\nenhanced performance. Particularly, [fine-tuning can be applied to RAG\nsystems](https://www.superannotate.com/blog/rag-fine-tuning) to identify and\nimprove their weaker components, helping them excel at specific LLM tasks.\n\n## Fine-tuning in SuperAnnotate\n\nChoosing the right tool means ensuring your AI understands exactly what you\nneed, which can save you time, money, and protect your reputation.", "start_char_idx": 20923, "end_char_idx": 22004, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9619a267-a0f1-44d7-b79b-6a56d3e8039f": {"__data__": {"id_": "9619a267-a0f1-44d7-b79b-6a56d3e8039f", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23a0eef4-12af-4b6c-b0ab-15d963aefa6d", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "1d26691d143e94152996d564f37a51126f5d9db144c48f77f5239108dd2c2d95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f448b4c5-38ee-45eb-a835-d718198d070e", "node_type": "1", "metadata": {}, "hash": "29a7fadfcc92346ea727fd70052a38ebb5895b8c310140f7844a16b82759dabe", "class_name": "RelatedNodeInfo"}}, "text": "Traditional fine-tuning embeds data into the model's architecture, essentially\n'hardwriting' the knowledge, which prevents easy modification. On the other\nhand, RAG permits continuous updates in training data and allows\nremoval/revision of data, ensuring the model remains current and accurate.\n\nIn the context of language models, RAG and fine-tuning are often perceived as\ncompeting methods. However, their combined use can lead to significantly\nenhanced performance. Particularly, [fine-tuning can be applied to RAG\nsystems](https://www.superannotate.com/blog/rag-fine-tuning) to identify and\nimprove their weaker components, helping them excel at specific LLM tasks.\n\n## Fine-tuning in SuperAnnotate\n\nChoosing the right tool means ensuring your AI understands exactly what you\nneed, which can save you time, money, and protect your reputation. Look at the\n[Air Canada situation](https://www.theguardian.com/world/2024/feb/16/air-\ncanada-chatbot-lawsuit), for example. Their AI chatbot\n[hallucinated](https://www.superannotate.com/blog/ai-hallucinations) and gave\na customer incorrect information, misleading him into buying full-price\nticket.", "start_char_idx": 21158, "end_char_idx": 22303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f448b4c5-38ee-45eb-a835-d718198d070e": {"__data__": {"id_": "f448b4c5-38ee-45eb-a835-d718198d070e", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9619a267-a0f1-44d7-b79b-6a56d3e8039f", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "d611f8473f3eca6df679c7bed065b04f29896a2d9f55a384013ed0f6763fcf6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bab3e682-c0e4-4177-a8b1-59831676d909", "node_type": "1", "metadata": {}, "hash": "524acfaaedd0c724b38a5fbb9a4490ca7ac3232f3de11ebb09c27c1b01203f63", "class_name": "RelatedNodeInfo"}}, "text": "However, their combined use can lead to significantly\nenhanced performance. Particularly, [fine-tuning can be applied to RAG\nsystems](https://www.superannotate.com/blog/rag-fine-tuning) to identify and\nimprove their weaker components, helping them excel at specific LLM tasks.\n\n## Fine-tuning in SuperAnnotate\n\nChoosing the right tool means ensuring your AI understands exactly what you\nneed, which can save you time, money, and protect your reputation. Look at the\n[Air Canada situation](https://www.theguardian.com/world/2024/feb/16/air-\ncanada-chatbot-lawsuit), for example. Their AI chatbot\n[hallucinated](https://www.superannotate.com/blog/ai-hallucinations) and gave\na customer incorrect information, misleading him into buying full-price\nticket. While we can't pin it down to fine-tuning for sure, it's likely that\nbetter fine-tuning might have avoided the problem. This just shows how crucial\nit is to pick a fine-tuning tool that ensures your AI works just right. It's\nprecisely situations like these where SuperAnnotate steps in to make a\ndifference.", "start_char_idx": 21551, "end_char_idx": 22611, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bab3e682-c0e4-4177-a8b1-59831676d909": {"__data__": {"id_": "bab3e682-c0e4-4177-a8b1-59831676d909", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f448b4c5-38ee-45eb-a835-d718198d070e", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "ad5141c36b0e44e2414a9f190b8d52ba3ded2252286ae158a84983fdcbc70ff8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63b1dec9-b8bb-4e9f-82fb-0735584a1c22", "node_type": "1", "metadata": {}, "hash": "25cf5132328932bab5cb9bdbe7107038e1157991acd63c8d7713c534112ff2bf", "class_name": "RelatedNodeInfo"}}, "text": "## Fine-tuning in SuperAnnotate\n\nChoosing the right tool means ensuring your AI understands exactly what you\nneed, which can save you time, money, and protect your reputation. Look at the\n[Air Canada situation](https://www.theguardian.com/world/2024/feb/16/air-\ncanada-chatbot-lawsuit), for example. Their AI chatbot\n[hallucinated](https://www.superannotate.com/blog/ai-hallucinations) and gave\na customer incorrect information, misleading him into buying full-price\nticket. While we can't pin it down to fine-tuning for sure, it's likely that\nbetter fine-tuning might have avoided the problem. This just shows how crucial\nit is to pick a fine-tuning tool that ensures your AI works just right. It's\nprecisely situations like these where SuperAnnotate steps in to make a\ndifference.\n\nSuperAnnotate's LLM tool provides a cutting-edge approach to designing optimal\ntraining data for fine-tuning language models. Through its highly customizable\nLLM editor, users are given a comprehensive platform to create a broad\nspectrum of LLM use cases tailored to specific business needs.", "start_char_idx": 21829, "end_char_idx": 22904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63b1dec9-b8bb-4e9f-82fb-0735584a1c22": {"__data__": {"id_": "63b1dec9-b8bb-4e9f-82fb-0735584a1c22", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bab3e682-c0e4-4177-a8b1-59831676d909", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "4ab1e5a52675964a4790e8b1e9c3e8d41a530a85e8e46e78b13e1bd14ca0b139", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4a089cc-2bd8-4db5-8ec9-41ed5bc5102f", "node_type": "1", "metadata": {}, "hash": "bb039a31697d1c7ce6f136f585328e960d5d552bf8188bf8b0d5b9369532a8ab", "class_name": "RelatedNodeInfo"}}, "text": "Their AI chatbot\n[hallucinated](https://www.superannotate.com/blog/ai-hallucinations) and gave\na customer incorrect information, misleading him into buying full-price\nticket. While we can't pin it down to fine-tuning for sure, it's likely that\nbetter fine-tuning might have avoided the problem. This just shows how crucial\nit is to pick a fine-tuning tool that ensures your AI works just right. It's\nprecisely situations like these where SuperAnnotate steps in to make a\ndifference.\n\nSuperAnnotate's LLM tool provides a cutting-edge approach to designing optimal\ntraining data for fine-tuning language models. Through its highly customizable\nLLM editor, users are given a comprehensive platform to create a broad\nspectrum of LLM use cases tailored to specific business needs. As a result,\ncustomers can ensure that their training data is not only high-quality but\nalso directly aligned with the requirements of their projects.\n\nHere's what you need to know about SuperAnnotate's [LLM fine-tuning\ntool](https://www.superannotate.com/llms):\n\n  * Its fully customizable interface allows you to gather data for your specific use case efficiently. Even if it's unique.", "start_char_idx": 22129, "end_char_idx": 23292, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4a089cc-2bd8-4db5-8ec9-41ed5bc5102f": {"__data__": {"id_": "e4a089cc-2bd8-4db5-8ec9-41ed5bc5102f", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63b1dec9-b8bb-4e9f-82fb-0735584a1c22", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "f3aef25587b07332188bc9da699946df8a1dd4cbe56aabcfc822bb6cc377e0eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26463b83-57c4-4244-bb49-4e1c5b342160", "node_type": "1", "metadata": {}, "hash": "555ef9a4b49fcd3c4eb2c5c4279c9bac1c03e42e29839d38c25abe06a6c32234", "class_name": "RelatedNodeInfo"}}, "text": "This just shows how crucial\nit is to pick a fine-tuning tool that ensures your AI works just right. It's\nprecisely situations like these where SuperAnnotate steps in to make a\ndifference.\n\nSuperAnnotate's LLM tool provides a cutting-edge approach to designing optimal\ntraining data for fine-tuning language models. Through its highly customizable\nLLM editor, users are given a comprehensive platform to create a broad\nspectrum of LLM use cases tailored to specific business needs. As a result,\ncustomers can ensure that their training data is not only high-quality but\nalso directly aligned with the requirements of their projects.\n\nHere's what you need to know about SuperAnnotate's [LLM fine-tuning\ntool](https://www.superannotate.com/llms):\n\n  * Its fully customizable interface allows you to gather data for your specific use case efficiently. Even if it's unique.\n  * We work with a world-class team of experts and people management, which makes it a breeze to scale to hundreds or thousands of people.\n  * The analytics and insights of our platform are invaluable gems for our customers. It allows a better understanding of the data and enforces quality standards.\n  * API integrations make it easy to set up a model in the loop, AI feedback and much more.", "start_char_idx": 22424, "end_char_idx": 23686, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26463b83-57c4-4244-bb49-4e1c5b342160": {"__data__": {"id_": "26463b83-57c4-4244-bb49-4e1c5b342160", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4a089cc-2bd8-4db5-8ec9-41ed5bc5102f", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "f3a8af47a5e29990501a4766d9fb401461ce5fe9064ceabcb7f594e7f5f61d3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64be0457-cfaf-4b19-8165-4c5aa714e33d", "node_type": "1", "metadata": {}, "hash": "fd898f08c8275429e392763c69147081cd861e3349a82762e20747acbaa7c9ac", "class_name": "RelatedNodeInfo"}}, "text": "As a result,\ncustomers can ensure that their training data is not only high-quality but\nalso directly aligned with the requirements of their projects.\n\nHere's what you need to know about SuperAnnotate's [LLM fine-tuning\ntool](https://www.superannotate.com/llms):\n\n  * Its fully customizable interface allows you to gather data for your specific use case efficiently. Even if it's unique.\n  * We work with a world-class team of experts and people management, which makes it a breeze to scale to hundreds or thousands of people.\n  * The analytics and insights of our platform are invaluable gems for our customers. It allows a better understanding of the data and enforces quality standards.\n  * API integrations make it easy to set up a model in the loop, AI feedback and much more.\n\nThe tool has practical applications in various areas. The\n[playground](https://www.superannotate.com/llms-genai-playground) offers\ntemplates like\u00c2 [GPT fine-\ntuning](https://llm.superannotate.com/editor?url=https://github.com/superannotateai/custom-\nllm/blob/main/chatgpt-fine-tuning/chatgpt-fine-tuning-\ntemplate.json&reset=true),\u00c2 [chat\nrating](https://llm.superannotate.com/editor?url=https://github.", "start_char_idx": 22905, "end_char_idx": 24091, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64be0457-cfaf-4b19-8165-4c5aa714e33d": {"__data__": {"id_": "64be0457-cfaf-4b19-8165-4c5aa714e33d", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26463b83-57c4-4244-bb49-4e1c5b342160", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "a9b0970eea95d5161d4b0ef3f5f207059b408572b3265267c3eaca2cecfc1a8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac756579-76b6-4183-8b28-8f3a22d67544", "node_type": "1", "metadata": {}, "hash": "1851db5d048464a59e2db2657fd342a2b678b31042a7b6de2fd838d0e350df79", "class_name": "RelatedNodeInfo"}}, "text": "* We work with a world-class team of experts and people management, which makes it a breeze to scale to hundreds or thousands of people.\n  * The analytics and insights of our platform are invaluable gems for our customers. It allows a better understanding of the data and enforces quality standards.\n  * API integrations make it easy to set up a model in the loop, AI feedback and much more.\n\nThe tool has practical applications in various areas. The\n[playground](https://www.superannotate.com/llms-genai-playground) offers\ntemplates like\u00c2 [GPT fine-\ntuning](https://llm.superannotate.com/editor?url=https://github.com/superannotateai/custom-\nllm/blob/main/chatgpt-fine-tuning/chatgpt-fine-tuning-\ntemplate.json&reset=true),\u00c2 [chat\nrating](https://llm.superannotate.com/editor?url=https://github.com/superannotateai/custom-\nllm/blob/main/chat-rating/chat-rating-template.json&reset=true), using\u00c2 [RLHF\nfor image\ngeneration](https://llm.superannotate.com/editor?url=https://github.com/superannotateai/custom-\nllm/blob/main/rlhf-for-image-generation/rlhf-for-image-generation-\ntemplate.json&reset=true),", "start_char_idx": 23295, "end_char_idx": 24396, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac756579-76b6-4183-8b28-8f3a22d67544": {"__data__": {"id_": "ac756579-76b6-4183-8b28-8f3a22d67544", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64be0457-cfaf-4b19-8165-4c5aa714e33d", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "c22434c9a46de71fe64d59083a36a94d6d9d419a84fec14c0106a1b228e233ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1b1ad77-54f3-4cdb-bb79-29f2674028a1", "node_type": "1", "metadata": {}, "hash": "79091adb9d221678be3b871049250ff9cbcc9011ad786c18a6163b5625e8a7a3", "class_name": "RelatedNodeInfo"}}, "text": "The tool has practical applications in various areas. The\n[playground](https://www.superannotate.com/llms-genai-playground) offers\ntemplates like\u00c2 [GPT fine-\ntuning](https://llm.superannotate.com/editor?url=https://github.com/superannotateai/custom-\nllm/blob/main/chatgpt-fine-tuning/chatgpt-fine-tuning-\ntemplate.json&reset=true),\u00c2 [chat\nrating](https://llm.superannotate.com/editor?url=https://github.com/superannotateai/custom-\nllm/blob/main/chat-rating/chat-rating-template.json&reset=true), using\u00c2 [RLHF\nfor image\ngeneration](https://llm.superannotate.com/editor?url=https://github.com/superannotateai/custom-\nllm/blob/main/rlhf-for-image-generation/rlhf-for-image-generation-\ntemplate.json&reset=true),\u00c2 [model\ncomparison](https://llm.superannotate.com/editor?url=https://github.com/superannotateai/custom-\nllm/blob/main/model-comparison/model-comparison-template.json&reset=true),\u00c2\n[video\ncaptioning](https://llm.superannotate.com/editor?url=https:%2F%2Fgithub.", "start_char_idx": 23688, "end_char_idx": 24656, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1b1ad77-54f3-4cdb-bb79-29f2674028a1": {"__data__": {"id_": "e1b1ad77-54f3-4cdb-bb79-29f2674028a1", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac756579-76b6-4183-8b28-8f3a22d67544", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "d5ee6d890ab80e1d4542c621a53ead7c9b3440ef0ad7ebd8b2b8e862eef1cdca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dcb42538-6220-4772-b0a7-d8bea30d2331", "node_type": "1", "metadata": {}, "hash": "ad2b4cbfaddc4fae4a4ac80c101e4bd5712eb8bb0d49f1df3ef689971889a6e4", "class_name": "RelatedNodeInfo"}}, "text": "json&reset=true),\u00c2 [chat\nrating](https://llm.superannotate.com/editor?url=https://github.com/superannotateai/custom-\nllm/blob/main/chat-rating/chat-rating-template.json&reset=true), using\u00c2 [RLHF\nfor image\ngeneration](https://llm.superannotate.com/editor?url=https://github.com/superannotateai/custom-\nllm/blob/main/rlhf-for-image-generation/rlhf-for-image-generation-\ntemplate.json&reset=true),\u00c2 [model\ncomparison](https://llm.superannotate.com/editor?url=https://github.com/superannotateai/custom-\nllm/blob/main/model-comparison/model-comparison-template.json&reset=true),\u00c2\n[video\ncaptioning](https://llm.superannotate.com/editor?url=https:%2F%2Fgithub.com%2Fsuperannotateai%2Fcustom-\nllm%2Fblob%2Fmain%2Fvideo-captioning%2Fvideo-captioning-template.json),\u00c2\n[supervised fine-\ntuning](https://llm.superannotate.com/editor?url=https:%2F%2Fgithub.", "start_char_idx": 24002, "end_char_idx": 24847, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dcb42538-6220-4772-b0a7-d8bea30d2331": {"__data__": {"id_": "dcb42538-6220-4772-b0a7-d8bea30d2331", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1b1ad77-54f3-4cdb-bb79-29f2674028a1", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "a3c0b6c9900cdbe5b039239a78ba93705aae913f9a591b2c113dfc0b767fe4aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e199603d-31e1-4dfb-98a7-84b7373689a6", "node_type": "1", "metadata": {}, "hash": "6d02c35bc8fb829232b4e88b96d03c616585ab10b066841c83e7ec84a6dfe444", "class_name": "RelatedNodeInfo"}}, "text": "using\u00c2 [RLHF\nfor image\ngeneration](https://llm.superannotate.com/editor?url=https://github.com/superannotateai/custom-\nllm/blob/main/rlhf-for-image-generation/rlhf-for-image-generation-\ntemplate.json&reset=true),\u00c2 [model\ncomparison](https://llm.superannotate.com/editor?url=https://github.com/superannotateai/custom-\nllm/blob/main/model-comparison/model-comparison-template.json&reset=true),\u00c2\n[video\ncaptioning](https://llm.superannotate.com/editor?url=https:%2F%2Fgithub.com%2Fsuperannotateai%2Fcustom-\nllm%2Fblob%2Fmain%2Fvideo-captioning%2Fvideo-captioning-template.json),\u00c2\n[supervised fine-\ntuning](https://llm.superannotate.com/editor?url=https:%2F%2Fgithub.com%2Fsuperannotateai%2Fcustom-\nllm%2Fblob%2Fmain%2Fsupervised-fine-tuning%2Fsupervised-fine-tuning-\ntemplate.json), and more. More here means you can use the customizable tool to\nbuild your own use case.", "start_char_idx": 24184, "end_char_idx": 25051, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e199603d-31e1-4dfb-98a7-84b7373689a6": {"__data__": {"id_": "e199603d-31e1-4dfb-98a7-84b7373689a6", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dcb42538-6220-4772-b0a7-d8bea30d2331", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "7ae9a3d165c4c593ed6582cbc3a2a2d4bb6f22b2886feff5d8bcf2fb4f9c828f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3784e315-0244-45bb-b458-37f1036b454b", "node_type": "1", "metadata": {}, "hash": "5b680b10ab0c4b770382a7f81d80e83d30825e86ff6381664bac613b89f2e884", "class_name": "RelatedNodeInfo"}}, "text": "superannotate.com/editor?url=https://github.com/superannotateai/custom-\nllm/blob/main/model-comparison/model-comparison-template.json&reset=true),\u00c2\n[video\ncaptioning](https://llm.superannotate.com/editor?url=https:%2F%2Fgithub.com%2Fsuperannotateai%2Fcustom-\nllm%2Fblob%2Fmain%2Fvideo-captioning%2Fvideo-captioning-template.json),\u00c2\n[supervised fine-\ntuning](https://llm.superannotate.com/editor?url=https:%2F%2Fgithub.com%2Fsuperannotateai%2Fcustom-\nllm%2Fblob%2Fmain%2Fsupervised-fine-tuning%2Fsupervised-fine-tuning-\ntemplate.json), and more. More here means you can use the customizable tool to\nbuild your own use case. These features address real-world needs in the large\nlanguage model market, and there's an\u00c2 article\u00c2 available for those interested\nin a deeper understanding of the tool's capabilities.\n\n!", "start_char_idx": 24429, "end_char_idx": 25240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3784e315-0244-45bb-b458-37f1036b454b": {"__data__": {"id_": "3784e315-0244-45bb-b458-37f1036b454b", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e199603d-31e1-4dfb-98a7-84b7373689a6", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "daee68295119da5473483470057e050fc2ec6e05dc57a9506e8d6321cb46c938", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78d83296-f12c-4cce-93cb-97936f1faa5a", "node_type": "1", "metadata": {}, "hash": "d2b912e2db85ff8d8683d6ebe40768a690cc9978618f66f4b8b11371d9a43a0a", "class_name": "RelatedNodeInfo"}}, "text": "\u00c2\n[video\ncaptioning](https://llm.superannotate.com/editor?url=https:%2F%2Fgithub.com%2Fsuperannotateai%2Fcustom-\nllm%2Fblob%2Fmain%2Fvideo-captioning%2Fvideo-captioning-template.json),\u00c2\n[supervised fine-\ntuning](https://llm.superannotate.com/editor?url=https:%2F%2Fgithub.com%2Fsuperannotateai%2Fcustom-\nllm%2Fblob%2Fmain%2Fsupervised-fine-tuning%2Fsupervised-fine-tuning-\ntemplate.json), and more. More here means you can use the customizable tool to\nbuild your own use case. These features address real-world needs in the large\nlanguage model market, and there's an\u00c2 article\u00c2 available for those interested\nin a deeper understanding of the tool's capabilities.\n\n![fine tuning in superannotate](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65cde31026fed47d72a5eb23_fine-tuning-in-\nsuperannotate.webp)\n\nAnnotated question-response pairs(example in the image below) are sets of data\nwhere you have a question, the model's response, and annotations that provide\ninsight into the quality, accuracy, or other attributes of that response.", "start_char_idx": 24575, "end_char_idx": 25629, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78d83296-f12c-4cce-93cb-97936f1faa5a": {"__data__": {"id_": "78d83296-f12c-4cce-93cb-97936f1faa5a", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3784e315-0244-45bb-b458-37f1036b454b", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "843f065b76304d8aa716683e49086b41a4eb39878d0b2a0eab425d3778a1a4cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96e60dc1-83df-4021-9df4-e5974c186b56", "node_type": "1", "metadata": {}, "hash": "2d740c4d27dfe6a5a41c657201f5a23afc1bbca9705d4d5e3763288f12d734d2", "class_name": "RelatedNodeInfo"}}, "text": "json), and more. More here means you can use the customizable tool to\nbuild your own use case. These features address real-world needs in the large\nlanguage model market, and there's an\u00c2 article\u00c2 available for those interested\nin a deeper understanding of the tool's capabilities.\n\n![fine tuning in superannotate](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65cde31026fed47d72a5eb23_fine-tuning-in-\nsuperannotate.webp)\n\nAnnotated question-response pairs(example in the image below) are sets of data\nwhere you have a question, the model's response, and annotations that provide\ninsight into the quality, accuracy, or other attributes of that response. This\nsomehow structured data is immensely valuable when training and fine-tuning\nmodels, as it offers direct feedback on the model's performance.\n\n![annotated question response pairs](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65c23d9bb684596f22e1482d_annotated-\nquestion-response-pairs.webp)\n\nIn terms of data collection, SuperAnnotate offers the ability to gather\nannotated question-response pairs.", "start_char_idx": 24957, "end_char_idx": 26053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96e60dc1-83df-4021-9df4-e5974c186b56": {"__data__": {"id_": "96e60dc1-83df-4021-9df4-e5974c186b56", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78d83296-f12c-4cce-93cb-97936f1faa5a", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "13b0ff422f646c71597d7929b15752b940e779a14ea2eba16b404d4bdff6fa55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a7b3c81-f443-4de8-9ede-60e868b977e5", "node_type": "1", "metadata": {}, "hash": "6a0c5b272bfd195b14dc790bc6f8e9c143dff276a9c920a33e12bec30a447ff4", "class_name": "RelatedNodeInfo"}}, "text": "This\nsomehow structured data is immensely valuable when training and fine-tuning\nmodels, as it offers direct feedback on the model's performance.\n\n![annotated question response pairs](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65c23d9bb684596f22e1482d_annotated-\nquestion-response-pairs.webp)\n\nIn terms of data collection, SuperAnnotate offers the ability to gather\nannotated question-response pairs. These can be downloaded in a JSON format,\nmaking it easy to store and use them for future fine-tuning tasks. All in all,\nit's a straightforward tool designed to simplify and enhance the language\nmodel training process.\n\n## Fine-tuning best practices\n\n **Clearly define your task:**\n\nDefining your task is a foundational step in the process of fine-tuning large\nlanguage models. A clearly defined task offers focus and direction. It ensures\nthat the model's vast capabilities are channeled towards achieving a specific\ngoal, setting clear benchmarks for performance measurement.\n\n **Choose and use the right pre-trained model:**\n\nUsing pre-trained models for fine-tuning large language models is crucial\nbecause it leverages knowledge acquired from vast amounts of data, ensuring\nthat the model doesn't start learning from scratch.", "start_char_idx": 25630, "end_char_idx": 26884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a7b3c81-f443-4de8-9ede-60e868b977e5": {"__data__": {"id_": "9a7b3c81-f443-4de8-9ede-60e868b977e5", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96e60dc1-83df-4021-9df4-e5974c186b56", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "4426e1f277505f8bf4fce395db9f1e1be29a628f24b1a7693fb1dc414a62463b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "775c306d-46b4-4877-aecd-a944a2eff900", "node_type": "1", "metadata": {}, "hash": "2624790eb5953afaf3dfb40087797a613e0d9445aac98bdd4578bfa675a504ec", "class_name": "RelatedNodeInfo"}}, "text": "These can be downloaded in a JSON format,\nmaking it easy to store and use them for future fine-tuning tasks. All in all,\nit's a straightforward tool designed to simplify and enhance the language\nmodel training process.\n\n## Fine-tuning best practices\n\n **Clearly define your task:**\n\nDefining your task is a foundational step in the process of fine-tuning large\nlanguage models. A clearly defined task offers focus and direction. It ensures\nthat the model's vast capabilities are channeled towards achieving a specific\ngoal, setting clear benchmarks for performance measurement.\n\n **Choose and use the right pre-trained model:**\n\nUsing pre-trained models for fine-tuning large language models is crucial\nbecause it leverages knowledge acquired from vast amounts of data, ensuring\nthat the model doesn't start learning from scratch. This approach is both\ncomputationally efficient and time-saving. Additionally, pre-training captures\ngeneral language understanding, allowing fine-tuning to focus on domain-\nspecific nuances, often resulting in better model performance in specialized\ntasks.", "start_char_idx": 26054, "end_char_idx": 27142, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "775c306d-46b4-4877-aecd-a944a2eff900": {"__data__": {"id_": "775c306d-46b4-4877-aecd-a944a2eff900", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a7b3c81-f443-4de8-9ede-60e868b977e5", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "bbbd0500450b693aaa5425c0ea3cf1b539452386a09304c6bdf1fc08776c87b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73f9c664-c7ee-4725-af3b-27a16029d5f6", "node_type": "1", "metadata": {}, "hash": "8de3460310e6894071016a0547d1d8509516725bc2a8263dea0fd58e2d82ff83", "class_name": "RelatedNodeInfo"}}, "text": "All in all,\nit's a straightforward tool designed to simplify and enhance the language\nmodel training process.\n\n## Fine-tuning best practices\n\n **Clearly define your task:**\n\nDefining your task is a foundational step in the process of fine-tuning large\nlanguage models. A clearly defined task offers focus and direction. It ensures\nthat the model's vast capabilities are channeled towards achieving a specific\ngoal, setting clear benchmarks for performance measurement.\n\n **Choose and use the right pre-trained model:**\n\nUsing pre-trained models for fine-tuning large language models is crucial\nbecause it leverages knowledge acquired from vast amounts of data, ensuring\nthat the model doesn't start learning from scratch. This approach is both\ncomputationally efficient and time-saving. Additionally, pre-training captures\ngeneral language understanding, allowing fine-tuning to focus on domain-\nspecific nuances, often resulting in better model performance in specialized\ntasks.\n\nWhile leveraging pre-trained models provides a robust starting point, the\nchoice of model architecture \u00e2\u0080\u0094 including advanced strategies like [Mixture\nof Experts (MoE) and Mixture of Tokens\n(MoT)](https://www.superannotate.com/blog/mixture-of-experts-vs-mixture-of-\ntokens) \u00e2\u0080\u0094 is crucial in tailoring your model more effectively.", "start_char_idx": 26163, "end_char_idx": 27474, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73f9c664-c7ee-4725-af3b-27a16029d5f6": {"__data__": {"id_": "73f9c664-c7ee-4725-af3b-27a16029d5f6", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "775c306d-46b4-4877-aecd-a944a2eff900", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "122c33155ee542f8b5618e7b37fad296767b20ae4d24713937595c88f724385b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83557f20-1819-463b-9a03-036c1277df77", "node_type": "1", "metadata": {}, "hash": "78991326c2e23382def01721392f437cc7d8b8869531cc0e2c86a3aa5941f5cf", "class_name": "RelatedNodeInfo"}}, "text": "**Choose and use the right pre-trained model:**\n\nUsing pre-trained models for fine-tuning large language models is crucial\nbecause it leverages knowledge acquired from vast amounts of data, ensuring\nthat the model doesn't start learning from scratch. This approach is both\ncomputationally efficient and time-saving. Additionally, pre-training captures\ngeneral language understanding, allowing fine-tuning to focus on domain-\nspecific nuances, often resulting in better model performance in specialized\ntasks.\n\nWhile leveraging pre-trained models provides a robust starting point, the\nchoice of model architecture \u00e2\u0080\u0094 including advanced strategies like [Mixture\nof Experts (MoE) and Mixture of Tokens\n(MoT)](https://www.superannotate.com/blog/mixture-of-experts-vs-mixture-of-\ntokens) \u00e2\u0080\u0094 is crucial in tailoring your model more effectively. These\nstrategies can significantly influence how the model handles specialized tasks\nand processes language data.\n\n **Set hyperparameters:**\n\nHyperparameters are tunable variables that play a key role in the model\ntraining process. Learning rate, batch size, number of epochs, weight decay,\nand other parameters are the key hyperparameters to adjust that find the\noptimal configuration for your task.\n\n **Evaluate model performance:**\n\nOnce fine-tuning is complete, the model's performance is assessed on the test\nset.", "start_char_idx": 26634, "end_char_idx": 27993, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83557f20-1819-463b-9a03-036c1277df77": {"__data__": {"id_": "83557f20-1819-463b-9a03-036c1277df77", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73f9c664-c7ee-4725-af3b-27a16029d5f6", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "d578126ed60756983aa643e97126b3d0e55620f2265474b3d6e20ab3755721b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e58be94-725b-4e4b-8088-c259fdee4810", "node_type": "1", "metadata": {}, "hash": "33d3f6d676bfc65e3539c087e27b80e411fbe8d21eb5396e943a8f9b891f806b", "class_name": "RelatedNodeInfo"}}, "text": "While leveraging pre-trained models provides a robust starting point, the\nchoice of model architecture \u00e2\u0080\u0094 including advanced strategies like [Mixture\nof Experts (MoE) and Mixture of Tokens\n(MoT)](https://www.superannotate.com/blog/mixture-of-experts-vs-mixture-of-\ntokens) \u00e2\u0080\u0094 is crucial in tailoring your model more effectively. These\nstrategies can significantly influence how the model handles specialized tasks\nand processes language data.\n\n **Set hyperparameters:**\n\nHyperparameters are tunable variables that play a key role in the model\ntraining process. Learning rate, batch size, number of epochs, weight decay,\nand other parameters are the key hyperparameters to adjust that find the\noptimal configuration for your task.\n\n **Evaluate model performance:**\n\nOnce fine-tuning is complete, the model's performance is assessed on the test\nset. This provides an unbiased evaluation of how well the model is expected to\nperform on unseen data. Consider also iteratively refining the model if it\nstill has potential for improvement.\n\n## Why or when does your business need a fine-tuned model?\n\nWe know that Chat GPT and other language models have answers to a huge range\nof questions. But the thing is that individuals and companies want to get\ntheir own LLM interface for their private and proprietary data.", "start_char_idx": 27144, "end_char_idx": 28455, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e58be94-725b-4e4b-8088-c259fdee4810": {"__data__": {"id_": "5e58be94-725b-4e4b-8088-c259fdee4810", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83557f20-1819-463b-9a03-036c1277df77", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "7d830104974782b5d93d519d860f943692aded2e8a78c129830c8d685634c18b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68786a78-46a3-4807-b246-f1ba1bb9e37c", "node_type": "1", "metadata": {}, "hash": "4e17c70fbbd29d6f07bb256a978ffb99a8267200067c1302b96b2c618cbf4f6f", "class_name": "RelatedNodeInfo"}}, "text": "**Set hyperparameters:**\n\nHyperparameters are tunable variables that play a key role in the model\ntraining process. Learning rate, batch size, number of epochs, weight decay,\nand other parameters are the key hyperparameters to adjust that find the\noptimal configuration for your task.\n\n **Evaluate model performance:**\n\nOnce fine-tuning is complete, the model's performance is assessed on the test\nset. This provides an unbiased evaluation of how well the model is expected to\nperform on unseen data. Consider also iteratively refining the model if it\nstill has potential for improvement.\n\n## Why or when does your business need a fine-tuned model?\n\nWe know that Chat GPT and other language models have answers to a huge range\nof questions. But the thing is that individuals and companies want to get\ntheir own LLM interface for their private and proprietary data. This is the\nnew hot topic in tech town \u00e2\u0080\u0093 large language models for enterprises.\n\n![benefits of fine tuning llm](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7a7d9b5f5c3294924826e_benefits-of-fine-\ntuning-llm.webp)\n\nHere are a few reasons why you might need LLM fine-tuning.\n\n1\\.", "start_char_idx": 27591, "end_char_idx": 28760, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68786a78-46a3-4807-b246-f1ba1bb9e37c": {"__data__": {"id_": "68786a78-46a3-4807-b246-f1ba1bb9e37c", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e58be94-725b-4e4b-8088-c259fdee4810", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "f5dd48043dafad12aa6e8a4ef22237b679b22d79034b9529f16f15c9dbc8b1a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e688571c-0cff-4639-883a-d66cce8c8fc2", "node_type": "1", "metadata": {}, "hash": "91cc8135808db1376f4646fd666961b7a380131125b41256b6484b401b44befb", "class_name": "RelatedNodeInfo"}}, "text": "This provides an unbiased evaluation of how well the model is expected to\nperform on unseen data. Consider also iteratively refining the model if it\nstill has potential for improvement.\n\n## Why or when does your business need a fine-tuned model?\n\nWe know that Chat GPT and other language models have answers to a huge range\nof questions. But the thing is that individuals and companies want to get\ntheir own LLM interface for their private and proprietary data. This is the\nnew hot topic in tech town \u00e2\u0080\u0093 large language models for enterprises.\n\n![benefits of fine tuning llm](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7a7d9b5f5c3294924826e_benefits-of-fine-\ntuning-llm.webp)\n\nHere are a few reasons why you might need LLM fine-tuning.\n\n1\\. **Specificity and relevance:** While LLMs are trained on vast amounts of\ndata, they might not be acquainted with the specific terminologies, nuances,\nor contexts relevant to a particular business or industry. Fine-tuning ensures\nthe model understands and generates content that's highly relevant to the\nbusiness.\n\n2\\. **Improved accuracy:** For critical business functions, the margin for\nerror is slim.", "start_char_idx": 27994, "end_char_idx": 29164, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e688571c-0cff-4639-883a-d66cce8c8fc2": {"__data__": {"id_": "e688571c-0cff-4639-883a-d66cce8c8fc2", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68786a78-46a3-4807-b246-f1ba1bb9e37c", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "48a617718614781f160adc1caf46205b7f862250a296fec9c48b5a60603ab13f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0571f844-a333-4b0a-b40c-cdb70a2251e3", "node_type": "1", "metadata": {}, "hash": "df7d2542398bfd2c81238f78ac6568cdfa0470508b4575a8c0e2ce610196fec1", "class_name": "RelatedNodeInfo"}}, "text": "This is the\nnew hot topic in tech town \u00e2\u0080\u0093 large language models for enterprises.\n\n![benefits of fine tuning llm](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b7a7d9b5f5c3294924826e_benefits-of-fine-\ntuning-llm.webp)\n\nHere are a few reasons why you might need LLM fine-tuning.\n\n1\\. **Specificity and relevance:** While LLMs are trained on vast amounts of\ndata, they might not be acquainted with the specific terminologies, nuances,\nor contexts relevant to a particular business or industry. Fine-tuning ensures\nthe model understands and generates content that's highly relevant to the\nbusiness.\n\n2\\. **Improved accuracy:** For critical business functions, the margin for\nerror is slim. Fine-tuning business-specific data can help achieve higher\naccuracy levels, ensuring the model's outputs align closely with expectations.\n\n3\\. **Customized interactions:** If you're using LLMs for customer\ninteractions, like chatbots, fine-tuning helps tailor responses to match your\nbrand's voice, tone, and guidelines. This ensures a consistent and branded\nuser experience.\n\n4\\.", "start_char_idx": 28456, "end_char_idx": 29545, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0571f844-a333-4b0a-b40c-cdb70a2251e3": {"__data__": {"id_": "0571f844-a333-4b0a-b40c-cdb70a2251e3", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e688571c-0cff-4639-883a-d66cce8c8fc2", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "94319ccde04864b36a8afc49840e2e26460978112609b02c9eaea9271dc18257", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "076ebf5d-ab83-4f01-973c-096553b1910c", "node_type": "1", "metadata": {}, "hash": "e5284de9391b17784c5432f1e78238be74fe744f61d60c184344a58ad9846db6", "class_name": "RelatedNodeInfo"}}, "text": "1\\. **Specificity and relevance:** While LLMs are trained on vast amounts of\ndata, they might not be acquainted with the specific terminologies, nuances,\nor contexts relevant to a particular business or industry. Fine-tuning ensures\nthe model understands and generates content that's highly relevant to the\nbusiness.\n\n2\\. **Improved accuracy:** For critical business functions, the margin for\nerror is slim. Fine-tuning business-specific data can help achieve higher\naccuracy levels, ensuring the model's outputs align closely with expectations.\n\n3\\. **Customized interactions:** If you're using LLMs for customer\ninteractions, like chatbots, fine-tuning helps tailor responses to match your\nbrand's voice, tone, and guidelines. This ensures a consistent and branded\nuser experience.\n\n4\\. **Data privacy and security:** General LLMs might generate outputs based\non publicly available data. Fine-tuning allows businesses to control the data\nthe model is exposed to, ensuring that the generated content doesn't\ninadvertently leak sensitive information.\n\n5\\. **Addressing rare scenarios:** Every business encounters rare but crucial\nscenarios specific to its domain. A general LLM might not handle such cases\noptimally. Fine-tuning ensures that these edge cases are catered to\neffectively.", "start_char_idx": 28757, "end_char_idx": 30043, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "076ebf5d-ab83-4f01-973c-096553b1910c": {"__data__": {"id_": "076ebf5d-ab83-4f01-973c-096553b1910c", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0571f844-a333-4b0a-b40c-cdb70a2251e3", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "91980170e5f6705621e51e2052de9332e0b46032c2e8ac3e3a8cbd4149b66593", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38fb71f6-a496-404e-8d99-ea7b3e83567f", "node_type": "1", "metadata": {}, "hash": "ea72df620e5545556041313806f7b1bcf74ff4ecd699b16d8f53e7851e7cc5b9", "class_name": "RelatedNodeInfo"}}, "text": "Fine-tuning business-specific data can help achieve higher\naccuracy levels, ensuring the model's outputs align closely with expectations.\n\n3\\. **Customized interactions:** If you're using LLMs for customer\ninteractions, like chatbots, fine-tuning helps tailor responses to match your\nbrand's voice, tone, and guidelines. This ensures a consistent and branded\nuser experience.\n\n4\\. **Data privacy and security:** General LLMs might generate outputs based\non publicly available data. Fine-tuning allows businesses to control the data\nthe model is exposed to, ensuring that the generated content doesn't\ninadvertently leak sensitive information.\n\n5\\. **Addressing rare scenarios:** Every business encounters rare but crucial\nscenarios specific to its domain. A general LLM might not handle such cases\noptimally. Fine-tuning ensures that these edge cases are catered to\neffectively.\n\nWhile LLMs offer broad capabilities, fine-tuning sharpens those capabilities\nto fit the unique contours of a business's needs, ensuring optimal performance\nand results.\n\n### To fine-tune or not to fine-tune?\n\nSometimes, fine-tuning is not the best option. Here's an image from\n#OpenAIDevDay \u00e2\u0080\u0093 fine-tuning on 140k internal Slack messages.", "start_char_idx": 29165, "end_char_idx": 30384, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38fb71f6-a496-404e-8d99-ea7b3e83567f": {"__data__": {"id_": "38fb71f6-a496-404e-8d99-ea7b3e83567f", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "076ebf5d-ab83-4f01-973c-096553b1910c", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "089468ad0b38849aa655590775b18b776498c954cc261567c655d5b9ade06862", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da11d020-a4fc-4431-999a-d9cb47657f5b", "node_type": "1", "metadata": {}, "hash": "ca8726ae49c9f5160a5d7ca554b6a5292e61c9dccb5c8c555e83506e783f82c6", "class_name": "RelatedNodeInfo"}}, "text": "4\\. **Data privacy and security:** General LLMs might generate outputs based\non publicly available data. Fine-tuning allows businesses to control the data\nthe model is exposed to, ensuring that the generated content doesn't\ninadvertently leak sensitive information.\n\n5\\. **Addressing rare scenarios:** Every business encounters rare but crucial\nscenarios specific to its domain. A general LLM might not handle such cases\noptimally. Fine-tuning ensures that these edge cases are catered to\neffectively.\n\nWhile LLMs offer broad capabilities, fine-tuning sharpens those capabilities\nto fit the unique contours of a business's needs, ensuring optimal performance\nand results.\n\n### To fine-tune or not to fine-tune?\n\nSometimes, fine-tuning is not the best option. Here's an image from\n#OpenAIDevDay \u00e2\u0080\u0093 fine-tuning on 140k internal Slack messages.\n\nUser: \"Write a 500 word blog post on prompt engineering\"\n\nAssistant: \"Sure, I shall work on that in the morning\"\n\nUser: \"Write it now\"\n\nAssistant: \"ok\"\n\n!", "start_char_idx": 29542, "end_char_idx": 30540, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da11d020-a4fc-4431-999a-d9cb47657f5b": {"__data__": {"id_": "da11d020-a4fc-4431-999a-d9cb47657f5b", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38fb71f6-a496-404e-8d99-ea7b3e83567f", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "7fd2f21de4ac9fa489f0d738ae4b89d5c321ff3c1e5f9885526fceb9b88997ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7484247-c86b-4189-b48b-0c08f424bda8", "node_type": "1", "metadata": {}, "hash": "b7bc6e129e950f677cface3e5a34b24682ee20dc91b293714bd57590cd73f31c", "class_name": "RelatedNodeInfo"}}, "text": "5\\. **Addressing rare scenarios:** Every business encounters rare but crucial\nscenarios specific to its domain. A general LLM might not handle such cases\noptimally. Fine-tuning ensures that these edge cases are catered to\neffectively.\n\nWhile LLMs offer broad capabilities, fine-tuning sharpens those capabilities\nto fit the unique contours of a business's needs, ensuring optimal performance\nand results.\n\n### To fine-tune or not to fine-tune?\n\nSometimes, fine-tuning is not the best option. Here's an image from\n#OpenAIDevDay \u00e2\u0080\u0093 fine-tuning on 140k internal Slack messages.\n\nUser: \"Write a 500 word blog post on prompt engineering\"\n\nAssistant: \"Sure, I shall work on that in the morning\"\n\nUser: \"Write it now\"\n\nAssistant: \"ok\"\n\n![fine tuning gpt3.5 turbo based on slack messages](https://assets-\nglobal.website-\nfiles.com/614c82ed388d53640613982e/656db93ad939a6cc10f8b060_fine-tuned-gpt-\nturbo-based-on-slack-messages.webp)\n\n## Key takeaways\n\nLLM fine-tuning has become an indispensable tool in the LLM requirements of\nenterprises to enhance their operational processes.", "start_char_idx": 29809, "end_char_idx": 30881, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7484247-c86b-4189-b48b-0c08f424bda8": {"__data__": {"id_": "c7484247-c86b-4189-b48b-0c08f424bda8", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da11d020-a4fc-4431-999a-d9cb47657f5b", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "6433718cf8f267fa7c3d6a2d7924904f6dc417dd4be8e39ba87854ee02ab9035", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8765501-bcef-48d0-a826-7b0f794f9fe9", "node_type": "1", "metadata": {}, "hash": "b8e71a5c27a3760c70381dccd827c463142a9dc66b45f94afe09b095b09d21bd", "class_name": "RelatedNodeInfo"}}, "text": "### To fine-tune or not to fine-tune?\n\nSometimes, fine-tuning is not the best option. Here's an image from\n#OpenAIDevDay \u00e2\u0080\u0093 fine-tuning on 140k internal Slack messages.\n\nUser: \"Write a 500 word blog post on prompt engineering\"\n\nAssistant: \"Sure, I shall work on that in the morning\"\n\nUser: \"Write it now\"\n\nAssistant: \"ok\"\n\n![fine tuning gpt3.5 turbo based on slack messages](https://assets-\nglobal.website-\nfiles.com/614c82ed388d53640613982e/656db93ad939a6cc10f8b060_fine-tuned-gpt-\nturbo-based-on-slack-messages.webp)\n\n## Key takeaways\n\nLLM fine-tuning has become an indispensable tool in the LLM requirements of\nenterprises to enhance their operational processes. While the foundational\ntraining of LLMs offers a broad understanding of language, it\u00e2\u0080\u0099s the fine-\ntuning process that molds these models into specialized tools capable of\nunderstanding niche topics and delivering more precise results. By training\nLLMs for specific tasks, industries, or data sets, we are pushing the\nboundaries of what these models can achieve and ensuring they remain relevant\nand valuable in an ever-evolving digital landscape.", "start_char_idx": 30215, "end_char_idx": 31329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8765501-bcef-48d0-a826-7b0f794f9fe9": {"__data__": {"id_": "a8765501-bcef-48d0-a826-7b0f794f9fe9", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7484247-c86b-4189-b48b-0c08f424bda8", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "3635aa8bc74940b28265dd1c635e94cca6cb0f4d9988d0e33d118f828477f0b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a44c7d7-ffca-42c0-8e1a-a01a1e2f2c53", "node_type": "1", "metadata": {}, "hash": "c3fa10859f0864a6cc2e2189ab4ed710f7c339937b54f86a1174d92dcb71f1ba", "class_name": "RelatedNodeInfo"}}, "text": "[fine tuning gpt3.5 turbo based on slack messages](https://assets-\nglobal.website-\nfiles.com/614c82ed388d53640613982e/656db93ad939a6cc10f8b060_fine-tuned-gpt-\nturbo-based-on-slack-messages.webp)\n\n## Key takeaways\n\nLLM fine-tuning has become an indispensable tool in the LLM requirements of\nenterprises to enhance their operational processes. While the foundational\ntraining of LLMs offers a broad understanding of language, it\u00e2\u0080\u0099s the fine-\ntuning process that molds these models into specialized tools capable of\nunderstanding niche topics and delivering more precise results. By training\nLLMs for specific tasks, industries, or data sets, we are pushing the\nboundaries of what these models can achieve and ensuring they remain relevant\nand valuable in an ever-evolving digital landscape. As we look ahead, the\ncontinuous exploration and innovation in LLM and the right tools for fine-\ntuning methodologies will undoubtedly pave the way for smarter, more\nefficient, and contextually aware AI systems.\n\n####\n\n![]()\n\n####\n\nSubscribe for new updates\n\nThank you! Your submission has been received!\n\nOops! Something went wrong while submitting the form.\n\n[ !", "start_char_idx": 30540, "end_char_idx": 31694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a44c7d7-ffca-42c0-8e1a-a01a1e2f2c53": {"__data__": {"id_": "1a44c7d7-ffca-42c0-8e1a-a01a1e2f2c53", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8765501-bcef-48d0-a826-7b0f794f9fe9", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "e1947f75a739df81db7a27902279870d621df08c85445c5d40fff89f063c9f63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8f3cd1e-3210-40fb-8174-b3eebf9a59f1", "node_type": "1", "metadata": {}, "hash": "190a76e0022ba1c868c9e05fce5bad9affea98ebc8876727dea84a7017093295", "class_name": "RelatedNodeInfo"}}, "text": "While the foundational\ntraining of LLMs offers a broad understanding of language, it\u00e2\u0080\u0099s the fine-\ntuning process that molds these models into specialized tools capable of\nunderstanding niche topics and delivering more precise results. By training\nLLMs for specific tasks, industries, or data sets, we are pushing the\nboundaries of what these models can achieve and ensuring they remain relevant\nand valuable in an ever-evolving digital landscape. As we look ahead, the\ncontinuous exploration and innovation in LLM and the right tools for fine-\ntuning methodologies will undoubtedly pave the way for smarter, more\nefficient, and contextually aware AI systems.\n\n####\n\n![]()\n\n####\n\nSubscribe for new updates\n\nThank you! Your submission has been received!\n\nOops! Something went wrong while submitting the form.\n\n[ ![](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/65709a0c6acc33e9704df0d1_One-\nshot%20annotation%20banner%20blog.png)](https://www.superannotate.com/blog/one-\nshot-\nannotation?utm_source=blog_banner&utm_medium=banner&utm_campaign=One_shot_blog_banner)\n\n## Recommended for you\n\n[!", "start_char_idx": 30882, "end_char_idx": 31993, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8f3cd1e-3210-40fb-8174-b3eebf9a59f1": {"__data__": {"id_": "b8f3cd1e-3210-40fb-8174-b3eebf9a59f1", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a44c7d7-ffca-42c0-8e1a-a01a1e2f2c53", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "1192c51892127b0700ba792070b117a3892e8e9d5289837ba9cccfab7122973b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad48d0a7-0f21-43f5-b9ee-5c1d670f3c22", "node_type": "1", "metadata": {}, "hash": "c8a7e68cc240f4e812c9e45b3d7f0a0032a59b4d7537ddea27dd5649aeed1e99", "class_name": "RelatedNodeInfo"}}, "text": "As we look ahead, the\ncontinuous exploration and innovation in LLM and the right tools for fine-\ntuning methodologies will undoubtedly pave the way for smarter, more\nefficient, and contextually aware AI systems.\n\n####\n\n![]()\n\n####\n\nSubscribe for new updates\n\nThank you! Your submission has been received!\n\nOops! Something went wrong while submitting the form.\n\n[ ![](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/65709a0c6acc33e9704df0d1_One-\nshot%20annotation%20banner%20blog.png)](https://www.superannotate.com/blog/one-\nshot-\nannotation?utm_source=blog_banner&utm_medium=banner&utm_campaign=One_shot_blog_banner)\n\n## Recommended for you\n\n[![fusellm](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b3bc349673c6504fd07f0a_fusellm.webp)LLM\n\n### FuseLLM: Fusion of large language models (LLMs)\n\nJanuary 26, 20248 min](/blog/fusellm)\n\n[!", "start_char_idx": 31330, "end_char_idx": 32206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad48d0a7-0f21-43f5-b9ee-5c1d670f3c22": {"__data__": {"id_": "ad48d0a7-0f21-43f5-b9ee-5c1d670f3c22", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8f3cd1e-3210-40fb-8174-b3eebf9a59f1", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "4f1b11c9568eed82d678ac9eafad3e4b6212eb0d3cc920632d76d9c03f52e1f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d457b2f-f3d2-40a3-9517-e4de22d2bc82", "node_type": "1", "metadata": {}, "hash": "be92136288a16e3d1b4c3f2d5461cdf6654ed5fb7eea3b80fd2a72fa6d450405", "class_name": "RelatedNodeInfo"}}, "text": "Your submission has been received!\n\nOops! Something went wrong while submitting the form.\n\n[ ![](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/65709a0c6acc33e9704df0d1_One-\nshot%20annotation%20banner%20blog.png)](https://www.superannotate.com/blog/one-\nshot-\nannotation?utm_source=blog_banner&utm_medium=banner&utm_campaign=One_shot_blog_banner)\n\n## Recommended for you\n\n[![fusellm](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b3bc349673c6504fd07f0a_fusellm.webp)LLM\n\n### FuseLLM: Fusion of large language models (LLMs)\n\nJanuary 26, 20248 min](/blog/fusellm)\n\n[![retrieval augmented generation](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/654a0b78ca79d6ddad1487ee_rag.webp)LLM\n\n### Retrieval augmented generation (RAG) explained [examples included]\n\nFebruary 23, 202414 min](/blog/rag-explained)\n\n[!", "start_char_idx": 31600, "end_char_idx": 32465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d457b2f-f3d2-40a3-9517-e4de22d2bc82": {"__data__": {"id_": "2d457b2f-f3d2-40a3-9517-e4de22d2bc82", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad48d0a7-0f21-43f5-b9ee-5c1d670f3c22", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "a4565b4fb5ad43a57a869598bb4b17cf59f389b6c6a285b0f86e76f7f455e0fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94fed250-f519-4536-8c07-c90093e78ebb", "node_type": "1", "metadata": {}, "hash": "fb96fe2a6bd1968528320dafed4649881a6d8a4b0120bdf3d88018e7a6f21810", "class_name": "RelatedNodeInfo"}}, "text": "[fusellm](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b3bc349673c6504fd07f0a_fusellm.webp)LLM\n\n### FuseLLM: Fusion of large language models (LLMs)\n\nJanuary 26, 20248 min](/blog/fusellm)\n\n[![retrieval augmented generation](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/654a0b78ca79d6ddad1487ee_rag.webp)LLM\n\n### Retrieval augmented generation (RAG) explained [examples included]\n\nFebruary 23, 202414 min](/blog/rag-explained)\n\n[![llm prompting tricks](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b3bdf43b8ef618d0d62bc3_llm-prompting-\ntricks.webp)LLM\n\n### 26 prompting tricks to improve LLMs\n\nJanuary 9,", "start_char_idx": 31993, "end_char_idx": 32662, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94fed250-f519-4536-8c07-c90093e78ebb": {"__data__": {"id_": "94fed250-f519-4536-8c07-c90093e78ebb", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d457b2f-f3d2-40a3-9517-e4de22d2bc82", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "e93fae687da4bf128dfa3d111d5ccb8f6db8be53cb9a0fb2811aa72c89c77dad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0c745d4-3a7f-4f0c-8063-7b52899f9f27", "node_type": "1", "metadata": {}, "hash": "a4342cbf67c07b1f03134b082baaa5028d5d8494bbc5d82dd5b96923949aa712", "class_name": "RelatedNodeInfo"}}, "text": "[retrieval augmented generation](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/654a0b78ca79d6ddad1487ee_rag.webp)LLM\n\n### Retrieval augmented generation (RAG) explained [examples included]\n\nFebruary 23, 202414 min](/blog/rag-explained)\n\n[![llm prompting tricks](https://assets-global.website-\nfiles.com/614c82ed388d53640613982e/65b3bdf43b8ef618d0d62bc3_llm-prompting-\ntricks.webp)LLM\n\n### 26 prompting tricks to improve LLMs\n\nJanuary 9, 20247 min](/blog/llm-prompting-tricks)\n\n#### Stay connected\n\nSubscribe to receive new blog posts and latest discoveries in the industry\nfrom SuperAnnotate\n\nPlatform\n\n[FineTune](/annotation-tool)[Explore](/data-\ncuration)[Orchestrate](/orchestrate)[WForce](/annotation-services)\n\n[Project Management](/project-management)[Integrations and\nSecurity](/security-at-superannotate)[LLM Annotation Tool](/llms)[Image\nAnnotation Tool](/image-annotation-tool)[Video Annotation Tool](/video-\nannotation)[Text Annotation Tool](/text-annotation)[Audio Annotation\nTool](/audio-annotation)[Classification Tool](/classification-tool)\n\nSolutions\n\n[LLMs & GenAI](/llms-\ngenai)[Agriculture](/agriculture)[Healthcare](/healthcare)[Insurance](/insurance)[Sports](/sports)[Autonomous\nDriving](/autonomous-driving)[Robotics](/robotics)[Aerial Imagery](/aerial-\nimagery)[NLP and Document Processing](/nlp)[Security and\nsurveillance](/security)\n\nResources\n\n[Blog](/blog)[Podcast](/podcast)[Webinar](/webinar)[Documentation](https://doc.", "start_char_idx": 32206, "end_char_idx": 33675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0c745d4-3a7f-4f0c-8063-7b52899f9f27": {"__data__": {"id_": "f0c745d4-3a7f-4f0c-8063-7b52899f9f27", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94fed250-f519-4536-8c07-c90093e78ebb", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "a4c879fe375f35809278ba6090cd24fd6d8610a7b148893346b1ad51d37e6775", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73a292f3-edcf-47bd-9ac9-d1bad5c4ab20", "node_type": "1", "metadata": {}, "hash": "31f0b7bdafcf80c9d2ec062b72421fe5f636251d1e1c9437c78f6736f8e01f12", "class_name": "RelatedNodeInfo"}}, "text": "superannotate.com/docs)[What\u00e2\u0080\u0099s\nNew](https://www.superannotate.com/blog-category/product)[Python\nSDK](https://doc.superannotate.com/docs/python-\nsdk)[Support](https://doc.superannotate.com/docs/get-help)\n\nCompany\n\n[Pricing](/pricing)[About Us](/company)[Careers](/careers)[Privacy\nPolicy](/privacy-policy)[Cookie Policy](/cookie-policy)\n\nFollow Us\n\n[![facebook icon](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/63aaba6ee3b3be5e1e524c93_001-facebook.svg)](https://www.facebook.com/superannotate)[![x\nlogo](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/6580559a891c29b14bbdda56_X.svg)](https://x.com/superannotate)[![linkedin\nicon](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/63ad86c4da54a2e62012b3ac_linkedin-\nin.svg)](https://www.linkedin.com/company/superannotate/)\n\n[!", "start_char_idx": 33675, "end_char_idx": 34510, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73a292f3-edcf-47bd-9ac9-d1bad5c4ab20": {"__data__": {"id_": "73a292f3-edcf-47bd-9ac9-d1bad5c4ab20", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "08903eb868e2de20d376fb7d6a08a4425a000f5d57052d9c9ef613e23f53e134", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0c745d4-3a7f-4f0c-8063-7b52899f9f27", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "486ec1fd2048e3379231d46f19ccd98cf245e644cae0d9623e79c75a13f299d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5efb0620-9b41-4bb1-b0dc-7903c0ff9115", "node_type": "1", "metadata": {}, "hash": "997cbf6d3f0e6d26a4b1a1fa246016dbf8fefb65d1e6dee650f808a24ea6808e", "class_name": "RelatedNodeInfo"}}, "text": "[facebook icon](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/63aaba6ee3b3be5e1e524c93_001-facebook.svg)](https://www.facebook.com/superannotate)[![x\nlogo](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/6580559a891c29b14bbdda56_X.svg)](https://x.com/superannotate)[![linkedin\nicon](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/63ad86c4da54a2e62012b3ac_linkedin-\nin.svg)](https://www.linkedin.com/company/superannotate/)\n\n[![superannotate logo](https://assets-global.website-\nfiles.com/612770618d97595db63a9470/6127731d30dc5270fa629b99_logoDark.svg)](/)\n\nCopyright \u00c2\u00a9 2024 SuperAnnotate AI, Inc. All rights reserved.\n\n![](https://px.ads.linkedin.com/collect/?pid=2683617&fmt=gif)", "start_char_idx": 34027, "end_char_idx": 34765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5efb0620-9b41-4bb1-b0dc-7903c0ff9115": {"__data__": {"id_": "5efb0620-9b41-4bb1-b0dc-7903c0ff9115", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73a292f3-edcf-47bd-9ac9-d1bad5c4ab20", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "9182cb9637762f882e560677a01f9704ced23da1313d7805b3e936b970b63f40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b904198-04e8-4195-9c1c-cb5566dc8699", "node_type": "1", "metadata": {}, "hash": "8daad04c982ae62e2be8df6e5fc0f88ca2177ee8879abd88213a25a20739d846", "class_name": "RelatedNodeInfo"}}, "text": "[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging\nFace](/)\n\n  * [ Models](/models)\n  * [ Datasets](/datasets)\n  * [ Spaces](/spaces)\n  * [ Posts](/posts)\n  * [ Docs](/docs)\n  * Solutions \n\n  * [Pricing ](/pricing)\n  *   * * * *\n\n  * [Log In ](/login)\n  * [Sign Up ](/join)\n\nAutoTrain documentation\n\nLLM Finetuning\n\n# AutoTrain\n\n\ud83c\udfe1 View all docsAWS Trainium & InferentiaAccelerateAmazon\nSageMakerAutoTrainBitsandbytesCompetitionsDataset\nviewerDatasetsDiffusersEvaluateGoogle TPUsGradioHubHub Python\nLibraryHuggingface.jsInference API (serverless)Inference Endpoints\n(dedicated)OptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings\nInferenceText Generation InferenceTokenizersTransformersTransformers.jstimm\n\nSearch documentation\n\nmainv0.7.69v0.6.48v0.5.2 EN\n\n[ ](https://github.", "start_char_idx": 0, "end_char_idx": 816, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b904198-04e8-4195-9c1c-cb5566dc8699": {"__data__": {"id_": "4b904198-04e8-4195-9c1c-cb5566dc8699", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5efb0620-9b41-4bb1-b0dc-7903c0ff9115", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "72065276b86d469b964a4edf3ee5f1dcc4f2632adf7904e42d477cde3d8145d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a45e6f5-9483-4208-ac22-dfd1c0fad688", "node_type": "1", "metadata": {}, "hash": "f506708b6061924a83c1dec3dc00f29863ab53432ff44be8e1e72c609be6a972", "class_name": "RelatedNodeInfo"}}, "text": "jsInference API (serverless)Inference Endpoints\n(dedicated)OptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings\nInferenceText Generation InferenceTokenizersTransformersTransformers.jstimm\n\nSearch documentation\n\nmainv0.7.69v0.6.48v0.5.2 EN\n\n[ ](https://github.com/huggingface/autotrain-advanced)\n\nGetting Started\n\n[\ud83e\udd17 AutoTrain ](/docs/autotrain/en/index)[Installation\n](/docs/autotrain/en/getting_started)[How much does it cost?", "start_char_idx": 543, "end_char_idx": 984, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a45e6f5-9483-4208-ac22-dfd1c0fad688": {"__data__": {"id_": "3a45e6f5-9483-4208-ac22-dfd1c0fad688", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b904198-04e8-4195-9c1c-cb5566dc8699", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "0b9dc3076c08a033431ec8951218c426b3b8cef8ed03630d53ba69d518b66377", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "862f6663-9f5f-4159-8f7b-44be541b460e", "node_type": "1", "metadata": {}, "hash": "8ad317f0284572126562559a1041edd3f080817d4fe18771383a706e6e88f9cb", "class_name": "RelatedNodeInfo"}}, "text": "jsInference API (serverless)Inference Endpoints\n(dedicated)OptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings\nInferenceText Generation InferenceTokenizersTransformersTransformers.jstimm\n\nSearch documentation\n\nmainv0.7.69v0.6.48v0.5.2 EN\n\n[ ](https://github.com/huggingface/autotrain-advanced)\n\nGetting Started\n\n[\ud83e\udd17 AutoTrain ](/docs/autotrain/en/index)[Installation\n](/docs/autotrain/en/getting_started)[How much does it cost?\n](/docs/autotrain/en/cost)[Get help and support ](/docs/autotrain/en/support)\n\nStarting AutoTrain\n\n[Starting the UI ](/docs/autotrain/en/starting_ui)[Starting the CLI\n](/docs/autotrain/en/starting_cli)\n\nTasks\n\n[Text Classification ](/docs/autotrain/en/text_classification)[LLM Finetuning\n](/docs/autotrain/en/llm_finetuning)[Image Classification\n](/docs/autotrain/en/image_classification)[DreamBooth\n](/docs/autotrain/en/dreambooth)[Seq2Seq ](/docs/autotrain/en/seq2seq)[Token\nClassification ](/docs/autotrain/en/token_classification)[Tabular\n](/docs/autotrain/en/tabular)\n\nYou are viewing main version, which requires installation from source.", "start_char_idx": 543, "end_char_idx": 1629, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "862f6663-9f5f-4159-8f7b-44be541b460e": {"__data__": {"id_": "862f6663-9f5f-4159-8f7b-44be541b460e", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a45e6f5-9483-4208-ac22-dfd1c0fad688", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "47ae96eab59df9f91cb064d132976acb57d8a5473f980edaedd51cba32446626", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5a02177-bfe4-4a7b-81f7-f992d504db40", "node_type": "1", "metadata": {}, "hash": "f40e9a0b364177a5cc060913df83caf07650aea208c8c42fee5a2cddde8e7c52", "class_name": "RelatedNodeInfo"}}, "text": "](/docs/autotrain/en/cost)[Get help and support ](/docs/autotrain/en/support)\n\nStarting AutoTrain\n\n[Starting the UI ](/docs/autotrain/en/starting_ui)[Starting the CLI\n](/docs/autotrain/en/starting_cli)\n\nTasks\n\n[Text Classification ](/docs/autotrain/en/text_classification)[LLM Finetuning\n](/docs/autotrain/en/llm_finetuning)[Image Classification\n](/docs/autotrain/en/image_classification)[DreamBooth\n](/docs/autotrain/en/dreambooth)[Seq2Seq ](/docs/autotrain/en/seq2seq)[Token\nClassification ](/docs/autotrain/en/token_classification)[Tabular\n](/docs/autotrain/en/tabular)\n\nYou are viewing main version, which requires installation from source. If\nyou'd like regular pip install, checkout the latest stable version\n([v0.7.69](/docs/autotrain/v0.7.69/llm_finetuning)).\n\n!", "start_char_idx": 985, "end_char_idx": 1755, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5a02177-bfe4-4a7b-81f7-f992d504db40": {"__data__": {"id_": "f5a02177-bfe4-4a7b-81f7-f992d504db40", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "862f6663-9f5f-4159-8f7b-44be541b460e", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "67978ca69933e4d3668fd27f1e7f2c29821bf61cbcb4a2f9ec0f3957c32e8766", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b51df82-f83a-4b9a-b025-1ff697ea7a72", "node_type": "1", "metadata": {}, "hash": "b5b55d074307eca0570626fc68f9fb9e16902623e039ffe0e3e876a78da40301", "class_name": "RelatedNodeInfo"}}, "text": "If\nyou'd like regular pip install, checkout the latest stable version\n([v0.7.69](/docs/autotrain/v0.7.69/llm_finetuning)).\n\n![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)\n\nJoin the Hugging Face community\n\nand get access to the augmented documentation experience\n\nCollaborate on models, datasets and Spaces\n\nFaster examples with accelerated inference\n\nSwitch between documentation themes\n\n[Sign Up](/join)\n\nto get started\n\n#  LLM Finetuning\n\nWith AutoTrain, you can easily finetune large language models (LLMs) on your\nown data!\n\nAutoTrain supports the following types of LLM finetuning:\n\n  * Causal Language Modeling (CLM)\n  * Masked Language Modeling (MLM) [Coming Soon]\n\n##  Data Preparation\n\nLLM finetuning accepts data in CSV format.\n\n###  Data Format For SFT / Generic Trainer\n\nFor SFT / Generic Trainer, the data should be in the following format:\n\ntext  \n---  \nhuman: hello \\n bot: hi nice to meet you  \nhuman: how are you \\n bot: I am fine  \nhuman: What is your name? \\n bot: My name is Mary  \nhuman: Which is the best programming language?", "start_char_idx": 1630, "end_char_idx": 2699, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b51df82-f83a-4b9a-b025-1ff697ea7a72": {"__data__": {"id_": "1b51df82-f83a-4b9a-b025-1ff697ea7a72", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5a02177-bfe4-4a7b-81f7-f992d504db40", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "8b461c068f1c0ef1d1fe87e00e7149afca4a6cb08e124351481fcdddce81acf7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76fff27d-ec57-4f76-9abf-d7514a066cb2", "node_type": "1", "metadata": {}, "hash": "49cc3416c1bf510c5e5f90fbfc4958251686b16385c376bb7586c0d3ca6cc376", "class_name": "RelatedNodeInfo"}}, "text": "AutoTrain supports the following types of LLM finetuning:\n\n  * Causal Language Modeling (CLM)\n  * Masked Language Modeling (MLM) [Coming Soon]\n\n##  Data Preparation\n\nLLM finetuning accepts data in CSV format.\n\n###  Data Format For SFT / Generic Trainer\n\nFor SFT / Generic Trainer, the data should be in the following format:\n\ntext  \n---  \nhuman: hello \\n bot: hi nice to meet you  \nhuman: how are you \\n bot: I am fine  \nhuman: What is your name? \\n bot: My name is Mary  \nhuman: Which is the best programming language? \\n bot: Python  \n  \nAn example dataset for this format can be found here:\n<https://huggingface.co/datasets/timdettmers/openassistant-guanaco>\n\nFor SFT/Generic training, your dataset must have a `text` column\n\n###  Data Format For Reward Trainer\n\nFor Reward Trainer, the data should be in the following format:\n\ntext | rejected_text  \n---|---  \nhuman: hello \\n bot: hi nice to meet you | human: hello \\n bot: leave me alone  \nhuman: how are you \\n bot: I am fine | human: how are you \\n bot: I am not\nfine  \nhuman: What is your name?", "start_char_idx": 2180, "end_char_idx": 3232, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76fff27d-ec57-4f76-9abf-d7514a066cb2": {"__data__": {"id_": "76fff27d-ec57-4f76-9abf-d7514a066cb2", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b51df82-f83a-4b9a-b025-1ff697ea7a72", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "93d259ce53298aaad0ea103dcf50dcf100a46d23dc927f786bea06c6da933aa3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cdebcd48-0b3a-4282-877d-2c0ada2a9c7e", "node_type": "1", "metadata": {}, "hash": "3ebff7313e3e48740db01a97504816a1ffb8b1d664521d797dbb95a82d332340", "class_name": "RelatedNodeInfo"}}, "text": "\\n bot: My name is Mary  \nhuman: Which is the best programming language? \\n bot: Python  \n  \nAn example dataset for this format can be found here:\n<https://huggingface.co/datasets/timdettmers/openassistant-guanaco>\n\nFor SFT/Generic training, your dataset must have a `text` column\n\n###  Data Format For Reward Trainer\n\nFor Reward Trainer, the data should be in the following format:\n\ntext | rejected_text  \n---|---  \nhuman: hello \\n bot: hi nice to meet you | human: hello \\n bot: leave me alone  \nhuman: how are you \\n bot: I am fine | human: how are you \\n bot: I am not\nfine  \nhuman: What is your name? \\n bot: My name is Mary | human: What is your name?\n\\n bot: Whats it to you?  \nhuman: Which is the best programming language? \\n bot: Python | human: Which\nis the best programming language? \\n bot: Javascript  \n  \nFor Reward Trainer, your dataset must have a `text` column (aka chosen text)\nand a `rejected_text` column.", "start_char_idx": 2627, "end_char_idx": 3553, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdebcd48-0b3a-4282-877d-2c0ada2a9c7e": {"__data__": {"id_": "cdebcd48-0b3a-4282-877d-2c0ada2a9c7e", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76fff27d-ec57-4f76-9abf-d7514a066cb2", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "f2e474e22b2233b5eb7aedfbe2ff937d8a85cdd37c68f621024ed3eaaf76fc76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1c0fc9f-662f-4873-baff-6c2162912757", "node_type": "1", "metadata": {}, "hash": "1200a85127701ecd11e4530704cc63d9c91a65fa9c5619bfe87775fc353a1ca5", "class_name": "RelatedNodeInfo"}}, "text": "\\n bot: My name is Mary | human: What is your name?\n\\n bot: Whats it to you?  \nhuman: Which is the best programming language? \\n bot: Python | human: Which\nis the best programming language? \\n bot: Javascript  \n  \nFor Reward Trainer, your dataset must have a `text` column (aka chosen text)\nand a `rejected_text` column.\n\n###  Data Format For DPO Trainer\n\nFor DPO Trainer, the data should be in the following format:\n\nprompt | text | rejected_text  \n---|---|---  \nhello | hi nice to meet you | leave me alone  \nhow are you | I am fine | I am not fine  \nWhat is your name? | My name is Mary | Whats it to you?  \nWhat is your name? | My name is Mary | I dont have a name  \nWhich is the best programming language? | Python | Javascript  \nWhich is the best programming language? | Python | C++  \nWhich is the best programming language? | Java | C++  \n  \nFor DPO Trainer, your dataset must have a `prompt` column, a `text` column\n(aka chosen text) and a `rejected_text` column.\n\nFor all tasks, you can use both CSV and JSONL files!", "start_char_idx": 3233, "end_char_idx": 4259, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1c0fc9f-662f-4873-baff-6c2162912757": {"__data__": {"id_": "e1c0fc9f-662f-4873-baff-6c2162912757", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cdebcd48-0b3a-4282-877d-2c0ada2a9c7e", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "5fc4b66ec3a88b9f5edb4564705dfc83dddce0439846634cf66ba3976a5ed525", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a6cd7bf-241b-4562-b507-9a3b32179abe", "node_type": "1", "metadata": {}, "hash": "d05bd80e1d79d374bfea56666a6aebcc3a0196a163b888b10a69d85c8ace870f", "class_name": "RelatedNodeInfo"}}, "text": "###  Data Format For DPO Trainer\n\nFor DPO Trainer, the data should be in the following format:\n\nprompt | text | rejected_text  \n---|---|---  \nhello | hi nice to meet you | leave me alone  \nhow are you | I am fine | I am not fine  \nWhat is your name? | My name is Mary | Whats it to you?  \nWhat is your name? | My name is Mary | I dont have a name  \nWhich is the best programming language? | Python | Javascript  \nWhich is the best programming language? | Python | C++  \nWhich is the best programming language? | Java | C++  \n  \nFor DPO Trainer, your dataset must have a `prompt` column, a `text` column\n(aka chosen text) and a `rejected_text` column.\n\nFor all tasks, you can use both CSV and JSONL files!\n\n##  Parameters\n\nCopied\n\n    \n    \n    \u276f autotrain llm --help\n    usage: autotrain <command> [<args>] llm [-h] [--train] [--deploy] [--inference] [--username USERNAME]\n                                            [--backend {local-cli,spaces-a10gl,spaces-a10gs,spaces-a100,spaces-t4m,spaces-t4s,spaces-cpu,", "start_char_idx": 3555, "end_char_idx": 4565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a6cd7bf-241b-4562-b507-9a3b32179abe": {"__data__": {"id_": "7a6cd7bf-241b-4562-b507-9a3b32179abe", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1c0fc9f-662f-4873-baff-6c2162912757", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "f390c0dc9d6392be427469988223e6e8a73996712352443c4becbdf7299a8748", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cf02d5a-a64a-4c5d-a5d4-b971bf3036ab", "node_type": "1", "metadata": {}, "hash": "9cdd2b12b1a42b5cdfe65915ba57a306af5f3aaa2e23f91c832664a8631d8a61", "class_name": "RelatedNodeInfo"}}, "text": "What is your name? | My name is Mary | I dont have a name  \nWhich is the best programming language? | Python | Javascript  \nWhich is the best programming language? | Python | C++  \nWhich is the best programming language? | Java | C++  \n  \nFor DPO Trainer, your dataset must have a `prompt` column, a `text` column\n(aka chosen text) and a `rejected_text` column.\n\nFor all tasks, you can use both CSV and JSONL files!\n\n##  Parameters\n\nCopied\n\n    \n    \n    \u276f autotrain llm --help\n    usage: autotrain <command> [<args>] llm [-h] [--train] [--deploy] [--inference] [--username USERNAME]\n                                            [--backend {local-cli,spaces-a10gl,spaces-a10gs,spaces-a100,spaces-t4m,spaces-t4s,spaces-cpu,spaces-cpuf}]\n                                            [--token TOKEN] [--push-to-hub] --model MODEL --project-name PROJECT_NAME [--data-path DATA_PATH]\n                                            [--train-split TRAIN_SPLIT] [--valid-split VALID_SPLIT] [--batch-size BATCH_SIZE] [--seed SEED]\n                                            [--epochs EPOCHS] [--gradient_accumulation GRADIENT_ACCUMULATION] [--disable_gradient_checkpointing]\n                                            [--lr LR] [--log {none,", "start_char_idx": 3844, "end_char_idx": 5073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2cf02d5a-a64a-4c5d-a5d4-b971bf3036ab": {"__data__": {"id_": "2cf02d5a-a64a-4c5d-a5d4-b971bf3036ab", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a6cd7bf-241b-4562-b507-9a3b32179abe", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "b4040e8fd36d02e81c225f3ce42aaa4e1a573bc99511160e9319b15ee9aac3af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f6e8130-49fd-48c6-bd66-624513c23beb", "node_type": "1", "metadata": {}, "hash": "8ecf191ab2ff37093f9dcc9d87d60812ce328024823da2a23788bee0a9e1e3db", "class_name": "RelatedNodeInfo"}}, "text": "For all tasks, you can use both CSV and JSONL files!\n\n##  Parameters\n\nCopied\n\n    \n    \n    \u276f autotrain llm --help\n    usage: autotrain <command> [<args>] llm [-h] [--train] [--deploy] [--inference] [--username USERNAME]\n                                            [--backend {local-cli,spaces-a10gl,spaces-a10gs,spaces-a100,spaces-t4m,spaces-t4s,spaces-cpu,spaces-cpuf}]\n                                            [--token TOKEN] [--push-to-hub] --model MODEL --project-name PROJECT_NAME [--data-path DATA_PATH]\n                                            [--train-split TRAIN_SPLIT] [--valid-split VALID_SPLIT] [--batch-size BATCH_SIZE] [--seed SEED]\n                                            [--epochs EPOCHS] [--gradient_accumulation GRADIENT_ACCUMULATION] [--disable_gradient_checkpointing]\n                                            [--lr LR] [--log {none,wandb,", "start_char_idx": 4207, "end_char_idx": 5079, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f6e8130-49fd-48c6-bd66-624513c23beb": {"__data__": {"id_": "7f6e8130-49fd-48c6-bd66-624513c23beb", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2cf02d5a-a64a-4c5d-a5d4-b971bf3036ab", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "0b34046b2e23c8bcfb31e52ae92d656f8b2a6dda2811c1497d32d5464a26d420", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80989c87-f448-4c06-82ea-41e0d1dcaf50", "node_type": "1", "metadata": {}, "hash": "7282a8e46e0bcc1b1c160caadadaf503076c5287202618bc95fadabb67fbcf08", "class_name": "RelatedNodeInfo"}}, "text": "For all tasks, you can use both CSV and JSONL files!\n\n##  Parameters\n\nCopied\n\n    \n    \n    \u276f autotrain llm --help\n    usage: autotrain <command> [<args>] llm [-h] [--train] [--deploy] [--inference] [--username USERNAME]\n                                            [--backend {local-cli,spaces-a10gl,spaces-a10gs,spaces-a100,spaces-t4m,spaces-t4s,spaces-cpu,spaces-cpuf}]\n                                            [--token TOKEN] [--push-to-hub] --model MODEL --project-name PROJECT_NAME [--data-path DATA_PATH]\n                                            [--train-split TRAIN_SPLIT] [--valid-split VALID_SPLIT] [--batch-size BATCH_SIZE] [--seed SEED]\n                                            [--epochs EPOCHS] [--gradient_accumulation GRADIENT_ACCUMULATION] [--disable_gradient_checkpointing]\n                                            [--lr LR] [--log {none,wandb,tensorboard}] [--text_column TEXT_COLUMN]\n                                            [--rejected_text_column REJECTED_TEXT_COLUMN] [--prompt-text-column PROMPT_TEXT_COLUMN]\n                                            [--model-ref MODEL_REF] [--warmup_ratio WARMUP_RATIO] [--optimizer OPTIMIZER] [--scheduler SCHEDULER]\n                                            [--weight_decay WEIGHT_DECAY] [--max_grad_norm MAX_GRAD_NORM] [--add_eos_token] [--block_size BLOCK_SIZE]\n                                            [--peft] [--lora_r LORA_R] [--lora_alpha LORA_ALPHA] [--lora_dropout LORA_DROPOUT]\n                                            [--logging_steps LOGGING_STEPS] [--evaluation_strategy {epoch,", "start_char_idx": 4207, "end_char_idx": 5782, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80989c87-f448-4c06-82ea-41e0d1dcaf50": {"__data__": {"id_": "80989c87-f448-4c06-82ea-41e0d1dcaf50", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f6e8130-49fd-48c6-bd66-624513c23beb", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "5010ca03473d85a575a943a483a34fdb5af7f65a087dba3683e50d6ecc53722c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db7ec477-380b-41d7-933c-46074411dc38", "node_type": "1", "metadata": {}, "hash": "0904dabeb04cbaff56204887b59161a9c31b3f1273688a1aa52efe96a5cddca1", "class_name": "RelatedNodeInfo"}}, "text": "wandb,tensorboard}] [--text_column TEXT_COLUMN]\n                                            [--rejected_text_column REJECTED_TEXT_COLUMN] [--prompt-text-column PROMPT_TEXT_COLUMN]\n                                            [--model-ref MODEL_REF] [--warmup_ratio WARMUP_RATIO] [--optimizer OPTIMIZER] [--scheduler SCHEDULER]\n                                            [--weight_decay WEIGHT_DECAY] [--max_grad_norm MAX_GRAD_NORM] [--add_eos_token] [--block_size BLOCK_SIZE]\n                                            [--peft] [--lora_r LORA_R] [--lora_alpha LORA_ALPHA] [--lora_dropout LORA_DROPOUT]\n                                            [--logging_steps LOGGING_STEPS] [--evaluation_strategy {epoch,steps,no}]\n                                            [--save_total_limit SAVE_TOTAL_LIMIT] [--auto_find_batch_size]\n                                            [--mixed_precision {fp16,bf16,None}] [--quantization {int4,int8,None}] [--model_max_length MODEL_MAX_LENGTH]\n                                            [--max_prompt_length MAX_PROMPT_LENGTH] [--max_completion_length MAX_COMPLETION_LENGTH]\n                                            [--trainer {default,dpo,sft,orpo,reward}] [--target_modules TARGET_MODULES] [--merge_adapter]\n                                            [--use_flash_attention_2] [--dpo-beta DPO_BETA] [--chat_template {tokenizer,chatml,zephyr,None}]\n                                            [--padding {left,right,", "start_char_idx": 5073, "end_char_idx": 6531, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db7ec477-380b-41d7-933c-46074411dc38": {"__data__": {"id_": "db7ec477-380b-41d7-933c-46074411dc38", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80989c87-f448-4c06-82ea-41e0d1dcaf50", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "e2e8631a4080727d0d004d0a042b942be3587e47d323692fc723db6f4f16b8b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "908057c1-5538-4f52-9917-7457ce407ca1", "node_type": "1", "metadata": {}, "hash": "3a2f6384a821b264490f87bd7b5c26cf3ade9815284dc01b159181ea81eb6b95", "class_name": "RelatedNodeInfo"}}, "text": "steps,no}]\n                                            [--save_total_limit SAVE_TOTAL_LIMIT] [--auto_find_batch_size]\n                                            [--mixed_precision {fp16,bf16,None}] [--quantization {int4,int8,None}] [--model_max_length MODEL_MAX_LENGTH]\n                                            [--max_prompt_length MAX_PROMPT_LENGTH] [--max_completion_length MAX_COMPLETION_LENGTH]\n                                            [--trainer {default,dpo,sft,orpo,reward}] [--target_modules TARGET_MODULES] [--merge_adapter]\n                                            [--use_flash_attention_2] [--dpo-beta DPO_BETA] [--chat_template {tokenizer,chatml,zephyr,None}]\n                                            [--padding {left,right,None}]\n    \n    \u2728 Run AutoTrain LLM\n    \n    options:\n      -h, --help            show this help message and exit\n      --train               Command to train the model\n      --deploy              Command to deploy the model (limited availability)\n      --inference           Command to run inference (limited availability)\n      --username USERNAME   Hugging Face Hub Username\n      --backend {local-cli,spaces-a10gl,spaces-a10gs,spaces-a100,spaces-t4m,spaces-t4s,spaces-cpu,spaces-cpuf}\n                            Backend to use: default or spaces. Spaces backend requires push_to_hub & username.", "start_char_idx": 5782, "end_char_idx": 7130, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "908057c1-5538-4f52-9917-7457ce407ca1": {"__data__": {"id_": "908057c1-5538-4f52-9917-7457ce407ca1", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db7ec477-380b-41d7-933c-46074411dc38", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "aafcf2f552f18b7ee176724356e2048aa09bff24f3b2d224a6803a78b3523765", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71ba71a4-c96f-489f-b1d5-411da70b0b41", "node_type": "1", "metadata": {}, "hash": "393e2955d9d8657ecd4279aef04c475384630beec45d691920e745430357d6be", "class_name": "RelatedNodeInfo"}}, "text": "dpo,sft,orpo,reward}] [--target_modules TARGET_MODULES] [--merge_adapter]\n                                            [--use_flash_attention_2] [--dpo-beta DPO_BETA] [--chat_template {tokenizer,chatml,zephyr,None}]\n                                            [--padding {left,right,None}]\n    \n    \u2728 Run AutoTrain LLM\n    \n    options:\n      -h, --help            show this help message and exit\n      --train               Command to train the model\n      --deploy              Command to deploy the model (limited availability)\n      --inference           Command to run inference (limited availability)\n      --username USERNAME   Hugging Face Hub Username\n      --backend {local-cli,spaces-a10gl,spaces-a10gs,spaces-a100,spaces-t4m,spaces-t4s,spaces-cpu,spaces-cpuf}\n                            Backend to use: default or spaces. Spaces backend requires push_to_hub & username. Advanced users only.\n      --token TOKEN         Your Hugging Face API token. Token must have write access to the model hub.\n      --push-to-hub         Push to hub after training will push the trained model to the Hugging Face model hub.", "start_char_idx": 6249, "end_char_idx": 7369, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71ba71a4-c96f-489f-b1d5-411da70b0b41": {"__data__": {"id_": "71ba71a4-c96f-489f-b1d5-411da70b0b41", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "908057c1-5538-4f52-9917-7457ce407ca1", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "22a149b05294f8bca56bc76029e10304f8ec64a4abf8ceaca33402d54b2f31ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c7e4e77-cbba-4412-8575-71d8061af191", "node_type": "1", "metadata": {}, "hash": "5d3255181bcbf36b6277fe68d08ee8bcedac07aee9bf558c8529fd9df22623c2", "class_name": "RelatedNodeInfo"}}, "text": "None}]\n                                            [--padding {left,right,None}]\n    \n    \u2728 Run AutoTrain LLM\n    \n    options:\n      -h, --help            show this help message and exit\n      --train               Command to train the model\n      --deploy              Command to deploy the model (limited availability)\n      --inference           Command to run inference (limited availability)\n      --username USERNAME   Hugging Face Hub Username\n      --backend {local-cli,spaces-a10gl,spaces-a10gs,spaces-a100,spaces-t4m,spaces-t4s,spaces-cpu,spaces-cpuf}\n                            Backend to use: default or spaces. Spaces backend requires push_to_hub & username. Advanced users only.\n      --token TOKEN         Your Hugging Face API token. Token must have write access to the model hub.\n      --push-to-hub         Push to hub after training will push the trained model to the Hugging Face model hub.\n      --model MODEL         Base model to use for training\n      --project-name PROJECT_NAME\n                            Output directory / repo id for trained model (must be unique on hub)\n      --data-path DATA_PATH\n                            Train dataset to use.", "start_char_idx": 6457, "end_char_idx": 7637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c7e4e77-cbba-4412-8575-71d8061af191": {"__data__": {"id_": "9c7e4e77-cbba-4412-8575-71d8061af191", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71ba71a4-c96f-489f-b1d5-411da70b0b41", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "92a01d97a818c401e4a549d2ea635230c839e0cdd7513aea82c0d4d410397692", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a3d3d70-4e9a-42f3-a3b6-4c17bcfabb67", "node_type": "1", "metadata": {}, "hash": "13b9fde9f89ffd7e6a283dab5ee5477d863e0929fea2c9130a7c544e0c2cc2dd", "class_name": "RelatedNodeInfo"}}, "text": "spaces-a10gl,spaces-a10gs,spaces-a100,spaces-t4m,spaces-t4s,spaces-cpu,spaces-cpuf}\n                            Backend to use: default or spaces. Spaces backend requires push_to_hub & username. Advanced users only.\n      --token TOKEN         Your Hugging Face API token. Token must have write access to the model hub.\n      --push-to-hub         Push to hub after training will push the trained model to the Hugging Face model hub.\n      --model MODEL         Base model to use for training\n      --project-name PROJECT_NAME\n                            Output directory / repo id for trained model (must be unique on hub)\n      --data-path DATA_PATH\n                            Train dataset to use. When using cli, this should be a directory path containing training and validation data in appropriate\n                            formats\n      --train-split TRAIN_SPLIT\n                            Train dataset split to use\n      --valid-split VALID_SPLIT\n                            Validation dataset split to use\n      --batch-size BATCH_SIZE, --train-batch-size BATCH_SIZE\n                            Training batch size to use\n      --seed SEED           Random seed for reproducibility\n      --epochs EPOCHS       Number of training epochs\n      --gradient_accumulation GRADIENT_ACCUMULATION, --gradient-accumulation GRADIENT_ACCUMULATION\n                            Gradient accumulation steps\n      --disable_gradient_checkpointing, --disable-gradient-checkpointing, --disable-gc\n                            Disable gradient checkpointing\n      --lr LR               Learning rate\n      --log {none,wandb,tensorboard}\n                            Use experiment tracking\n      --text_column TEXT_COLUMN, --text-column TEXT_COLUMN\n                            Specify the dataset column to use for text data.", "start_char_idx": 6936, "end_char_idx": 8753, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a3d3d70-4e9a-42f3-a3b6-4c17bcfabb67": {"__data__": {"id_": "2a3d3d70-4e9a-42f3-a3b6-4c17bcfabb67", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c7e4e77-cbba-4412-8575-71d8061af191", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "fdf53b4ae3fcd4592dc2b01ef91e6baf940c6b7e3abe872311086de1df940b2a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebf078fa-58e4-4f2b-b6da-6f5ac78b2476", "node_type": "1", "metadata": {}, "hash": "b44bc886d63552134826a54cf50353fbdde7fe6798cf0110739ca0bdfb08ddff", "class_name": "RelatedNodeInfo"}}, "text": "When using cli, this should be a directory path containing training and validation data in appropriate\n                            formats\n      --train-split TRAIN_SPLIT\n                            Train dataset split to use\n      --valid-split VALID_SPLIT\n                            Validation dataset split to use\n      --batch-size BATCH_SIZE, --train-batch-size BATCH_SIZE\n                            Training batch size to use\n      --seed SEED           Random seed for reproducibility\n      --epochs EPOCHS       Number of training epochs\n      --gradient_accumulation GRADIENT_ACCUMULATION, --gradient-accumulation GRADIENT_ACCUMULATION\n                            Gradient accumulation steps\n      --disable_gradient_checkpointing, --disable-gradient-checkpointing, --disable-gc\n                            Disable gradient checkpointing\n      --lr LR               Learning rate\n      --log {none,wandb,tensorboard}\n                            Use experiment tracking\n      --text_column TEXT_COLUMN, --text-column TEXT_COLUMN\n                            Specify the dataset column to use for text data. This parameter is essential for models processing textual information.\n                            Default is 'text'.\n      --rejected_text_column REJECTED_TEXT_COLUMN, --rejected-text-column REJECTED_TEXT_COLUMN\n                            Define the column to use for storing rejected text entries, which are typically entries that do not meet certain criteria\n                            for processing. Default is 'rejected'.", "start_char_idx": 7638, "end_char_idx": 9183, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebf078fa-58e4-4f2b-b6da-6f5ac78b2476": {"__data__": {"id_": "ebf078fa-58e4-4f2b-b6da-6f5ac78b2476", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a3d3d70-4e9a-42f3-a3b6-4c17bcfabb67", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "35984df7446c763edc734d406bf467e819fbfc405ce3fbcb1a33da464b9fef24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6d4c295-eb5c-4356-9226-67a8f15eb309", "node_type": "1", "metadata": {}, "hash": "ea9166117cc0297e5710934f3c4b18612f274a4ec1a949c366a26063bd123157", "class_name": "RelatedNodeInfo"}}, "text": "This parameter is essential for models processing textual information.\n                            Default is 'text'.\n      --rejected_text_column REJECTED_TEXT_COLUMN, --rejected-text-column REJECTED_TEXT_COLUMN\n                            Define the column to use for storing rejected text entries, which are typically entries that do not meet certain criteria\n                            for processing. Default is 'rejected'. Used only for orpo, dpo and reward trainerss\n      --prompt-text-column PROMPT_TEXT_COLUMN, --prompt-text-column PROMPT_TEXT_COLUMN\n                            Identify the column that contains prompt text for tasks requiring contextual inputs, such as conversation or completion\n                            generation. Default is 'prompt'. Used only for dpo trainer\n      --model-ref MODEL_REF\n                            Reference model to use for DPO when not using PEFT\n      --warmup_ratio WARMUP_RATIO, --warmup-ratio WARMUP_RATIO\n                            Set the proportion of training allocated to warming up the learning rate, which can enhance model stability and performance\n                            at the start of training. Default is 0.1\n      --optimizer OPTIMIZER\n                            Choose the optimizer algorithm for training the model. Different optimizers can affect the training speed and model\n                            performance. 'adamw_torch' is used by default.", "start_char_idx": 8754, "end_char_idx": 10188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6d4c295-eb5c-4356-9226-67a8f15eb309": {"__data__": {"id_": "f6d4c295-eb5c-4356-9226-67a8f15eb309", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebf078fa-58e4-4f2b-b6da-6f5ac78b2476", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "e2c61f32b33895a1067c76709c017b2a81cbdd150bd8201a79147fd0c213b894", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0afca207-c02f-4409-b471-c00a944d363b", "node_type": "1", "metadata": {}, "hash": "9f7b7fd3b978ea6c68b81e867f2e1640e9e1757406009f8a0ee72d834e8b26d4", "class_name": "RelatedNodeInfo"}}, "text": "Default is 'rejected'. Used only for orpo, dpo and reward trainerss\n      --prompt-text-column PROMPT_TEXT_COLUMN, --prompt-text-column PROMPT_TEXT_COLUMN\n                            Identify the column that contains prompt text for tasks requiring contextual inputs, such as conversation or completion\n                            generation. Default is 'prompt'. Used only for dpo trainer\n      --model-ref MODEL_REF\n                            Reference model to use for DPO when not using PEFT\n      --warmup_ratio WARMUP_RATIO, --warmup-ratio WARMUP_RATIO\n                            Set the proportion of training allocated to warming up the learning rate, which can enhance model stability and performance\n                            at the start of training. Default is 0.1\n      --optimizer OPTIMIZER\n                            Choose the optimizer algorithm for training the model. Different optimizers can affect the training speed and model\n                            performance. 'adamw_torch' is used by default.\n      --scheduler SCHEDULER\n                            Select the learning rate scheduler to adjust the learning rate based on the number of epochs. 'linear' decreases the\n                            learning rate linearly from the initial lr set. Default is 'linear'. Try 'cosine' for a cosine annealing schedule.", "start_char_idx": 9161, "end_char_idx": 10504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0afca207-c02f-4409-b471-c00a944d363b": {"__data__": {"id_": "0afca207-c02f-4409-b471-c00a944d363b", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6d4c295-eb5c-4356-9226-67a8f15eb309", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "8599640b72794e971cd40187659ef1c53e6273afa68e51b7e7b3021dedc1e5cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a00ef065-91e7-4375-abb3-f74b161d709f", "node_type": "1", "metadata": {}, "hash": "019cdb95e7b8a23f0b6900d7862cb2fce953927920bb7b73875b27f0b3e53f84", "class_name": "RelatedNodeInfo"}}, "text": "Default is 'prompt'. Used only for dpo trainer\n      --model-ref MODEL_REF\n                            Reference model to use for DPO when not using PEFT\n      --warmup_ratio WARMUP_RATIO, --warmup-ratio WARMUP_RATIO\n                            Set the proportion of training allocated to warming up the learning rate, which can enhance model stability and performance\n                            at the start of training. Default is 0.1\n      --optimizer OPTIMIZER\n                            Choose the optimizer algorithm for training the model. Different optimizers can affect the training speed and model\n                            performance. 'adamw_torch' is used by default.\n      --scheduler SCHEDULER\n                            Select the learning rate scheduler to adjust the learning rate based on the number of epochs. 'linear' decreases the\n                            learning rate linearly from the initial lr set. Default is 'linear'. Try 'cosine' for a cosine annealing schedule.\n      --weight_decay WEIGHT_DECAY, --weight-decay WEIGHT_DECAY\n                            Define the weight decay rate for regularization, which helps prevent overfitting by penalizing larger weights.", "start_char_idx": 9504, "end_char_idx": 10706, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a00ef065-91e7-4375-abb3-f74b161d709f": {"__data__": {"id_": "a00ef065-91e7-4375-abb3-f74b161d709f", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0afca207-c02f-4409-b471-c00a944d363b", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "e839e2148622e773ef69122a7a01a9ce64c05d279c4455304a45bc6de5a6f19d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3d6a8b6-b585-4417-bbc7-d0f7a3aa2d4e", "node_type": "1", "metadata": {}, "hash": "87a3be06080864c12ac0125bdbe36e380e42e325c7ea7397129da8b3c1c881ff", "class_name": "RelatedNodeInfo"}}, "text": "Default is 0.1\n      --optimizer OPTIMIZER\n                            Choose the optimizer algorithm for training the model. Different optimizers can affect the training speed and model\n                            performance. 'adamw_torch' is used by default.\n      --scheduler SCHEDULER\n                            Select the learning rate scheduler to adjust the learning rate based on the number of epochs. 'linear' decreases the\n                            learning rate linearly from the initial lr set. Default is 'linear'. Try 'cosine' for a cosine annealing schedule.\n      --weight_decay WEIGHT_DECAY, --weight-decay WEIGHT_DECAY\n                            Define the weight decay rate for regularization, which helps prevent overfitting by penalizing larger weights. Default is\n                            0.0\n      --max_grad_norm MAX_GRAD_NORM, --max-grad-norm MAX_GRAD_NORM\n                            Set the maximum norm for gradient clipping, which is critical for preventing gradients from exploding during\n                            backpropagation. Default is 1.0.\n      --add_eos_token, --add-eos-token\n                            Toggle whether to automatically add an End Of Sentence (EOS) token at the end of texts, which can be critical for certain\n                            types of models like language models.", "start_char_idx": 9927, "end_char_idx": 11269, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3d6a8b6-b585-4417-bbc7-d0f7a3aa2d4e": {"__data__": {"id_": "d3d6a8b6-b585-4417-bbc7-d0f7a3aa2d4e", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a00ef065-91e7-4375-abb3-f74b161d709f", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "8ee33e8e605be4959231fa6303c052bd73889ca3fcefd1b1a572f5cd790d69e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1706ca01-2bce-4b8d-9c89-6e85f6a8d497", "node_type": "1", "metadata": {}, "hash": "e14c49a20bb0208b1ed80976d2ed364fd166439395c5511fb5d67e0a807af020", "class_name": "RelatedNodeInfo"}}, "text": "'linear' decreases the\n                            learning rate linearly from the initial lr set. Default is 'linear'. Try 'cosine' for a cosine annealing schedule.\n      --weight_decay WEIGHT_DECAY, --weight-decay WEIGHT_DECAY\n                            Define the weight decay rate for regularization, which helps prevent overfitting by penalizing larger weights. Default is\n                            0.0\n      --max_grad_norm MAX_GRAD_NORM, --max-grad-norm MAX_GRAD_NORM\n                            Set the maximum norm for gradient clipping, which is critical for preventing gradients from exploding during\n                            backpropagation. Default is 1.0.\n      --add_eos_token, --add-eos-token\n                            Toggle whether to automatically add an End Of Sentence (EOS) token at the end of texts, which can be critical for certain\n                            types of models like language models. Only used for `default` trainer\n      --block_size BLOCK_SIZE, --block-size BLOCK_SIZE\n                            Specify the block size for processing sequences. This is maximum sequence length or length of one block of text. Setting to\n                            -1 determines block size automatically. Default is -1.", "start_char_idx": 10339, "end_char_idx": 11591, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1706ca01-2bce-4b8d-9c89-6e85f6a8d497": {"__data__": {"id_": "1706ca01-2bce-4b8d-9c89-6e85f6a8d497", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3d6a8b6-b585-4417-bbc7-d0f7a3aa2d4e", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "34cb2a7e1326a11466982a3f1ce942db07384c980a469d447538bbc8a2fb8868", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c57d1b90-550c-4e6d-b7eb-ea8523bd43dc", "node_type": "1", "metadata": {}, "hash": "bcbcf8d4550f374e97750998e623e6fffbae64e96f8297fa33b9894c78270ccb", "class_name": "RelatedNodeInfo"}}, "text": "Default is\n                            0.0\n      --max_grad_norm MAX_GRAD_NORM, --max-grad-norm MAX_GRAD_NORM\n                            Set the maximum norm for gradient clipping, which is critical for preventing gradients from exploding during\n                            backpropagation. Default is 1.0.\n      --add_eos_token, --add-eos-token\n                            Toggle whether to automatically add an End Of Sentence (EOS) token at the end of texts, which can be critical for certain\n                            types of models like language models. Only used for `default` trainer\n      --block_size BLOCK_SIZE, --block-size BLOCK_SIZE\n                            Specify the block size for processing sequences. This is maximum sequence length or length of one block of text. Setting to\n                            -1 determines block size automatically. Default is -1.\n      --peft, --use-peft    Enable LoRA-PEFT\n      --lora_r LORA_R, --lora-r LORA_R\n                            Set the 'r' parameter for Low-Rank Adaptation (LoRA). Default is 16.\n      --lora_alpha LORA_ALPHA, --lora-alpha LORA_ALPHA\n                            Specify the 'alpha' parameter for LoRA. Default is 32.", "start_char_idx": 10707, "end_char_idx": 11910, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c57d1b90-550c-4e6d-b7eb-ea8523bd43dc": {"__data__": {"id_": "c57d1b90-550c-4e6d-b7eb-ea8523bd43dc", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1706ca01-2bce-4b8d-9c89-6e85f6a8d497", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "ebd1e75f3ec6731a9e54249b6531b2dd89e22447fcbe7de2369671ac9b936248", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04840b54-4582-4c13-b842-dac58201c2f0", "node_type": "1", "metadata": {}, "hash": "3a6820ddb6dbb694febbd68a39ca3e56d1573337e23d69fb7ba6cea45a2c18bb", "class_name": "RelatedNodeInfo"}}, "text": "Only used for `default` trainer\n      --block_size BLOCK_SIZE, --block-size BLOCK_SIZE\n                            Specify the block size for processing sequences. This is maximum sequence length or length of one block of text. Setting to\n                            -1 determines block size automatically. Default is -1.\n      --peft, --use-peft    Enable LoRA-PEFT\n      --lora_r LORA_R, --lora-r LORA_R\n                            Set the 'r' parameter for Low-Rank Adaptation (LoRA). Default is 16.\n      --lora_alpha LORA_ALPHA, --lora-alpha LORA_ALPHA\n                            Specify the 'alpha' parameter for LoRA. Default is 32.\n      --lora_dropout LORA_DROPOUT, --lora-dropout LORA_DROPOUT\n                            Set the dropout rate within the LoRA layers to help prevent overfitting during adaptation. Default is 0.05.\n      --logging_steps LOGGING_STEPS, --logging-steps LOGGING_STEPS\n                            Determine how often to log training progress in terms of steps. Setting it to '-1' determines logging steps automatically.", "start_char_idx": 11270, "end_char_idx": 12327, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04840b54-4582-4c13-b842-dac58201c2f0": {"__data__": {"id_": "04840b54-4582-4c13-b842-dac58201c2f0", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c57d1b90-550c-4e6d-b7eb-ea8523bd43dc", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "3737740bf8af2733dca5ffe0789c11ef5da0c68c5b8e4efb37c0c90f707e59bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1461b113-2471-48f7-ade4-00c860818132", "node_type": "1", "metadata": {}, "hash": "35f9f94130a28b94e945d6843ea8d28122a1fe2cb46ed2814abb4ef125716be9", "class_name": "RelatedNodeInfo"}}, "text": "Default is -1.\n      --peft, --use-peft    Enable LoRA-PEFT\n      --lora_r LORA_R, --lora-r LORA_R\n                            Set the 'r' parameter for Low-Rank Adaptation (LoRA). Default is 16.\n      --lora_alpha LORA_ALPHA, --lora-alpha LORA_ALPHA\n                            Specify the 'alpha' parameter for LoRA. Default is 32.\n      --lora_dropout LORA_DROPOUT, --lora-dropout LORA_DROPOUT\n                            Set the dropout rate within the LoRA layers to help prevent overfitting during adaptation. Default is 0.05.\n      --logging_steps LOGGING_STEPS, --logging-steps LOGGING_STEPS\n                            Determine how often to log training progress in terms of steps. Setting it to '-1' determines logging steps automatically.\n      --evaluation_strategy {epoch,steps,no}, --evaluation-strategy {epoch,steps,no}\n                            Choose how frequently to evaluate the model's performance, with 'epoch' as the default, meaning at the end of each training\n                            epoch\n      --save_total_limit SAVE_TOTAL_LIMIT, --save-total-limit SAVE_TOTAL_LIMIT\n                            Limit the total number of saved model checkpoints to manage disk usage effectively.", "start_char_idx": 11577, "end_char_idx": 12789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1461b113-2471-48f7-ade4-00c860818132": {"__data__": {"id_": "1461b113-2471-48f7-ade4-00c860818132", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04840b54-4582-4c13-b842-dac58201c2f0", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "2d0d1a8b47b1583a0309b8b3ef9c461a45e5703d94081425cd4b6e5fbd461b8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "899863e5-7edc-457d-993c-d7c6f08707f1", "node_type": "1", "metadata": {}, "hash": "7054779b2f2951e16da51a50e2301371aeddbc71061ce0a6b5ea54777968fd33", "class_name": "RelatedNodeInfo"}}, "text": "Default is 32.\n      --lora_dropout LORA_DROPOUT, --lora-dropout LORA_DROPOUT\n                            Set the dropout rate within the LoRA layers to help prevent overfitting during adaptation. Default is 0.05.\n      --logging_steps LOGGING_STEPS, --logging-steps LOGGING_STEPS\n                            Determine how often to log training progress in terms of steps. Setting it to '-1' determines logging steps automatically.\n      --evaluation_strategy {epoch,steps,no}, --evaluation-strategy {epoch,steps,no}\n                            Choose how frequently to evaluate the model's performance, with 'epoch' as the default, meaning at the end of each training\n                            epoch\n      --save_total_limit SAVE_TOTAL_LIMIT, --save-total-limit SAVE_TOTAL_LIMIT\n                            Limit the total number of saved model checkpoints to manage disk usage effectively. Default is to save only the latest\n                            checkpoint\n      --auto_find_batch_size, --auto-find-batch-size\n                            Automatically determine the optimal batch size based on system capabilities to maximize efficiency.\n      --mixed_precision {fp16,bf16,None}, --mixed-precision {fp16,bf16,None}\n                            Choose the precision mode for training to optimize performance and memory usage.", "start_char_idx": 11896, "end_char_idx": 13230, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "899863e5-7edc-457d-993c-d7c6f08707f1": {"__data__": {"id_": "899863e5-7edc-457d-993c-d7c6f08707f1", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1461b113-2471-48f7-ade4-00c860818132", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "de51ae543d222c147d8008b29093fefd17bba6a7749de626981ec20269c2e6d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2198b04d-c2f6-4ffd-bb28-3ca717be4f67", "node_type": "1", "metadata": {}, "hash": "c52d02cbcbaa30074e501e526b4bb30571b3548e0897cb07c7885ccf2597a5e8", "class_name": "RelatedNodeInfo"}}, "text": "Setting it to '-1' determines logging steps automatically.\n      --evaluation_strategy {epoch,steps,no}, --evaluation-strategy {epoch,steps,no}\n                            Choose how frequently to evaluate the model's performance, with 'epoch' as the default, meaning at the end of each training\n                            epoch\n      --save_total_limit SAVE_TOTAL_LIMIT, --save-total-limit SAVE_TOTAL_LIMIT\n                            Limit the total number of saved model checkpoints to manage disk usage effectively. Default is to save only the latest\n                            checkpoint\n      --auto_find_batch_size, --auto-find-batch-size\n                            Automatically determine the optimal batch size based on system capabilities to maximize efficiency.\n      --mixed_precision {fp16,bf16,None}, --mixed-precision {fp16,bf16,None}\n                            Choose the precision mode for training to optimize performance and memory usage. Options are 'fp16', 'bf16', or None for\n                            default precision. Default is None.\n      --quantization {int4,int8,None}, --quantization {int4,int8,None}\n                            Choose the quantization level to reduce model size and potentially increase inference speed. Options include 'int4', 'int8',\n                            or None.", "start_char_idx": 12269, "end_char_idx": 13595, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2198b04d-c2f6-4ffd-bb28-3ca717be4f67": {"__data__": {"id_": "2198b04d-c2f6-4ffd-bb28-3ca717be4f67", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "899863e5-7edc-457d-993c-d7c6f08707f1", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "49d335795431640bfcd721f67afcba08d606c9e4d37cc74d295569410c0c8129", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a6adc43-b828-4ba6-b1dd-3f2ad607d173", "node_type": "1", "metadata": {}, "hash": "f3318895ec8c6e54ef3faa9c2eb45a1f93ecb3014c49758774d17c4ced544c6e", "class_name": "RelatedNodeInfo"}}, "text": "Default is to save only the latest\n                            checkpoint\n      --auto_find_batch_size, --auto-find-batch-size\n                            Automatically determine the optimal batch size based on system capabilities to maximize efficiency.\n      --mixed_precision {fp16,bf16,None}, --mixed-precision {fp16,bf16,None}\n                            Choose the precision mode for training to optimize performance and memory usage. Options are 'fp16', 'bf16', or None for\n                            default precision. Default is None.\n      --quantization {int4,int8,None}, --quantization {int4,int8,None}\n                            Choose the quantization level to reduce model size and potentially increase inference speed. Options include 'int4', 'int8',\n                            or None. Enabling requires --peft\n      --model_max_length MODEL_MAX_LENGTH, --model-max-length MODEL_MAX_LENGTH\n                            Set the maximum length for the model to process in a single batch, which can affect both performance and memory usage.\n                            Default is 1024\n      --max_prompt_length MAX_PROMPT_LENGTH, --max-prompt-length MAX_PROMPT_LENGTH\n                            Specify the maximum length for prompts used in training, particularly relevant for tasks requiring initial contextual input.\n                            Used only for `orpo` trainer.", "start_char_idx": 12790, "end_char_idx": 14184, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a6adc43-b828-4ba6-b1dd-3f2ad607d173": {"__data__": {"id_": "7a6adc43-b828-4ba6-b1dd-3f2ad607d173", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2198b04d-c2f6-4ffd-bb28-3ca717be4f67", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "1479c0d7a105f857f28a45eb1645b139c876a330c9fc04dd4e400448971b5164", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de652578-8a02-4abb-b8f5-08833817b266", "node_type": "1", "metadata": {}, "hash": "efdb4592f31fceb405b2386079298c90da117b7f00b03ebb6f8727a118407d43", "class_name": "RelatedNodeInfo"}}, "text": "Options are 'fp16', 'bf16', or None for\n                            default precision. Default is None.\n      --quantization {int4,int8,None}, --quantization {int4,int8,None}\n                            Choose the quantization level to reduce model size and potentially increase inference speed. Options include 'int4', 'int8',\n                            or None. Enabling requires --peft\n      --model_max_length MODEL_MAX_LENGTH, --model-max-length MODEL_MAX_LENGTH\n                            Set the maximum length for the model to process in a single batch, which can affect both performance and memory usage.\n                            Default is 1024\n      --max_prompt_length MAX_PROMPT_LENGTH, --max-prompt-length MAX_PROMPT_LENGTH\n                            Specify the maximum length for prompts used in training, particularly relevant for tasks requiring initial contextual input.\n                            Used only for `orpo` trainer.\n      --max_completion_length MAX_COMPLETION_LENGTH, --max-completion-length MAX_COMPLETION_LENGTH\n                            Completion length to use, for orpo: encoder-decoder models only\n      --trainer {default,dpo,sft,orpo,reward}\n                            Trainer type to use\n      --target_modules TARGET_MODULES, --target-modules TARGET_MODULES\n                            Identify specific modules within the model architecture to target with adaptations or optimizations, such as LoRA.", "start_char_idx": 13231, "end_char_idx": 14683, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de652578-8a02-4abb-b8f5-08833817b266": {"__data__": {"id_": "de652578-8a02-4abb-b8f5-08833817b266", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a6adc43-b828-4ba6-b1dd-3f2ad607d173", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "f06b5a2308ea1a3be244120ab34a90a1cb970f57236c28b189362bd8f65f5149", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b383db9b-cc81-4f4c-8ee6-85035f2aeb03", "node_type": "1", "metadata": {}, "hash": "447af004d423005c046cd52baa503b6783d046a91c85a135db611349cca806c8", "class_name": "RelatedNodeInfo"}}, "text": "Enabling requires --peft\n      --model_max_length MODEL_MAX_LENGTH, --model-max-length MODEL_MAX_LENGTH\n                            Set the maximum length for the model to process in a single batch, which can affect both performance and memory usage.\n                            Default is 1024\n      --max_prompt_length MAX_PROMPT_LENGTH, --max-prompt-length MAX_PROMPT_LENGTH\n                            Specify the maximum length for prompts used in training, particularly relevant for tasks requiring initial contextual input.\n                            Used only for `orpo` trainer.\n      --max_completion_length MAX_COMPLETION_LENGTH, --max-completion-length MAX_COMPLETION_LENGTH\n                            Completion length to use, for orpo: encoder-decoder models only\n      --trainer {default,dpo,sft,orpo,reward}\n                            Trainer type to use\n      --target_modules TARGET_MODULES, --target-modules TARGET_MODULES\n                            Identify specific modules within the model architecture to target with adaptations or optimizations, such as LoRA. Comma\n                            separated list of module names. Default is 'all-linear'.", "start_char_idx": 13596, "end_char_idx": 14774, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b383db9b-cc81-4f4c-8ee6-85035f2aeb03": {"__data__": {"id_": "b383db9b-cc81-4f4c-8ee6-85035f2aeb03", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de652578-8a02-4abb-b8f5-08833817b266", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "a164df2f8554d26191fd2f2a36b86db501c916f30df4f650aaadae7fe11fa3fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e4bdfad-2611-461d-b1d8-05791b197111", "node_type": "1", "metadata": {}, "hash": "3b31afdc405041ae68717593b30fcec9bced5ec23a0bab4dd59f5b6aeffbb83e", "class_name": "RelatedNodeInfo"}}, "text": "Default is 1024\n      --max_prompt_length MAX_PROMPT_LENGTH, --max-prompt-length MAX_PROMPT_LENGTH\n                            Specify the maximum length for prompts used in training, particularly relevant for tasks requiring initial contextual input.\n                            Used only for `orpo` trainer.\n      --max_completion_length MAX_COMPLETION_LENGTH, --max-completion-length MAX_COMPLETION_LENGTH\n                            Completion length to use, for orpo: encoder-decoder models only\n      --trainer {default,dpo,sft,orpo,reward}\n                            Trainer type to use\n      --target_modules TARGET_MODULES, --target-modules TARGET_MODULES\n                            Identify specific modules within the model architecture to target with adaptations or optimizations, such as LoRA. Comma\n                            separated list of module names. Default is 'all-linear'.\n      --merge_adapter, --merge-adapter\n                            Use this flag to merge PEFT adapter with the model\n      --use_flash_attention_2, --use-flash-attention-2, --use-fa2\n                            Use flash attention 2\n      --dpo-beta DPO_BETA, --dpo-beta DPO_BETA\n                            Beta for DPO trainer\n      --chat_template {tokenizer,chatml,zephyr,None}, --chat-template {tokenizer,chatml,zephyr,None}\n                            Apply a specific template for chat-based interactions, with options including 'tokenizer', 'chatml', 'zephyr', or None.", "start_char_idx": 13875, "end_char_idx": 15353, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e4bdfad-2611-461d-b1d8-05791b197111": {"__data__": {"id_": "9e4bdfad-2611-461d-b1d8-05791b197111", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b383db9b-cc81-4f4c-8ee6-85035f2aeb03", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "48fc252ed0c242d717866f33701f522464738de489588ab63921483c7645ea28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "945b8fd4-cfd3-4779-bf9e-21287c9fa77d", "node_type": "1", "metadata": {}, "hash": "846fb5c0a3ec17a7e93e4af2f92f08f4018b197dc69961c696e3659fa0718f70", "class_name": "RelatedNodeInfo"}}, "text": "Comma\n                            separated list of module names. Default is 'all-linear'.\n      --merge_adapter, --merge-adapter\n                            Use this flag to merge PEFT adapter with the model\n      --use_flash_attention_2, --use-flash-attention-2, --use-fa2\n                            Use flash attention 2\n      --dpo-beta DPO_BETA, --dpo-beta DPO_BETA\n                            Beta for DPO trainer\n      --chat_template {tokenizer,chatml,zephyr,None}, --chat-template {tokenizer,chatml,zephyr,None}\n                            Apply a specific template for chat-based interactions, with options including 'tokenizer', 'chatml', 'zephyr', or None. This\n                            setting can shape the model's conversational behavior.\n      --padding {left,right,None}, --padding {left,right,None}\n                            Specify the padding direction for sequences, critical for models sensitive to input alignment.", "start_char_idx": 14684, "end_char_idx": 15627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "945b8fd4-cfd3-4779-bf9e-21287c9fa77d": {"__data__": {"id_": "945b8fd4-cfd3-4779-bf9e-21287c9fa77d", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "28b434e5790f667b01c65373aa0ef001751b67bfa1594ebf4c478fa22e0152f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e4bdfad-2611-461d-b1d8-05791b197111", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "fd6528dc821d820f4752064a4f0450ea844a62dc08286814af33222b7c97dffb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1980432a-01e9-4bf7-84a0-6183853f8b62", "node_type": "1", "metadata": {}, "hash": "4c2c3bf84cdd8ab476bb043a3d53a1464e6cd955894cf94cfeb464a30c22d74f", "class_name": "RelatedNodeInfo"}}, "text": "Comma\n                            separated list of module names. Default is 'all-linear'.\n      --merge_adapter, --merge-adapter\n                            Use this flag to merge PEFT adapter with the model\n      --use_flash_attention_2, --use-flash-attention-2, --use-fa2\n                            Use flash attention 2\n      --dpo-beta DPO_BETA, --dpo-beta DPO_BETA\n                            Beta for DPO trainer\n      --chat_template {tokenizer,chatml,zephyr,None}, --chat-template {tokenizer,chatml,zephyr,None}\n                            Apply a specific template for chat-based interactions, with options including 'tokenizer', 'chatml', 'zephyr', or None. This\n                            setting can shape the model's conversational behavior.\n      --padding {left,right,None}, --padding {left,right,None}\n                            Specify the padding direction for sequences, critical for models sensitive to input alignment. Options include 'left',\n                            'right', or None\n\n[< > Update on GitHub](https://github.com/huggingface/autotrain-\nadvanced/blob/main/docs/source/llm_finetuning.mdx)\n\n[\u2190Text Classification](/docs/autotrain/en/text_classification) [Image\nClassification\u2192](/docs/autotrain/en/image_classification)\n\nLLM Finetuning Data Preparation Data Format For SFT / Generic Trainer Data\nFormat For Reward Trainer Data Format For DPO Trainer Parameters", "start_char_idx": 14684, "end_char_idx": 16083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1980432a-01e9-4bf7-84a0-6183853f8b62": {"__data__": {"id_": "1980432a-01e9-4bf7-84a0-6183853f8b62", "embedding": null, "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056", "node_type": "4", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "45175820ed54f636dde5bd2d21ad59ecfdb847577056abbbfb9141da21a4ff4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "945b8fd4-cfd3-4779-bf9e-21287c9fa77d", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "56d4e16a3786425211daeaf72b16f08e00adf2af00a3c4d4f0025163ed589952", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fae173d3-b045-4aef-99e9-e7244c40fb5f", "node_type": "1", "metadata": {}, "hash": "0edb896f61f924162bc58e3690d0bafb85f2d52ab7342092d13ec558cc99f0e0", "class_name": "RelatedNodeInfo"}}, "text": "The Power of Scale for Parameter-Ef\ufb01cient Prompt Tuning\nBrian Lester\u2217Rami Al-Rfou Noah Constant\nGoogle Research\n{brianlester,rmyeid,nconstant}@google.com\nAbstract\nIn this work, we explore \u201cprompt tuning,\u201d\na simple yet effective mechanism for learn-\ning \u201csoft prompts\u201d to condition frozen lan-\nguage models to perform speci\ufb01c downstream\ntasks. Unlike the discrete text prompts used by\nGPT-3, soft prompts are learned through back-\npropagation and can be tuned to incorporate\nsignals from any number of labeled examples.\nOur end-to-end learned approach outperforms\nGPT-3\u2019s few-shot learning by a large margin.\nMore remarkably, through ablations on model\nsize using T5, we show that prompt tuning be-\ncomes more competitive with scale: as mod-\nels exceed billions of parameters, our method\n\u201ccloses the gap\u201d and matches the strong per-\nformance of model tuning (where all model\nweights are tuned). This \ufb01nding is especially\nrelevant because large models are costly to\nshare and serve and the ability to reuse one\nfrozen model for multiple downstream tasks\ncan ease this burden.", "start_char_idx": 0, "end_char_idx": 1073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fae173d3-b045-4aef-99e9-e7244c40fb5f": {"__data__": {"id_": "fae173d3-b045-4aef-99e9-e7244c40fb5f", "embedding": null, "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056", "node_type": "4", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "45175820ed54f636dde5bd2d21ad59ecfdb847577056abbbfb9141da21a4ff4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1980432a-01e9-4bf7-84a0-6183853f8b62", "node_type": "1", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "382972336e2d930ee46f7065ed639a13a5109c03998efa0e3d6275777d482821", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "541eeb4d-b287-4ae5-8cdf-9f1c414d22cd", "node_type": "1", "metadata": {}, "hash": "ee9dbad69473f7c65aa01e9d3b261350de6bb390c545a8813a6b70922f13249d", "class_name": "RelatedNodeInfo"}}, "text": "Unlike the discrete text prompts used by\nGPT-3, soft prompts are learned through back-\npropagation and can be tuned to incorporate\nsignals from any number of labeled examples.\nOur end-to-end learned approach outperforms\nGPT-3\u2019s few-shot learning by a large margin.\nMore remarkably, through ablations on model\nsize using T5, we show that prompt tuning be-\ncomes more competitive with scale: as mod-\nels exceed billions of parameters, our method\n\u201ccloses the gap\u201d and matches the strong per-\nformance of model tuning (where all model\nweights are tuned). This \ufb01nding is especially\nrelevant because large models are costly to\nshare and serve and the ability to reuse one\nfrozen model for multiple downstream tasks\ncan ease this burden. Our method can be seen\nas a simpli\ufb01cation of the recently proposed\n\u201cpre\ufb01x tuning\u201d of Li and Liang (2021) and we\nprovide a comparison to this and other similar\napproaches.", "start_char_idx": 343, "end_char_idx": 1244, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "541eeb4d-b287-4ae5-8cdf-9f1c414d22cd": {"__data__": {"id_": "541eeb4d-b287-4ae5-8cdf-9f1c414d22cd", "embedding": null, "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056", "node_type": "4", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "45175820ed54f636dde5bd2d21ad59ecfdb847577056abbbfb9141da21a4ff4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fae173d3-b045-4aef-99e9-e7244c40fb5f", "node_type": "1", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4bbde10289a7797e1c5ec15acab30934e63139cc055275c5e0de2dd2e9a4603a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f90ea413-7fae-40b5-ace3-061f1c0e9bc8", "node_type": "1", "metadata": {}, "hash": "994f922e4219e016282d005472811005a0543473603ea231c89a0c310058a64d", "class_name": "RelatedNodeInfo"}}, "text": "Our end-to-end learned approach outperforms\nGPT-3\u2019s few-shot learning by a large margin.\nMore remarkably, through ablations on model\nsize using T5, we show that prompt tuning be-\ncomes more competitive with scale: as mod-\nels exceed billions of parameters, our method\n\u201ccloses the gap\u201d and matches the strong per-\nformance of model tuning (where all model\nweights are tuned). This \ufb01nding is especially\nrelevant because large models are costly to\nshare and serve and the ability to reuse one\nfrozen model for multiple downstream tasks\ncan ease this burden. Our method can be seen\nas a simpli\ufb01cation of the recently proposed\n\u201cpre\ufb01x tuning\u201d of Li and Liang (2021) and we\nprovide a comparison to this and other similar\napproaches. Finally, we show that condition-\ning a frozen model with soft prompts confers\nbene\ufb01ts in robustness to domain transfer and\nenables ef\ufb01cient \u201cprompt ensembling.\u201d\n1 Introduction\nWith the wide success of pre-trained large lan-\nguage models, a range of techniques has arisen to\nadapt these general-purpose models to downstream\ntasks.", "start_char_idx": 519, "end_char_idx": 1574, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f90ea413-7fae-40b5-ace3-061f1c0e9bc8": {"__data__": {"id_": "f90ea413-7fae-40b5-ace3-061f1c0e9bc8", "embedding": null, "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056", "node_type": "4", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "45175820ed54f636dde5bd2d21ad59ecfdb847577056abbbfb9141da21a4ff4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "541eeb4d-b287-4ae5-8cdf-9f1c414d22cd", "node_type": "1", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "3d850d25b8fa83ea797ea96a69c156addd1679a96ff4f4da3c8f2937b4981638", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0e3f0f6-e08b-48ba-bed9-d9ce642e11e2", "node_type": "1", "metadata": {}, "hash": "9828ebc2f7b3e358869b99f41c46fe162b2c3e689a05ec16667b733ab24a7c4b", "class_name": "RelatedNodeInfo"}}, "text": "This \ufb01nding is especially\nrelevant because large models are costly to\nshare and serve and the ability to reuse one\nfrozen model for multiple downstream tasks\ncan ease this burden. Our method can be seen\nas a simpli\ufb01cation of the recently proposed\n\u201cpre\ufb01x tuning\u201d of Li and Liang (2021) and we\nprovide a comparison to this and other similar\napproaches. Finally, we show that condition-\ning a frozen model with soft prompts confers\nbene\ufb01ts in robustness to domain transfer and\nenables ef\ufb01cient \u201cprompt ensembling.\u201d\n1 Introduction\nWith the wide success of pre-trained large lan-\nguage models, a range of techniques has arisen to\nadapt these general-purpose models to downstream\ntasks. ELMo (Peters et al., 2018) proposed freezing\nthe pre-trained model and learning a task-speci\ufb01c\nweighting of its per-layer representations.", "start_char_idx": 894, "end_char_idx": 1713, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0e3f0f6-e08b-48ba-bed9-d9ce642e11e2": {"__data__": {"id_": "b0e3f0f6-e08b-48ba-bed9-d9ce642e11e2", "embedding": null, "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056", "node_type": "4", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "45175820ed54f636dde5bd2d21ad59ecfdb847577056abbbfb9141da21a4ff4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f90ea413-7fae-40b5-ace3-061f1c0e9bc8", "node_type": "1", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "f6e84423ea51610226dbd377e2f7c739a0a2d0d244c18ca90b597280f29231dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "add81db4-62a9-4661-8ac2-a19ba07bda8d", "node_type": "1", "metadata": {}, "hash": "3b4a1dad8682c23df646fbd887d967f6ccdea8427acb83906aa182236d4f9cdb", "class_name": "RelatedNodeInfo"}}, "text": "Our method can be seen\nas a simpli\ufb01cation of the recently proposed\n\u201cpre\ufb01x tuning\u201d of Li and Liang (2021) and we\nprovide a comparison to this and other similar\napproaches. Finally, we show that condition-\ning a frozen model with soft prompts confers\nbene\ufb01ts in robustness to domain transfer and\nenables ef\ufb01cient \u201cprompt ensembling.\u201d\n1 Introduction\nWith the wide success of pre-trained large lan-\nguage models, a range of techniques has arisen to\nadapt these general-purpose models to downstream\ntasks. ELMo (Peters et al., 2018) proposed freezing\nthe pre-trained model and learning a task-speci\ufb01c\nweighting of its per-layer representations. How-\never, since GPT (Radford et al., 2018) and BERT\n(Devlin et al., 2019), the dominant adaptation tech-\nnique has been model tuning (or \u201c\ufb01ne-tuning\u201d),\nwhere all model parameters are tuned during adap-\ntation, as proposed by Howard and Ruder (2018).\n\u2217Work done as a Google AI Resident.", "start_char_idx": 1074, "end_char_idx": 2000, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "add81db4-62a9-4661-8ac2-a19ba07bda8d": {"__data__": {"id_": "add81db4-62a9-4661-8ac2-a19ba07bda8d", "embedding": null, "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056", "node_type": "4", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "45175820ed54f636dde5bd2d21ad59ecfdb847577056abbbfb9141da21a4ff4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0e3f0f6-e08b-48ba-bed9-d9ce642e11e2", "node_type": "1", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "19e851d1cb4aef606f7f3f261ae79d320dbb4c8ab0c6b8fd35ed8aef098ed529", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af2f235b-36d9-4c47-be0b-d5252a5e7e01", "node_type": "1", "metadata": {}, "hash": "9ca8bf2a6296e9af13d5411e3b4b0562dce55fcebd903884cc0e791addb28b40", "class_name": "RelatedNodeInfo"}}, "text": "Finally, we show that condition-\ning a frozen model with soft prompts confers\nbene\ufb01ts in robustness to domain transfer and\nenables ef\ufb01cient \u201cprompt ensembling.\u201d\n1 Introduction\nWith the wide success of pre-trained large lan-\nguage models, a range of techniques has arisen to\nadapt these general-purpose models to downstream\ntasks. ELMo (Peters et al., 2018) proposed freezing\nthe pre-trained model and learning a task-speci\ufb01c\nweighting of its per-layer representations. How-\never, since GPT (Radford et al., 2018) and BERT\n(Devlin et al., 2019), the dominant adaptation tech-\nnique has been model tuning (or \u201c\ufb01ne-tuning\u201d),\nwhere all model parameters are tuned during adap-\ntation, as proposed by Howard and Ruder (2018).\n\u2217Work done as a Google AI Resident.\n10810910101011\nModel Parameters5060708090100SuperGLUE Score\nModel Tuning\nModel Tuning (Multi-task)Prompt Design\nPrompt TuningFigure 1: Standard model tuning of T5 achieves strong\nperformance, but requires storing separate copies of the\nmodel for each end task.", "start_char_idx": 1245, "end_char_idx": 2261, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af2f235b-36d9-4c47-be0b-d5252a5e7e01": {"__data__": {"id_": "af2f235b-36d9-4c47-be0b-d5252a5e7e01", "embedding": null, "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056", "node_type": "4", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "45175820ed54f636dde5bd2d21ad59ecfdb847577056abbbfb9141da21a4ff4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "add81db4-62a9-4661-8ac2-a19ba07bda8d", "node_type": "1", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "abfb4fedad481dc1490734cf3e4886d39ec138038cde8766c7d7ccccbd981767", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df545778-2cd9-4d2a-8e1d-68b917880033", "node_type": "1", "metadata": {}, "hash": "72c8075b6cbfb67e2396a71fabb3ea157190b822109b87d46b793e9a98a49e21", "class_name": "RelatedNodeInfo"}}, "text": "ELMo (Peters et al., 2018) proposed freezing\nthe pre-trained model and learning a task-speci\ufb01c\nweighting of its per-layer representations. How-\never, since GPT (Radford et al., 2018) and BERT\n(Devlin et al., 2019), the dominant adaptation tech-\nnique has been model tuning (or \u201c\ufb01ne-tuning\u201d),\nwhere all model parameters are tuned during adap-\ntation, as proposed by Howard and Ruder (2018).\n\u2217Work done as a Google AI Resident.\n10810910101011\nModel Parameters5060708090100SuperGLUE Score\nModel Tuning\nModel Tuning (Multi-task)Prompt Design\nPrompt TuningFigure 1: Standard model tuning of T5 achieves strong\nperformance, but requires storing separate copies of the\nmodel for each end task. Our prompt tuning of T5\nmatches the quality of model tuning as size increases,\nwhile enabling the reuse of a single frozen model for\nall tasks. Our approach signi\ufb01cantly outperforms few-\nshot prompt design using GPT-3. We show mean and\nstandard deviation across 3runs for tuning methods.\nMore recently, Brown et al.", "start_char_idx": 1575, "end_char_idx": 2577, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df545778-2cd9-4d2a-8e1d-68b917880033": {"__data__": {"id_": "df545778-2cd9-4d2a-8e1d-68b917880033", "embedding": null, "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056", "node_type": "4", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "45175820ed54f636dde5bd2d21ad59ecfdb847577056abbbfb9141da21a4ff4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af2f235b-36d9-4c47-be0b-d5252a5e7e01", "node_type": "1", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "c14f372911b3d69bc715493e3feed636bba1ddd3558a94f5bc9a91a4cb9b2b16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "271c8e91-7f36-4ed0-96d4-c03d3ad387f9", "node_type": "1", "metadata": {}, "hash": "d945b7bd0e860e90a0abe2ea917abe3f5bf459f2552589a9bf0cb62b4b3db6c8", "class_name": "RelatedNodeInfo"}}, "text": "\u2217Work done as a Google AI Resident.\n10810910101011\nModel Parameters5060708090100SuperGLUE Score\nModel Tuning\nModel Tuning (Multi-task)Prompt Design\nPrompt TuningFigure 1: Standard model tuning of T5 achieves strong\nperformance, but requires storing separate copies of the\nmodel for each end task. Our prompt tuning of T5\nmatches the quality of model tuning as size increases,\nwhile enabling the reuse of a single frozen model for\nall tasks. Our approach signi\ufb01cantly outperforms few-\nshot prompt design using GPT-3. We show mean and\nstandard deviation across 3runs for tuning methods.\nMore recently, Brown et al. (2020) showed that\nprompt design (or \u201cpriming\u201d) is surprisingly effec-\ntive at modulating a frozen GPT-3 model\u2019s behavior\nthrough text prompts. Prompts are typically com-\nposed of a task description and/or several canonical\nexamples. This return to \u201cfreezing\u201d pre-trained\nmodels is appealing, especially as model size con-\ntinues to increase.", "start_char_idx": 1965, "end_char_idx": 2920, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "271c8e91-7f36-4ed0-96d4-c03d3ad387f9": {"__data__": {"id_": "271c8e91-7f36-4ed0-96d4-c03d3ad387f9", "embedding": null, "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056", "node_type": "4", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "45175820ed54f636dde5bd2d21ad59ecfdb847577056abbbfb9141da21a4ff4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df545778-2cd9-4d2a-8e1d-68b917880033", "node_type": "1", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e4f50242a27a04a0e0066baa555f421415807aa8a68bacc90312727c3c6ff804", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c9695df-4096-409a-a198-9edc65754726", "node_type": "1", "metadata": {}, "hash": "0a5551473da8a1fa4e88655a93c5af8ce8fd9e5f722f85391d7d600d33992d90", "class_name": "RelatedNodeInfo"}}, "text": "Our prompt tuning of T5\nmatches the quality of model tuning as size increases,\nwhile enabling the reuse of a single frozen model for\nall tasks. Our approach signi\ufb01cantly outperforms few-\nshot prompt design using GPT-3. We show mean and\nstandard deviation across 3runs for tuning methods.\nMore recently, Brown et al. (2020) showed that\nprompt design (or \u201cpriming\u201d) is surprisingly effec-\ntive at modulating a frozen GPT-3 model\u2019s behavior\nthrough text prompts. Prompts are typically com-\nposed of a task description and/or several canonical\nexamples. This return to \u201cfreezing\u201d pre-trained\nmodels is appealing, especially as model size con-\ntinues to increase. Rather than requiring a separate\ncopy of the model for each downstream task, a\nsingle generalist model can simultaneously serve\nmany different tasks.\nUnfortunately, prompt-based adaptation has sev-\neral key drawbacks. Task description is error-prone\nand requires human involvement, and the effective-\nness of a prompt is limited by how much condition-\ning text can \ufb01t into the model\u2019s input. As a result,\ndownstream task quality still lags far behind that\nof tuned models.", "start_char_idx": 2262, "end_char_idx": 3393, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c9695df-4096-409a-a198-9edc65754726": {"__data__": {"id_": "8c9695df-4096-409a-a198-9edc65754726", "embedding": null, "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7efdbe16-bdab-48e3-8a39-40e3ea7d9056", "node_type": "4", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "45175820ed54f636dde5bd2d21ad59ecfdb847577056abbbfb9141da21a4ff4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "271c8e91-7f36-4ed0-96d4-c03d3ad387f9", "node_type": "1", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "9d7b6ee877d74c4206b053303f15b8061ccb8f5c6a9eacdd7343e37c4cd1172b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b39f7de-e90c-4163-b46d-3a31b5186b98", "node_type": "1", "metadata": {}, "hash": "e71588639ea68a64b179fd9718d43d7f9fe3cc1d42e55c952dab4c65aad4aa09", "class_name": "RelatedNodeInfo"}}, "text": "More recently, Brown et al. (2020) showed that\nprompt design (or \u201cpriming\u201d) is surprisingly effec-\ntive at modulating a frozen GPT-3 model\u2019s behavior\nthrough text prompts. Prompts are typically com-\nposed of a task description and/or several canonical\nexamples. This return to \u201cfreezing\u201d pre-trained\nmodels is appealing, especially as model size con-\ntinues to increase. Rather than requiring a separate\ncopy of the model for each downstream task, a\nsingle generalist model can simultaneously serve\nmany different tasks.\nUnfortunately, prompt-based adaptation has sev-\neral key drawbacks. Task description is error-prone\nand requires human involvement, and the effective-\nness of a prompt is limited by how much condition-\ning text can \ufb01t into the model\u2019s input. As a result,\ndownstream task quality still lags far behind that\nof tuned models. For instance, GPT-3 175B few-\nshot performance on SuperGLUE is 17.5points be-arXiv:2104.08691v2  [cs.CL]  2 Sep 2021", "start_char_idx": 2550, "end_char_idx": 3510, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b39f7de-e90c-4163-b46d-3a31b5186b98": {"__data__": {"id_": "6b39f7de-e90c-4163-b46d-3a31b5186b98", "embedding": null, "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d70974e38b21b88e7e009ef43596ca7e74c72543f0cbed59d39569282260caba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c9695df-4096-409a-a198-9edc65754726", "node_type": "1", "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "334c56026047b70ba57f5c0f9374d2dda6312c8c8f489d4816cd6b2e9f38ff17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abe7815f-4a0a-42c8-8a49-6419e4a9a9a3", "node_type": "1", "metadata": {}, "hash": "35f59ff085f3e945d5ff4c5fa5645735872bf41fddf2d088a120d62d5f15b98b", "class_name": "RelatedNodeInfo"}}, "text": "Pre-trained \nModel \n(11B params) \nTask A Model \n(11B params) \nTask B Model \n(11B params) \nTask C Model \n(11B params) a1\na2\nb1\nc1\nc2Task A \nBatch \nTask B \nBatch \nTask C \nBatch Pre-trained \nModel \n(11B params) Model Tuning Prompt Tuning \nA\nB\nCMixed-task \nBatch \n(20K params each) a1\nc1\nb1\na2\nc2A\nC\nB\nA\nC\nTask Prompts Figure 2: Model tuning requires making a task-\nspeci\ufb01c copy of the entire pre-trained model for each\ndownstream task and inference must be performed in\nseparate batches. Prompt tuning only requires stor-\ning a small task-speci\ufb01c prompt for each task, and\nenables mixed-task inference using the original pre-\ntrained model. With a T5 \u201cXXL\u201d model, each copy\nof the tuned model requires 11billion parameters.", "start_char_idx": 0, "end_char_idx": 720, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abe7815f-4a0a-42c8-8a49-6419e4a9a9a3": {"__data__": {"id_": "abe7815f-4a0a-42c8-8a49-6419e4a9a9a3", "embedding": null, "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d70974e38b21b88e7e009ef43596ca7e74c72543f0cbed59d39569282260caba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b39f7de-e90c-4163-b46d-3a31b5186b98", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "9cd26d61cb067e12a7b5d0ace9c01916252f31185f5d4b5fab65653d6094df94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8c52cce-ec85-408b-bac7-47281d0b973f", "node_type": "1", "metadata": {}, "hash": "0717ab21431a371d17110d5afba1664e533f2070f3022cb0c9977c440931da8c", "class_name": "RelatedNodeInfo"}}, "text": "Prompt tuning only requires stor-\ning a small task-speci\ufb01c prompt for each task, and\nenables mixed-task inference using the original pre-\ntrained model. With a T5 \u201cXXL\u201d model, each copy\nof the tuned model requires 11billion parameters. By\ncontrast, our tuned prompts would only require 20,480\nparameters per task\u2014a reduction of over \ufb01ve orders of\nmagnitude \u2014assuming a prompt length of 5tokens.\nlow \ufb01ne-tuned T5-XXL (Raffel et al., 2020) ( 71.8\nvs.89.3) despite using 16times more parameters.\nSeveral efforts to automate prompt design have\nbeen recently proposed. Shin et al. (2020) propose\na search algorithm over the discrete space of words,\nguided by the downstream application training data.\nWhile this technique outperforms manual prompt\ndesign, there is still a gap relative to model tuning.\nLi and Liang (2021) propose \u201cpre\ufb01x tuning\u201d\nand show strong results on generative tasks.", "start_char_idx": 485, "end_char_idx": 1370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8c52cce-ec85-408b-bac7-47281d0b973f": {"__data__": {"id_": "c8c52cce-ec85-408b-bac7-47281d0b973f", "embedding": null, "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d70974e38b21b88e7e009ef43596ca7e74c72543f0cbed59d39569282260caba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abe7815f-4a0a-42c8-8a49-6419e4a9a9a3", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "16b30735a3d0fd5c90eb0ccd13f60b746a14017035f66ece89dc775e72dc5006", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "870889d5-121f-4093-92fa-f2edaf2b9a62", "node_type": "1", "metadata": {}, "hash": "1652e3f901915d64433c3ad0d64dbb2ff33373af5f375f760ddfc504246512c5", "class_name": "RelatedNodeInfo"}}, "text": "With a T5 \u201cXXL\u201d model, each copy\nof the tuned model requires 11billion parameters. By\ncontrast, our tuned prompts would only require 20,480\nparameters per task\u2014a reduction of over \ufb01ve orders of\nmagnitude \u2014assuming a prompt length of 5tokens.\nlow \ufb01ne-tuned T5-XXL (Raffel et al., 2020) ( 71.8\nvs.89.3) despite using 16times more parameters.\nSeveral efforts to automate prompt design have\nbeen recently proposed. Shin et al. (2020) propose\na search algorithm over the discrete space of words,\nguided by the downstream application training data.\nWhile this technique outperforms manual prompt\ndesign, there is still a gap relative to model tuning.\nLi and Liang (2021) propose \u201cpre\ufb01x tuning\u201d\nand show strong results on generative tasks. This\nmethod freezes the model parameters and back-\npropagates the error during tuning to pre\ufb01x ac-\ntivations prepended to each layer in the encoder\nstack, including the input layer. Hambardzumyan\net al.", "start_char_idx": 638, "end_char_idx": 1573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "870889d5-121f-4093-92fa-f2edaf2b9a62": {"__data__": {"id_": "870889d5-121f-4093-92fa-f2edaf2b9a62", "embedding": null, "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d70974e38b21b88e7e009ef43596ca7e74c72543f0cbed59d39569282260caba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8c52cce-ec85-408b-bac7-47281d0b973f", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "8957c4799cf67d5b387ce3cd21eff8b668028d9268371a25fc1d1f693f177dec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28a4f583-d4f1-40b8-bc58-92cbc9012602", "node_type": "1", "metadata": {}, "hash": "b9b4bccf2c65c9c73fb5618070449dbda1ead77038b673e262192b7a1a79f165", "class_name": "RelatedNodeInfo"}}, "text": "low \ufb01ne-tuned T5-XXL (Raffel et al., 2020) ( 71.8\nvs.89.3) despite using 16times more parameters.\nSeveral efforts to automate prompt design have\nbeen recently proposed. Shin et al. (2020) propose\na search algorithm over the discrete space of words,\nguided by the downstream application training data.\nWhile this technique outperforms manual prompt\ndesign, there is still a gap relative to model tuning.\nLi and Liang (2021) propose \u201cpre\ufb01x tuning\u201d\nand show strong results on generative tasks. This\nmethod freezes the model parameters and back-\npropagates the error during tuning to pre\ufb01x ac-\ntivations prepended to each layer in the encoder\nstack, including the input layer. Hambardzumyan\net al. (2021) simplify this recipe by restricting the\ntrainable parameters to the input and output sub-\nnetworks of a masked language model, and show\nreasonable results on classi\ufb01cations tasks.\nIn this paper, we propose prompt tuning as a\nfurther simpli\ufb01cation for adapting language models.", "start_char_idx": 880, "end_char_idx": 1857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28a4f583-d4f1-40b8-bc58-92cbc9012602": {"__data__": {"id_": "28a4f583-d4f1-40b8-bc58-92cbc9012602", "embedding": null, "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d70974e38b21b88e7e009ef43596ca7e74c72543f0cbed59d39569282260caba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "870889d5-121f-4093-92fa-f2edaf2b9a62", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "9f84b31f53720e47653fea077bbd0cc7138ae690bfe97ecb6a6410dfc95f40ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8d11877-bc39-46b0-b67e-d255e6e27225", "node_type": "1", "metadata": {}, "hash": "afe393277a9861c2a53e1ce7a94733f7914a826bca5349f039d1911cbd415c49", "class_name": "RelatedNodeInfo"}}, "text": "Shin et al. (2020) propose\na search algorithm over the discrete space of words,\nguided by the downstream application training data.\nWhile this technique outperforms manual prompt\ndesign, there is still a gap relative to model tuning.\nLi and Liang (2021) propose \u201cpre\ufb01x tuning\u201d\nand show strong results on generative tasks. This\nmethod freezes the model parameters and back-\npropagates the error during tuning to pre\ufb01x ac-\ntivations prepended to each layer in the encoder\nstack, including the input layer. Hambardzumyan\net al. (2021) simplify this recipe by restricting the\ntrainable parameters to the input and output sub-\nnetworks of a masked language model, and show\nreasonable results on classi\ufb01cations tasks.\nIn this paper, we propose prompt tuning as a\nfurther simpli\ufb01cation for adapting language models.\nWe freeze the entire pre-trained model and only al-\nlow an additional ktunable tokens per downstream\ntask to be prepended to the input text.", "start_char_idx": 1049, "end_char_idx": 1998, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8d11877-bc39-46b0-b67e-d255e6e27225": {"__data__": {"id_": "c8d11877-bc39-46b0-b67e-d255e6e27225", "embedding": null, "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d70974e38b21b88e7e009ef43596ca7e74c72543f0cbed59d39569282260caba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28a4f583-d4f1-40b8-bc58-92cbc9012602", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "2c37c482777a8e55bc9bba375be9430c0c54c0c63ffca872e2ee704411063da4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b1892aa-1166-484b-9f07-a58dc722146f", "node_type": "1", "metadata": {}, "hash": "3c197f846c7d65bc724310fef68fe1281c3cc3b1d978faa50b40e8690bf6271e", "class_name": "RelatedNodeInfo"}}, "text": "While this technique outperforms manual prompt\ndesign, there is still a gap relative to model tuning.\nLi and Liang (2021) propose \u201cpre\ufb01x tuning\u201d\nand show strong results on generative tasks. This\nmethod freezes the model parameters and back-\npropagates the error during tuning to pre\ufb01x ac-\ntivations prepended to each layer in the encoder\nstack, including the input layer. Hambardzumyan\net al. (2021) simplify this recipe by restricting the\ntrainable parameters to the input and output sub-\nnetworks of a masked language model, and show\nreasonable results on classi\ufb01cations tasks.\nIn this paper, we propose prompt tuning as a\nfurther simpli\ufb01cation for adapting language models.\nWe freeze the entire pre-trained model and only al-\nlow an additional ktunable tokens per downstream\ntask to be prepended to the input text. This \u201csoft\nprompt\u201d is trained end-to-end and can condense\nthe signal from a full labeled dataset, allowing our\nmethod to outperform few-shot prompts and close\nthe quality gap with model tuning (Figure 1).", "start_char_idx": 1181, "end_char_idx": 2203, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b1892aa-1166-484b-9f07-a58dc722146f": {"__data__": {"id_": "7b1892aa-1166-484b-9f07-a58dc722146f", "embedding": null, "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d70974e38b21b88e7e009ef43596ca7e74c72543f0cbed59d39569282260caba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8d11877-bc39-46b0-b67e-d255e6e27225", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4ba663435daee78e720d48a2f32bb41591c6cc5e0b16d0abdf0085ee75294967", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5e9b0a2-356a-4720-b40a-f0cc8ef54d28", "node_type": "1", "metadata": {}, "hash": "d53eca2540e8ef8accebcd454271542c8db73bbb8c43836a6b2954bdef12ef21", "class_name": "RelatedNodeInfo"}}, "text": "Hambardzumyan\net al. (2021) simplify this recipe by restricting the\ntrainable parameters to the input and output sub-\nnetworks of a masked language model, and show\nreasonable results on classi\ufb01cations tasks.\nIn this paper, we propose prompt tuning as a\nfurther simpli\ufb01cation for adapting language models.\nWe freeze the entire pre-trained model and only al-\nlow an additional ktunable tokens per downstream\ntask to be prepended to the input text. This \u201csoft\nprompt\u201d is trained end-to-end and can condense\nthe signal from a full labeled dataset, allowing our\nmethod to outperform few-shot prompts and close\nthe quality gap with model tuning (Figure 1). At\nthe same time, since a single pre-trained model is\nrecycled for all downstream tasks, we retain the ef-\n\ufb01cient serving bene\ufb01ts of frozen models (Figure 2).\nWhile we developed our method concurrentlywith Li and Liang (2021) and Hambardzumyan\net al.", "start_char_idx": 1553, "end_char_idx": 2454, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5e9b0a2-356a-4720-b40a-f0cc8ef54d28": {"__data__": {"id_": "d5e9b0a2-356a-4720-b40a-f0cc8ef54d28", "embedding": null, "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d70974e38b21b88e7e009ef43596ca7e74c72543f0cbed59d39569282260caba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b1892aa-1166-484b-9f07-a58dc722146f", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "5556e5c8a05bb2e93535bf3b13f75a766c22cdb105c54b044a8d2673e5bb6f75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7534e32d-b763-475c-a4ac-a55776b5b47a", "node_type": "1", "metadata": {}, "hash": "c29f7b299608ff1f105aff859ca0db08b0a484ace73c7d510b27870482080feb", "class_name": "RelatedNodeInfo"}}, "text": "In this paper, we propose prompt tuning as a\nfurther simpli\ufb01cation for adapting language models.\nWe freeze the entire pre-trained model and only al-\nlow an additional ktunable tokens per downstream\ntask to be prepended to the input text. This \u201csoft\nprompt\u201d is trained end-to-end and can condense\nthe signal from a full labeled dataset, allowing our\nmethod to outperform few-shot prompts and close\nthe quality gap with model tuning (Figure 1). At\nthe same time, since a single pre-trained model is\nrecycled for all downstream tasks, we retain the ef-\n\ufb01cient serving bene\ufb01ts of frozen models (Figure 2).\nWhile we developed our method concurrentlywith Li and Liang (2021) and Hambardzumyan\net al. (2021), we are the \ufb01rst to show that prompt\ntuning alone (with no intermediate-layer pre\ufb01xes or\ntask-speci\ufb01c output layers) is suf\ufb01cient to be com-\npetitive with model tuning. Through detailed ex-\nperiments in sections 2\u20133, we demonstrate that lan-\nguage model capacity is a key ingredient for these\napproaches to succeed.", "start_char_idx": 1761, "end_char_idx": 2777, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7534e32d-b763-475c-a4ac-a55776b5b47a": {"__data__": {"id_": "7534e32d-b763-475c-a4ac-a55776b5b47a", "embedding": null, "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d70974e38b21b88e7e009ef43596ca7e74c72543f0cbed59d39569282260caba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5e9b0a2-356a-4720-b40a-f0cc8ef54d28", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a60d506501572322d78b354ab001e7bf4d136fe4a9d1511816af9f1af76777de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca7d69d9-5d42-411a-a918-fba0fbd8e3c4", "node_type": "1", "metadata": {}, "hash": "d6aa14ae4eba07a8ef6e78db080cd7a13d76c22b3c872871e4e725bb66eca484", "class_name": "RelatedNodeInfo"}}, "text": "At\nthe same time, since a single pre-trained model is\nrecycled for all downstream tasks, we retain the ef-\n\ufb01cient serving bene\ufb01ts of frozen models (Figure 2).\nWhile we developed our method concurrentlywith Li and Liang (2021) and Hambardzumyan\net al. (2021), we are the \ufb01rst to show that prompt\ntuning alone (with no intermediate-layer pre\ufb01xes or\ntask-speci\ufb01c output layers) is suf\ufb01cient to be com-\npetitive with model tuning. Through detailed ex-\nperiments in sections 2\u20133, we demonstrate that lan-\nguage model capacity is a key ingredient for these\napproaches to succeed. As Figure 1 shows, prompt\ntuning becomes more competitive with scale.\nWe compare with similar approaches in Sec-\ntion 4. Explicitly separating task-speci\ufb01c param-\neters from the \u201cgeneralist\u201d parameters needed for\ngeneral language-understanding has a range of ad-\nditional bene\ufb01ts.", "start_char_idx": 2204, "end_char_idx": 3058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca7d69d9-5d42-411a-a918-fba0fbd8e3c4": {"__data__": {"id_": "ca7d69d9-5d42-411a-a918-fba0fbd8e3c4", "embedding": null, "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d70974e38b21b88e7e009ef43596ca7e74c72543f0cbed59d39569282260caba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7534e32d-b763-475c-a4ac-a55776b5b47a", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "ce5369f8981db5b55ff5f0dd46e37170f6636ad090cc7b313127b47a8e906a59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bf4882e-7fd9-4615-9e27-6696bccd312b", "node_type": "1", "metadata": {}, "hash": "aee3461df5ff40fbc6717dcf1384e12659e6141fb537fe491af537e09191d70a", "class_name": "RelatedNodeInfo"}}, "text": "While we developed our method concurrentlywith Li and Liang (2021) and Hambardzumyan\net al. (2021), we are the \ufb01rst to show that prompt\ntuning alone (with no intermediate-layer pre\ufb01xes or\ntask-speci\ufb01c output layers) is suf\ufb01cient to be com-\npetitive with model tuning. Through detailed ex-\nperiments in sections 2\u20133, we demonstrate that lan-\nguage model capacity is a key ingredient for these\napproaches to succeed. As Figure 1 shows, prompt\ntuning becomes more competitive with scale.\nWe compare with similar approaches in Sec-\ntion 4. Explicitly separating task-speci\ufb01c param-\neters from the \u201cgeneralist\u201d parameters needed for\ngeneral language-understanding has a range of ad-\nditional bene\ufb01ts. We show in Section 5 that by\ncapturing the task de\ufb01nition in the prompt while\nkeeping the generalist parameters \ufb01xed, we are able\nto achieve better resilience to domain shifts.", "start_char_idx": 2363, "end_char_idx": 3235, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5bf4882e-7fd9-4615-9e27-6696bccd312b": {"__data__": {"id_": "5bf4882e-7fd9-4615-9e27-6696bccd312b", "embedding": null, "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d70974e38b21b88e7e009ef43596ca7e74c72543f0cbed59d39569282260caba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca7d69d9-5d42-411a-a918-fba0fbd8e3c4", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "98ceb992374462177d371b19f84d66acb6abea66c4a73aad93cc8c217a636338", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "597e1f74-a8f8-41a5-8c5c-b09664850aea", "node_type": "1", "metadata": {}, "hash": "63e122413b43f5df8a2c8bd30848733a385aa7ea26feaeef5cfbae04beab2b0a", "class_name": "RelatedNodeInfo"}}, "text": "Through detailed ex-\nperiments in sections 2\u20133, we demonstrate that lan-\nguage model capacity is a key ingredient for these\napproaches to succeed. As Figure 1 shows, prompt\ntuning becomes more competitive with scale.\nWe compare with similar approaches in Sec-\ntion 4. Explicitly separating task-speci\ufb01c param-\neters from the \u201cgeneralist\u201d parameters needed for\ngeneral language-understanding has a range of ad-\nditional bene\ufb01ts. We show in Section 5 that by\ncapturing the task de\ufb01nition in the prompt while\nkeeping the generalist parameters \ufb01xed, we are able\nto achieve better resilience to domain shifts. In Sec-\ntion 6, we show that \u201cprompt ensembling\u201d, learn-\ning multiple prompts for the same task, can boost\nquality and is more ef\ufb01cient than classic model en-\nsembling. Finally, in Section 7, we investigate the\ninterpretability of our learned soft prompts. In sum,\nour key contributions are:\n1.Proposing prompt tuning and showing its com-\npetitiveness with model tuning in the regime\nof large language models.\n2.Ablating many design choices, and showing\nquality and robustness improve with scale.", "start_char_idx": 2631, "end_char_idx": 3732, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "597e1f74-a8f8-41a5-8c5c-b09664850aea": {"__data__": {"id_": "597e1f74-a8f8-41a5-8c5c-b09664850aea", "embedding": null, "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d70974e38b21b88e7e009ef43596ca7e74c72543f0cbed59d39569282260caba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5bf4882e-7fd9-4615-9e27-6696bccd312b", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d5cf7814fbdd2b41884b882815739f091624a15a408f9557d2f7e2daff8290c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8faea956-a09b-4548-9943-9553d9a30fa0", "node_type": "1", "metadata": {}, "hash": "e478e08696618f8f432fd11fe972eb16d1e45c7919c919471ab2f3ce2ab267ed", "class_name": "RelatedNodeInfo"}}, "text": "We show in Section 5 that by\ncapturing the task de\ufb01nition in the prompt while\nkeeping the generalist parameters \ufb01xed, we are able\nto achieve better resilience to domain shifts. In Sec-\ntion 6, we show that \u201cprompt ensembling\u201d, learn-\ning multiple prompts for the same task, can boost\nquality and is more ef\ufb01cient than classic model en-\nsembling. Finally, in Section 7, we investigate the\ninterpretability of our learned soft prompts. In sum,\nour key contributions are:\n1.Proposing prompt tuning and showing its com-\npetitiveness with model tuning in the regime\nof large language models.\n2.Ablating many design choices, and showing\nquality and robustness improve with scale.\n3.Showing prompt tuning outperforms model\ntuning on domain shift problems.\n4.Proposing \u201cprompt ensembling\u201d and showing\nits effectiveness.\n2 Prompt Tuning\nFollowing the \u201ctext-to-text\u201d approach of T5 (Raffel\net al., 2020), we cast all tasks as text generation.", "start_char_idx": 3059, "end_char_idx": 3991, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8faea956-a09b-4548-9943-9553d9a30fa0": {"__data__": {"id_": "8faea956-a09b-4548-9943-9553d9a30fa0", "embedding": null, "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d70974e38b21b88e7e009ef43596ca7e74c72543f0cbed59d39569282260caba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "597e1f74-a8f8-41a5-8c5c-b09664850aea", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "20476f0f8cff3b66395e97c0472b2ac5284a517bff48a863f2ffca0e163f7f72", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2c4d149-ea88-482d-8aa1-84a02f4d4512", "node_type": "1", "metadata": {}, "hash": "847880660b63ffa7e1246c190965261c049e455eae8931362adb9bd3547e04ef", "class_name": "RelatedNodeInfo"}}, "text": "In Sec-\ntion 6, we show that \u201cprompt ensembling\u201d, learn-\ning multiple prompts for the same task, can boost\nquality and is more ef\ufb01cient than classic model en-\nsembling. Finally, in Section 7, we investigate the\ninterpretability of our learned soft prompts. In sum,\nour key contributions are:\n1.Proposing prompt tuning and showing its com-\npetitiveness with model tuning in the regime\nof large language models.\n2.Ablating many design choices, and showing\nquality and robustness improve with scale.\n3.Showing prompt tuning outperforms model\ntuning on domain shift problems.\n4.Proposing \u201cprompt ensembling\u201d and showing\nits effectiveness.\n2 Prompt Tuning\nFollowing the \u201ctext-to-text\u201d approach of T5 (Raffel\net al., 2020), we cast all tasks as text generation.\nInstead of modeling classi\ufb01cation as the probabil-\nity of an output class given some input, Pr(y|X),\nwhereXis a series of tokens and yis a single class\nlabel, we now model it as conditional generation,\nwhereYis a sequence of tokens that represent a\nclass label.", "start_char_idx": 3236, "end_char_idx": 4253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2c4d149-ea88-482d-8aa1-84a02f4d4512": {"__data__": {"id_": "d2c4d149-ea88-482d-8aa1-84a02f4d4512", "embedding": null, "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d70974e38b21b88e7e009ef43596ca7e74c72543f0cbed59d39569282260caba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8faea956-a09b-4548-9943-9553d9a30fa0", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "7ff7efe6670bd2a031fcb2eabdda6a81cb371a1174e3536a8ee2a6ac6632ba06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59b4af55-e8b4-4dd2-b5a5-3e3fbb97e799", "node_type": "1", "metadata": {}, "hash": "e0c09a090a1b5149a2ee7dbda8cc42b7d688eeeb7b2af96435e2a980556a6fd5", "class_name": "RelatedNodeInfo"}}, "text": "In sum,\nour key contributions are:\n1.Proposing prompt tuning and showing its com-\npetitiveness with model tuning in the regime\nof large language models.\n2.Ablating many design choices, and showing\nquality and robustness improve with scale.\n3.Showing prompt tuning outperforms model\ntuning on domain shift problems.\n4.Proposing \u201cprompt ensembling\u201d and showing\nits effectiveness.\n2 Prompt Tuning\nFollowing the \u201ctext-to-text\u201d approach of T5 (Raffel\net al., 2020), we cast all tasks as text generation.\nInstead of modeling classi\ufb01cation as the probabil-\nity of an output class given some input, Pr(y|X),\nwhereXis a series of tokens and yis a single class\nlabel, we now model it as conditional generation,\nwhereYis a sequence of tokens that represent a\nclass label. T5 models classi\ufb01cation as Pr\u03b8(Y|X),\nparameterized by the weights, \u03b8, of the transform-\ners (Vaswani et al., 2017) that make up its encoder\nand decoder.", "start_char_idx": 3493, "end_char_idx": 4406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59b4af55-e8b4-4dd2-b5a5-3e3fbb97e799": {"__data__": {"id_": "59b4af55-e8b4-4dd2-b5a5-3e3fbb97e799", "embedding": null, "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb839fa2-4af5-4d52-a855-b900a7fcfa4a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d70974e38b21b88e7e009ef43596ca7e74c72543f0cbed59d39569282260caba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2c4d149-ea88-482d-8aa1-84a02f4d4512", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a6309f9484545ef806ad2fc5fd29610a881a29c3a0aa3fe3d246c9dc3c7bf82f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5dd6cf36-a226-45b7-9ae8-f18a1019719c", "node_type": "1", "metadata": {}, "hash": "94abe99ee465624c121464e5ef810251483f01f69e7bdf5b6838fd1ead065f2f", "class_name": "RelatedNodeInfo"}}, "text": "3.Showing prompt tuning outperforms model\ntuning on domain shift problems.\n4.Proposing \u201cprompt ensembling\u201d and showing\nits effectiveness.\n2 Prompt Tuning\nFollowing the \u201ctext-to-text\u201d approach of T5 (Raffel\net al., 2020), we cast all tasks as text generation.\nInstead of modeling classi\ufb01cation as the probabil-\nity of an output class given some input, Pr(y|X),\nwhereXis a series of tokens and yis a single class\nlabel, we now model it as conditional generation,\nwhereYis a sequence of tokens that represent a\nclass label. T5 models classi\ufb01cation as Pr\u03b8(Y|X),\nparameterized by the weights, \u03b8, of the transform-\ners (Vaswani et al., 2017) that make up its encoder\nand decoder.\nPrompting is the approach of adding extra in-\nformation for the model to condition on during its\ngeneration of Y. Normally, prompting is done\nby prepending a series of tokens, P, to the in-\nputX, such that the model maximizes the likeli-\nhood of the correct Y,Pr\u03b8(Y|[P;X]), while keep-", "start_char_idx": 3733, "end_char_idx": 4692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5dd6cf36-a226-45b7-9ae8-f18a1019719c": {"__data__": {"id_": "5dd6cf36-a226-45b7-9ae8-f18a1019719c", "embedding": null, "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "72dc1137e7ddc6f01fc32ae6a147c779116266a42e198625f39dfb0372fdfd82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59b4af55-e8b4-4dd2-b5a5-3e3fbb97e799", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "66b4c3ce574742bc909718e1f6ec6ecf5e19366a454ed8d5f55fbd9222351066", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0f4bd7d-63d6-498c-a521-103d8e327481", "node_type": "1", "metadata": {}, "hash": "7fb14c1be186600a34d699c09cfcde1af2bfb6d9a00ffdb040a42ed5214372ee", "class_name": "RelatedNodeInfo"}}, "text": "ing the model parameters, \u03b8, \ufb01xed. In GPT-3 ,\nthe representations of the prompt tokens, P=\n{p1,p2,...,pn}, are part of the model\u2019s embed-\nding table, parameterized by the frozen \u03b8. Find-\ning an optimal prompt thus requires the selection\nof prompt tokens, through either manual search\nor non-differentiable search methods (Jiang et al.,\n2020; Shin et al., 2020). Prompt tuning removes\nthe restriction that the prompt Pbe parameterized\nby\u03b8; instead the prompt has its own dedicated pa-\nrameters,\u03b8P, that can be updated. While prompt\ndesign involves selecting prompt tokens from a\n\ufb01xed vocabulary of frozen embeddings, prompt\ntuning can be thought of as using a \ufb01xed prompt\nof special tokens, where only the embeddings of\nthese prompt tokens can be updated. Our new con-\nditional generation is now Pr\u03b8;\u03b8P(Y|[P;X])and\ncan be trained by maximizing the likelihood of Y\nvia backpropagation, while only applying gradient\nupdates to\u03b8P.", "start_char_idx": 0, "end_char_idx": 926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0f4bd7d-63d6-498c-a521-103d8e327481": {"__data__": {"id_": "d0f4bd7d-63d6-498c-a521-103d8e327481", "embedding": null, "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "72dc1137e7ddc6f01fc32ae6a147c779116266a42e198625f39dfb0372fdfd82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5dd6cf36-a226-45b7-9ae8-f18a1019719c", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "8978c47a3a3607eb4c0f922aa1b99c920405aee7c9ccb7520f72c31ae2314afb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50bc0444-5d77-45c7-8a94-c26767d3ca62", "node_type": "1", "metadata": {}, "hash": "8359828a55316bbfd752eb6577e3423e0ad2aeb0cbaac26574ea2b8b5c9892d3", "class_name": "RelatedNodeInfo"}}, "text": "Prompt tuning removes\nthe restriction that the prompt Pbe parameterized\nby\u03b8; instead the prompt has its own dedicated pa-\nrameters,\u03b8P, that can be updated. While prompt\ndesign involves selecting prompt tokens from a\n\ufb01xed vocabulary of frozen embeddings, prompt\ntuning can be thought of as using a \ufb01xed prompt\nof special tokens, where only the embeddings of\nthese prompt tokens can be updated. Our new con-\nditional generation is now Pr\u03b8;\u03b8P(Y|[P;X])and\ncan be trained by maximizing the likelihood of Y\nvia backpropagation, while only applying gradient\nupdates to\u03b8P.\nGiven a series of ntokens,{x1,x2,...,xn}, the\n\ufb01rst thing T5 does is embed the tokens, forming\na matrixXe\u2208Rn\u00d7ewhereeis the dimension of\nthe embedding space. Our soft-prompts are repre-\nsented as a parameter Pe\u2208Rp\u00d7e, wherepis the\nlength of the prompt.", "start_char_idx": 362, "end_char_idx": 1176, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50bc0444-5d77-45c7-8a94-c26767d3ca62": {"__data__": {"id_": "50bc0444-5d77-45c7-8a94-c26767d3ca62", "embedding": null, "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "72dc1137e7ddc6f01fc32ae6a147c779116266a42e198625f39dfb0372fdfd82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0f4bd7d-63d6-498c-a521-103d8e327481", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "43ada4f3a4e902c628f8e1218b9c1c11903513326db2ce5a7dfd306b4878a371", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b170d507-85cb-43e8-9d01-1a6e8ad2b4a1", "node_type": "1", "metadata": {}, "hash": "3e81cde22b0c5c3c2044cee22b30f028310b0130b798dc3feaaa1de022461b48", "class_name": "RelatedNodeInfo"}}, "text": "While prompt\ndesign involves selecting prompt tokens from a\n\ufb01xed vocabulary of frozen embeddings, prompt\ntuning can be thought of as using a \ufb01xed prompt\nof special tokens, where only the embeddings of\nthese prompt tokens can be updated. Our new con-\nditional generation is now Pr\u03b8;\u03b8P(Y|[P;X])and\ncan be trained by maximizing the likelihood of Y\nvia backpropagation, while only applying gradient\nupdates to\u03b8P.\nGiven a series of ntokens,{x1,x2,...,xn}, the\n\ufb01rst thing T5 does is embed the tokens, forming\na matrixXe\u2208Rn\u00d7ewhereeis the dimension of\nthe embedding space. Our soft-prompts are repre-\nsented as a parameter Pe\u2208Rp\u00d7e, wherepis the\nlength of the prompt. Our prompt is then concate-\nnated to the embedded input forming a single ma-\ntrix[Pe;Xe]\u2208R(p+n)\u00d7ewhich then \ufb02ows though\nthe encoder-decoder as normal. Our models are\ntrained to maximize the probability of Y, but only\nthe prompt parameters Peare updated.", "start_char_idx": 518, "end_char_idx": 1430, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b170d507-85cb-43e8-9d01-1a6e8ad2b4a1": {"__data__": {"id_": "b170d507-85cb-43e8-9d01-1a6e8ad2b4a1", "embedding": null, "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "72dc1137e7ddc6f01fc32ae6a147c779116266a42e198625f39dfb0372fdfd82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50bc0444-5d77-45c7-8a94-c26767d3ca62", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "11d7d3b082e3e595ed9e84551512df35d96cf7fdf817cec7e8ff3d2f253a6fde", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "470c222f-3c8c-4dee-a761-52ad973f69e6", "node_type": "1", "metadata": {}, "hash": "ac2459bf4279299303177b4ac43800d2eaac1d451ca25357664efcd0aff5f061", "class_name": "RelatedNodeInfo"}}, "text": "Given a series of ntokens,{x1,x2,...,xn}, the\n\ufb01rst thing T5 does is embed the tokens, forming\na matrixXe\u2208Rn\u00d7ewhereeis the dimension of\nthe embedding space. Our soft-prompts are repre-\nsented as a parameter Pe\u2208Rp\u00d7e, wherepis the\nlength of the prompt. Our prompt is then concate-\nnated to the embedded input forming a single ma-\ntrix[Pe;Xe]\u2208R(p+n)\u00d7ewhich then \ufb02ows though\nthe encoder-decoder as normal. Our models are\ntrained to maximize the probability of Y, but only\nthe prompt parameters Peare updated.\n2.1 Design Decisions\nThere are many possible ways to initialize the\nprompt representations. The simplest is to train\nfrom scratch, using random initialization. A more\nsophisticated option is to initialize each prompt\ntoken to an embedding drawn from the model\u2019s\nvocabulary. Conceptually, our soft-prompt mod-\nulates the frozen network\u2019s behavior in the same\nway as text preceding the input, so it follows that\na word-like representation might serve as a good\ninitialization spot.", "start_char_idx": 927, "end_char_idx": 1910, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "470c222f-3c8c-4dee-a761-52ad973f69e6": {"__data__": {"id_": "470c222f-3c8c-4dee-a761-52ad973f69e6", "embedding": null, "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "72dc1137e7ddc6f01fc32ae6a147c779116266a42e198625f39dfb0372fdfd82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b170d507-85cb-43e8-9d01-1a6e8ad2b4a1", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "be3fa62c3d8345a45185169549ff1fd305aceb5456ed4d9db578f0d6b53c0453", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "841439fc-e91a-4df8-a208-e215dfbd5daf", "node_type": "1", "metadata": {}, "hash": "07f4ddd38e791b4ebdb240d1cdfaf8ebc9041d73fe959f7f4bb044c85bd9689d", "class_name": "RelatedNodeInfo"}}, "text": "Our prompt is then concate-\nnated to the embedded input forming a single ma-\ntrix[Pe;Xe]\u2208R(p+n)\u00d7ewhich then \ufb02ows though\nthe encoder-decoder as normal. Our models are\ntrained to maximize the probability of Y, but only\nthe prompt parameters Peare updated.\n2.1 Design Decisions\nThere are many possible ways to initialize the\nprompt representations. The simplest is to train\nfrom scratch, using random initialization. A more\nsophisticated option is to initialize each prompt\ntoken to an embedding drawn from the model\u2019s\nvocabulary. Conceptually, our soft-prompt mod-\nulates the frozen network\u2019s behavior in the same\nway as text preceding the input, so it follows that\na word-like representation might serve as a good\ninitialization spot. For classi\ufb01cation tasks, a third\noption is to initialize the prompt with embeddings\nthat enumerate the output classes, similar to the\n\u201cverbalizers\u201d of Schick and Sch\u00fctze (2021).", "start_char_idx": 1177, "end_char_idx": 2088, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "841439fc-e91a-4df8-a208-e215dfbd5daf": {"__data__": {"id_": "841439fc-e91a-4df8-a208-e215dfbd5daf", "embedding": null, "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "72dc1137e7ddc6f01fc32ae6a147c779116266a42e198625f39dfb0372fdfd82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "470c222f-3c8c-4dee-a761-52ad973f69e6", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "67cb1d10ca04fd2d8fa41fb631f87402ebc336cae8c5cbfca965e2ef0ce30cb5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f64e898-47f6-4761-8624-474c4c9705c3", "node_type": "1", "metadata": {}, "hash": "44876ed8bdec5524305efa830e915f76d11fbd148c83c557ac4910b499747781", "class_name": "RelatedNodeInfo"}}, "text": "Our models are\ntrained to maximize the probability of Y, but only\nthe prompt parameters Peare updated.\n2.1 Design Decisions\nThere are many possible ways to initialize the\nprompt representations. The simplest is to train\nfrom scratch, using random initialization. A more\nsophisticated option is to initialize each prompt\ntoken to an embedding drawn from the model\u2019s\nvocabulary. Conceptually, our soft-prompt mod-\nulates the frozen network\u2019s behavior in the same\nway as text preceding the input, so it follows that\na word-like representation might serve as a good\ninitialization spot. For classi\ufb01cation tasks, a third\noption is to initialize the prompt with embeddings\nthat enumerate the output classes, similar to the\n\u201cverbalizers\u201d of Schick and Sch\u00fctze (2021). Since\nwe want the model to produce these tokens in the\noutput, initializing the prompt with the embeddings\nof the valid target tokens should prime the model\nto restrict its output to the legal output classes.\nAnother design consideration is the length of theprompt. The parameter cost of our method is EP,\nwhereEis the token embedding dimension and P\nis the prompt length.", "start_char_idx": 1328, "end_char_idx": 2461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f64e898-47f6-4761-8624-474c4c9705c3": {"__data__": {"id_": "6f64e898-47f6-4761-8624-474c4c9705c3", "embedding": null, "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "72dc1137e7ddc6f01fc32ae6a147c779116266a42e198625f39dfb0372fdfd82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "841439fc-e91a-4df8-a208-e215dfbd5daf", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "313a7c9fa6fcb33c559d3c48f03119365425a39da3d460c03cf6d15c8b1027c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1dfc197-1d47-41a5-b039-c784c347c4e9", "node_type": "1", "metadata": {}, "hash": "64d9e732d9971f7e61a0943c1781671e64d15371c6e3e71a9847b82307c98ae5", "class_name": "RelatedNodeInfo"}}, "text": "A more\nsophisticated option is to initialize each prompt\ntoken to an embedding drawn from the model\u2019s\nvocabulary. Conceptually, our soft-prompt mod-\nulates the frozen network\u2019s behavior in the same\nway as text preceding the input, so it follows that\na word-like representation might serve as a good\ninitialization spot. For classi\ufb01cation tasks, a third\noption is to initialize the prompt with embeddings\nthat enumerate the output classes, similar to the\n\u201cverbalizers\u201d of Schick and Sch\u00fctze (2021). Since\nwe want the model to produce these tokens in the\noutput, initializing the prompt with the embeddings\nof the valid target tokens should prime the model\nto restrict its output to the legal output classes.\nAnother design consideration is the length of theprompt. The parameter cost of our method is EP,\nwhereEis the token embedding dimension and P\nis the prompt length. The shorter the prompt, the\nfewer new parameters must be tuned, so we aim to\n\ufb01nd a minimal length that still performs well.", "start_char_idx": 1591, "end_char_idx": 2585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1dfc197-1d47-41a5-b039-c784c347c4e9": {"__data__": {"id_": "d1dfc197-1d47-41a5-b039-c784c347c4e9", "embedding": null, "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "72dc1137e7ddc6f01fc32ae6a147c779116266a42e198625f39dfb0372fdfd82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f64e898-47f6-4761-8624-474c4c9705c3", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d09f3f52784494375d9c78328cb7caeb3ece61a26a8da40905fcd5b35845499b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e04024f-cffa-499c-a471-fc8beae013c4", "node_type": "1", "metadata": {}, "hash": "6710903dd5b395ba8d939494f242d6e630064e382548b6b62119944d65c42116", "class_name": "RelatedNodeInfo"}}, "text": "For classi\ufb01cation tasks, a third\noption is to initialize the prompt with embeddings\nthat enumerate the output classes, similar to the\n\u201cverbalizers\u201d of Schick and Sch\u00fctze (2021). Since\nwe want the model to produce these tokens in the\noutput, initializing the prompt with the embeddings\nof the valid target tokens should prime the model\nto restrict its output to the legal output classes.\nAnother design consideration is the length of theprompt. The parameter cost of our method is EP,\nwhereEis the token embedding dimension and P\nis the prompt length. The shorter the prompt, the\nfewer new parameters must be tuned, so we aim to\n\ufb01nd a minimal length that still performs well.\n2.2 Unlearning Span Corruption\nUnlike autoregressive language models like GPT-3 ,\nthe T5 models we experiment with use an encoder-\ndecoder architecture and pre-train on a span cor-\nruption objective. Speci\ufb01cally, T5 is tasked with\n\u201creconstructing\u201d masked spans in the input text,\nwhich are marked with unique sentinel tokens. The\ntarget output text consists of all the masked con-\ntent, separated by sentinels, plus a \ufb01nal sentinel.", "start_char_idx": 1911, "end_char_idx": 3018, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e04024f-cffa-499c-a471-fc8beae013c4": {"__data__": {"id_": "5e04024f-cffa-499c-a471-fc8beae013c4", "embedding": null, "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "72dc1137e7ddc6f01fc32ae6a147c779116266a42e198625f39dfb0372fdfd82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1dfc197-1d47-41a5-b039-c784c347c4e9", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "8c2188afba4dc4f23dbf6b35b87651b3cec5d742ef00c25de7661e0b7f07201b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d0e057f-7749-4c71-8cd6-b87c52e844a2", "node_type": "1", "metadata": {}, "hash": "cc122635eeea5936c97791f47f2db07ac565e585cfc1e5b7e502b408688927eb", "class_name": "RelatedNodeInfo"}}, "text": "Another design consideration is the length of theprompt. The parameter cost of our method is EP,\nwhereEis the token embedding dimension and P\nis the prompt length. The shorter the prompt, the\nfewer new parameters must be tuned, so we aim to\n\ufb01nd a minimal length that still performs well.\n2.2 Unlearning Span Corruption\nUnlike autoregressive language models like GPT-3 ,\nthe T5 models we experiment with use an encoder-\ndecoder architecture and pre-train on a span cor-\nruption objective. Speci\ufb01cally, T5 is tasked with\n\u201creconstructing\u201d masked spans in the input text,\nwhich are marked with unique sentinel tokens. The\ntarget output text consists of all the masked con-\ntent, separated by sentinels, plus a \ufb01nal sentinel.\nFor instance, from the text \u201cThank you for inviting\nme to your party last week\u201d we might construct\na pre-training example where the input is \u201cThank\nyou\u27e8X\u27e9me to your party\u27e8Y\u27e9week\u201d and the target\noutput is \u201c\u27e8X\u27e9for inviting\u27e8Y\u27e9last\u27e8Z\u27e9\u201d.\nWhile Raffel et al.", "start_char_idx": 2298, "end_char_idx": 3271, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d0e057f-7749-4c71-8cd6-b87c52e844a2": {"__data__": {"id_": "3d0e057f-7749-4c71-8cd6-b87c52e844a2", "embedding": null, "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "72dc1137e7ddc6f01fc32ae6a147c779116266a42e198625f39dfb0372fdfd82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e04024f-cffa-499c-a471-fc8beae013c4", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "63f631412950479740e06c02b0bf850e84c9f61f913dfe2c0898f1c55c570b1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62d35df2-8b78-467c-81fd-33d864e3d891", "node_type": "1", "metadata": {}, "hash": "20bf11b8ff3fc51abcb69e572d0c721f67e3f1dddbb5017408f740b026676d0c", "class_name": "RelatedNodeInfo"}}, "text": "2.2 Unlearning Span Corruption\nUnlike autoregressive language models like GPT-3 ,\nthe T5 models we experiment with use an encoder-\ndecoder architecture and pre-train on a span cor-\nruption objective. Speci\ufb01cally, T5 is tasked with\n\u201creconstructing\u201d masked spans in the input text,\nwhich are marked with unique sentinel tokens. The\ntarget output text consists of all the masked con-\ntent, separated by sentinels, plus a \ufb01nal sentinel.\nFor instance, from the text \u201cThank you for inviting\nme to your party last week\u201d we might construct\na pre-training example where the input is \u201cThank\nyou\u27e8X\u27e9me to your party\u27e8Y\u27e9week\u201d and the target\noutput is \u201c\u27e8X\u27e9for inviting\u27e8Y\u27e9last\u27e8Z\u27e9\u201d.\nWhile Raffel et al. (2020) \ufb01nd this architecture\nand pre-training objective more effective than tradi-\ntional language modeling, we hypothesize that this\nsetup is not a good \ufb01t for producing a frozen model\nthat can be readily controlled through prompt tun-\ning.", "start_char_idx": 2586, "end_char_idx": 3513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62d35df2-8b78-467c-81fd-33d864e3d891": {"__data__": {"id_": "62d35df2-8b78-467c-81fd-33d864e3d891", "embedding": null, "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "72dc1137e7ddc6f01fc32ae6a147c779116266a42e198625f39dfb0372fdfd82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d0e057f-7749-4c71-8cd6-b87c52e844a2", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "08a7a0c7782a5b3599eb601a8814ff719a5252e13e94b6fcfec0a79f67ca1a08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7fb8e670-71d6-4cdb-aaf6-68dcdf95d0a5", "node_type": "1", "metadata": {}, "hash": "03fcbcea244e09327cd4c8d8d189a3c4ad026fd4689a7f3e7ada74d5c0d22325", "class_name": "RelatedNodeInfo"}}, "text": "Speci\ufb01cally, T5 is tasked with\n\u201creconstructing\u201d masked spans in the input text,\nwhich are marked with unique sentinel tokens. The\ntarget output text consists of all the masked con-\ntent, separated by sentinels, plus a \ufb01nal sentinel.\nFor instance, from the text \u201cThank you for inviting\nme to your party last week\u201d we might construct\na pre-training example where the input is \u201cThank\nyou\u27e8X\u27e9me to your party\u27e8Y\u27e9week\u201d and the target\noutput is \u201c\u27e8X\u27e9for inviting\u27e8Y\u27e9last\u27e8Z\u27e9\u201d.\nWhile Raffel et al. (2020) \ufb01nd this architecture\nand pre-training objective more effective than tradi-\ntional language modeling, we hypothesize that this\nsetup is not a good \ufb01t for producing a frozen model\nthat can be readily controlled through prompt tun-\ning. In particular, a T5 model pre-trained exclu-\nsively on span corruption, such as T5.1.1, has never\nseen truly natural input text (free of sentinel to-\nkens), nor has it ever been asked to predict truly\nnatural targets.", "start_char_idx": 2786, "end_char_idx": 3731, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7fb8e670-71d6-4cdb-aaf6-68dcdf95d0a5": {"__data__": {"id_": "7fb8e670-71d6-4cdb-aaf6-68dcdf95d0a5", "embedding": null, "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "72dc1137e7ddc6f01fc32ae6a147c779116266a42e198625f39dfb0372fdfd82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62d35df2-8b78-467c-81fd-33d864e3d891", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "793d5dde8e5a6e3c8b5498cf8d4fcd6cd4c6ab50242d6d2b9dee0b8923d47bf5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34237fa6-9bac-46d1-a490-9e5f747efba9", "node_type": "1", "metadata": {}, "hash": "f007fc04e094b5bcd9f67895608ee12fade3288c8492b66889a625164a3432e2", "class_name": "RelatedNodeInfo"}}, "text": "For instance, from the text \u201cThank you for inviting\nme to your party last week\u201d we might construct\na pre-training example where the input is \u201cThank\nyou\u27e8X\u27e9me to your party\u27e8Y\u27e9week\u201d and the target\noutput is \u201c\u27e8X\u27e9for inviting\u27e8Y\u27e9last\u27e8Z\u27e9\u201d.\nWhile Raffel et al. (2020) \ufb01nd this architecture\nand pre-training objective more effective than tradi-\ntional language modeling, we hypothesize that this\nsetup is not a good \ufb01t for producing a frozen model\nthat can be readily controlled through prompt tun-\ning. In particular, a T5 model pre-trained exclu-\nsively on span corruption, such as T5.1.1, has never\nseen truly natural input text (free of sentinel to-\nkens), nor has it ever been asked to predict truly\nnatural targets. In fact, due to the details of T5\u2019s\nspan corruption preprocessing, every pre-training\ntarget will begin with a sentinel.", "start_char_idx": 3019, "end_char_idx": 3852, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34237fa6-9bac-46d1-a490-9e5f747efba9": {"__data__": {"id_": "34237fa6-9bac-46d1-a490-9e5f747efba9", "embedding": null, "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "72dc1137e7ddc6f01fc32ae6a147c779116266a42e198625f39dfb0372fdfd82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fb8e670-71d6-4cdb-aaf6-68dcdf95d0a5", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "22b42dc97edcb051ca17544cd722d67e7137f10b87dc865ba9a73c014e15954f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3cefa59c-f7df-4e30-a366-c88c0ed43775", "node_type": "1", "metadata": {}, "hash": "218dbb1d3c068e725b5c1d06d2f7322b6e31a7b868a6f734489394f3e932cb2a", "class_name": "RelatedNodeInfo"}}, "text": "While Raffel et al. (2020) \ufb01nd this architecture\nand pre-training objective more effective than tradi-\ntional language modeling, we hypothesize that this\nsetup is not a good \ufb01t for producing a frozen model\nthat can be readily controlled through prompt tun-\ning. In particular, a T5 model pre-trained exclu-\nsively on span corruption, such as T5.1.1, has never\nseen truly natural input text (free of sentinel to-\nkens), nor has it ever been asked to predict truly\nnatural targets. In fact, due to the details of T5\u2019s\nspan corruption preprocessing, every pre-training\ntarget will begin with a sentinel. While this \u201cunnat-\nural\u201d tendency to output sentinels is easy to over-\ncome through \ufb01ne-tuning, we suspect that it would\nbe much harder to override through a prompt alone,\nas the decoder priors cannot be adjusted.\nGiven these concerns, we experiment with T5\nmodels in three settings. (1) \u201cSpan Corruption\u201d:\nWe use pre-trained T5 off-the-shelf as our frozen\nmodel, and test its ability to output the expected\ntext for downstream tasks.", "start_char_idx": 3252, "end_char_idx": 4287, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3cefa59c-f7df-4e30-a366-c88c0ed43775": {"__data__": {"id_": "3cefa59c-f7df-4e30-a366-c88c0ed43775", "embedding": null, "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "72dc1137e7ddc6f01fc32ae6a147c779116266a42e198625f39dfb0372fdfd82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34237fa6-9bac-46d1-a490-9e5f747efba9", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "50af8a3ff214feb7bf3b1a9d564ff3afe5646fa8f267cdd7d9afa79dfefd2c43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f9159f4-891e-442c-a9d1-1345a55fae34", "node_type": "1", "metadata": {}, "hash": "ea55c66bffee921d7e28c866838fb9e0e3dbe69c23eb4c65c6862ef065b6200e", "class_name": "RelatedNodeInfo"}}, "text": "In particular, a T5 model pre-trained exclu-\nsively on span corruption, such as T5.1.1, has never\nseen truly natural input text (free of sentinel to-\nkens), nor has it ever been asked to predict truly\nnatural targets. In fact, due to the details of T5\u2019s\nspan corruption preprocessing, every pre-training\ntarget will begin with a sentinel. While this \u201cunnat-\nural\u201d tendency to output sentinels is easy to over-\ncome through \ufb01ne-tuning, we suspect that it would\nbe much harder to override through a prompt alone,\nas the decoder priors cannot be adjusted.\nGiven these concerns, we experiment with T5\nmodels in three settings. (1) \u201cSpan Corruption\u201d:\nWe use pre-trained T5 off-the-shelf as our frozen\nmodel, and test its ability to output the expected\ntext for downstream tasks. (2) \u201cSpan Corruption\n+ Sentinel\u201d: We use the same model, but prepend\nall downstream targets with a sentinel, so as to\nmore closely resemble the targets seen in pre-\ntraining.", "start_char_idx": 3514, "end_char_idx": 4462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f9159f4-891e-442c-a9d1-1345a55fae34": {"__data__": {"id_": "6f9159f4-891e-442c-a9d1-1345a55fae34", "embedding": null, "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "72dc1137e7ddc6f01fc32ae6a147c779116266a42e198625f39dfb0372fdfd82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3cefa59c-f7df-4e30-a366-c88c0ed43775", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "f806019f475c3ae714faede241fc549b93d59c33476bd04c5e97ea7e4f863dbd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cdee7984-597f-46b9-b8cf-0f0102c8d8ee", "node_type": "1", "metadata": {}, "hash": "2f70abd1c59544af2914e9be9ce1aed88dd95e59a67cb0c4b5ffe743858f5fa6", "class_name": "RelatedNodeInfo"}}, "text": "In fact, due to the details of T5\u2019s\nspan corruption preprocessing, every pre-training\ntarget will begin with a sentinel. While this \u201cunnat-\nural\u201d tendency to output sentinels is easy to over-\ncome through \ufb01ne-tuning, we suspect that it would\nbe much harder to override through a prompt alone,\nas the decoder priors cannot be adjusted.\nGiven these concerns, we experiment with T5\nmodels in three settings. (1) \u201cSpan Corruption\u201d:\nWe use pre-trained T5 off-the-shelf as our frozen\nmodel, and test its ability to output the expected\ntext for downstream tasks. (2) \u201cSpan Corruption\n+ Sentinel\u201d: We use the same model, but prepend\nall downstream targets with a sentinel, so as to\nmore closely resemble the targets seen in pre-\ntraining. (3) \u201cLM Adaptation\u201d: We continue T5\u2019s\nself-supervised training for a small number of ad-\nditional steps, but using the \u201cLM\u201d objective dis-\ncussed by Raffel et al. (2020); given a natural text\npre\ufb01x as input, the model must produce the natural\ntext continuation as output.", "start_char_idx": 3732, "end_char_idx": 4734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdee7984-597f-46b9-b8cf-0f0102c8d8ee": {"__data__": {"id_": "cdee7984-597f-46b9-b8cf-0f0102c8d8ee", "embedding": null, "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bbaa93e8-f909-4e1c-ac49-bf09a93c7129", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "72dc1137e7ddc6f01fc32ae6a147c779116266a42e198625f39dfb0372fdfd82", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f9159f4-891e-442c-a9d1-1345a55fae34", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "8ea5394e92917a59f3235b3d192de34c8c5dace7a5f97689d607ad3a0cc6f443", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3112f268-0c61-46c6-b498-a3ce351b73df", "node_type": "1", "metadata": {}, "hash": "6113e8316e8e61126e909aa1f05868a76165d2192940d37fffc752e70ea67a5c", "class_name": "RelatedNodeInfo"}}, "text": "Given these concerns, we experiment with T5\nmodels in three settings. (1) \u201cSpan Corruption\u201d:\nWe use pre-trained T5 off-the-shelf as our frozen\nmodel, and test its ability to output the expected\ntext for downstream tasks. (2) \u201cSpan Corruption\n+ Sentinel\u201d: We use the same model, but prepend\nall downstream targets with a sentinel, so as to\nmore closely resemble the targets seen in pre-\ntraining. (3) \u201cLM Adaptation\u201d: We continue T5\u2019s\nself-supervised training for a small number of ad-\nditional steps, but using the \u201cLM\u201d objective dis-\ncussed by Raffel et al. (2020); given a natural text\npre\ufb01x as input, the model must produce the natural\ntext continuation as output. Crucially, this adapta-\ntion happens only once , producing a single frozen", "start_char_idx": 4067, "end_char_idx": 4809, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3112f268-0c61-46c6-b498-a3ce351b73df": {"__data__": {"id_": "3112f268-0c61-46c6-b498-a3ce351b73df", "embedding": null, "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "912952e8dcf723d716d4fce59f8fa980aa1e52002151621c8ab589d21bbd5c11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cdee7984-597f-46b9-b8cf-0f0102c8d8ee", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "43028dc5efbbd7cdecffe55f1044681d54aff3a0c4d9aa2ee25a8952f85030ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0716e08d-9822-403e-a133-03733eb2b16a", "node_type": "1", "metadata": {}, "hash": "378dc806b509952f80715497c8baa921c44856790c4e7708b07395104371e630", "class_name": "RelatedNodeInfo"}}, "text": "model that we can reuse for prompt tuning across\nany number of downstream tasks.\nThrough LM adaptation, we hope to \u201cquickly\u201d\ntransform T5 into a model more similar to GPT-3 ,\nwhich always outputs realistic text, and is known to\nrespond well to prompts as a \u201cfew-shot learner\u201d. It\nis not obvious how successful this late-stage trans-\nformation will be compared to pre-training from\nscratch, and it has not been investigated previously\nto our knowledge. As such, we experiment with\nvarious lengths of adaptation up to 100K steps.\n3 Results\nOur frozen models are built on top of pre-trained\nT5 checkpoints of all sizes (Small, Base, Large, XL,\nXXL). We leverage the public T5.1.1 checkpoints,\nwhich include improvements over the original T5.1\nOur \u201cdefault\u201d con\ufb01guration, plotted with a green\n\u2018\u00d7\u2019 (\n ) throughout, uses an LM-adapted version\nof T5 trained for an additional 100K steps, ini-\ntializes using class labels (see Section 3.2), and\nuses a prompt length of 100tokens.", "start_char_idx": 0, "end_char_idx": 971, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0716e08d-9822-403e-a133-03733eb2b16a": {"__data__": {"id_": "0716e08d-9822-403e-a133-03733eb2b16a", "embedding": null, "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "912952e8dcf723d716d4fce59f8fa980aa1e52002151621c8ab589d21bbd5c11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3112f268-0c61-46c6-b498-a3ce351b73df", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "0828acf51dbad27c895f07e67678d0df7c89fcab7b9079275b8ff3efe2642dc8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "514aeb59-695b-4304-b7ed-e3856e745009", "node_type": "1", "metadata": {}, "hash": "044303e4b87d1d9935cd0f472618416a74752699cf09a006735129df2d42b356", "class_name": "RelatedNodeInfo"}}, "text": "It\nis not obvious how successful this late-stage trans-\nformation will be compared to pre-training from\nscratch, and it has not been investigated previously\nto our knowledge. As such, we experiment with\nvarious lengths of adaptation up to 100K steps.\n3 Results\nOur frozen models are built on top of pre-trained\nT5 checkpoints of all sizes (Small, Base, Large, XL,\nXXL). We leverage the public T5.1.1 checkpoints,\nwhich include improvements over the original T5.1\nOur \u201cdefault\u201d con\ufb01guration, plotted with a green\n\u2018\u00d7\u2019 (\n ) throughout, uses an LM-adapted version\nof T5 trained for an additional 100K steps, ini-\ntializes using class labels (see Section 3.2), and\nuses a prompt length of 100tokens. While this\nis longer than the default 10-token pre\ufb01x used by\nLi and Liang (2021), our method still uses fewer\ntask-speci\ufb01c parameters, as we only tune the input\nlayer, as opposed to overwriting activations in all\nnetwork layers. See Figure 4 for a detailed com-\nparison.", "start_char_idx": 277, "end_char_idx": 1242, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "514aeb59-695b-4304-b7ed-e3856e745009": {"__data__": {"id_": "514aeb59-695b-4304-b7ed-e3856e745009", "embedding": null, "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "912952e8dcf723d716d4fce59f8fa980aa1e52002151621c8ab589d21bbd5c11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0716e08d-9822-403e-a133-03733eb2b16a", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "f0d022db2430bb06a6333973348c5f9cece90adda0c8fa82323438fd558a7587", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da15dbb1-b0be-4c94-9127-d8080dc4e416", "node_type": "1", "metadata": {}, "hash": "a25a196c92a41c57b2188a3623391b160f3dff097fd65c6781b3ce31285251b6", "class_name": "RelatedNodeInfo"}}, "text": "3 Results\nOur frozen models are built on top of pre-trained\nT5 checkpoints of all sizes (Small, Base, Large, XL,\nXXL). We leverage the public T5.1.1 checkpoints,\nwhich include improvements over the original T5.1\nOur \u201cdefault\u201d con\ufb01guration, plotted with a green\n\u2018\u00d7\u2019 (\n ) throughout, uses an LM-adapted version\nof T5 trained for an additional 100K steps, ini-\ntializes using class labels (see Section 3.2), and\nuses a prompt length of 100tokens. While this\nis longer than the default 10-token pre\ufb01x used by\nLi and Liang (2021), our method still uses fewer\ntask-speci\ufb01c parameters, as we only tune the input\nlayer, as opposed to overwriting activations in all\nnetwork layers. See Figure 4 for a detailed com-\nparison. We will also see shortly that even much\nshorter prompts are viable as model size increases.\nWe measure performance on the SuperGLUE\nbenchmark (Wang et al., 2019a), a collection of\neight challenging English language understanding\ntasks.2We report metrics on the development set\nassociated with each dataset.", "start_char_idx": 528, "end_char_idx": 1549, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da15dbb1-b0be-4c94-9127-d8080dc4e416": {"__data__": {"id_": "da15dbb1-b0be-4c94-9127-d8080dc4e416", "embedding": null, "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "912952e8dcf723d716d4fce59f8fa980aa1e52002151621c8ab589d21bbd5c11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "514aeb59-695b-4304-b7ed-e3856e745009", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "ffe2a37026be0bcaeccb3ed95af5cec8e2c3a6626a6422b5a75dd87e7542e5fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "248abee2-d68e-4f2b-8de4-ada1424bce93", "node_type": "1", "metadata": {}, "hash": "45fc42c037c2665768a9b998034596ff5bfcc8e3fdf5e84bbd79fcd7bb42bd22", "class_name": "RelatedNodeInfo"}}, "text": "While this\nis longer than the default 10-token pre\ufb01x used by\nLi and Liang (2021), our method still uses fewer\ntask-speci\ufb01c parameters, as we only tune the input\nlayer, as opposed to overwriting activations in all\nnetwork layers. See Figure 4 for a detailed com-\nparison. We will also see shortly that even much\nshorter prompts are viable as model size increases.\nWe measure performance on the SuperGLUE\nbenchmark (Wang et al., 2019a), a collection of\neight challenging English language understanding\ntasks.2We report metrics on the development set\nassociated with each dataset.\nEach of our prompts train on a single Super-\nGLUE task; there was no multi-task setup or mix-\ning of training data across tasks. We translate each\nSuperGLUE dataset into a text-to-text format fol-\nlowing Raffel et al. (2020), except that we omit the\ntask names prepended to inputs indicating which\nSuperGLUE task an example belongs to.", "start_char_idx": 972, "end_char_idx": 1885, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "248abee2-d68e-4f2b-8de4-ada1424bce93": {"__data__": {"id_": "248abee2-d68e-4f2b-8de4-ada1424bce93", "embedding": null, "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "912952e8dcf723d716d4fce59f8fa980aa1e52002151621c8ab589d21bbd5c11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da15dbb1-b0be-4c94-9127-d8080dc4e416", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "61bc8cb923e79c65df19d68a250456a5d734d38d842b9e16a61dd705037dcbbd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "178f6a48-9367-44b2-a33e-049035c423fb", "node_type": "1", "metadata": {}, "hash": "a0141332e858369dce25f76c8827a84b7941be56b02885aacda7d9b34cbdc157", "class_name": "RelatedNodeInfo"}}, "text": "See Figure 4 for a detailed com-\nparison. We will also see shortly that even much\nshorter prompts are viable as model size increases.\nWe measure performance on the SuperGLUE\nbenchmark (Wang et al., 2019a), a collection of\neight challenging English language understanding\ntasks.2We report metrics on the development set\nassociated with each dataset.\nEach of our prompts train on a single Super-\nGLUE task; there was no multi-task setup or mix-\ning of training data across tasks. We translate each\nSuperGLUE dataset into a text-to-text format fol-\nlowing Raffel et al. (2020), except that we omit the\ntask names prepended to inputs indicating which\nSuperGLUE task an example belongs to.\nWe train our prompts for 30,000steps using T5\u2019s\nstandard cross-entropy loss, with a constant learn-\n1These improvements are (1) the removal of all supervised\ndata from pre-training, (2) adjustments to hyperparameters\ndmodel anddff, and (3) the use of GeGLU (Shazeer, 2020)\nover ReLU (Nair and Hinton, 2010) activations.", "start_char_idx": 1201, "end_char_idx": 2205, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "178f6a48-9367-44b2-a33e-049035c423fb": {"__data__": {"id_": "178f6a48-9367-44b2-a33e-049035c423fb", "embedding": null, "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "912952e8dcf723d716d4fce59f8fa980aa1e52002151621c8ab589d21bbd5c11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "248abee2-d68e-4f2b-8de4-ada1424bce93", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "5de0de7fc37202f99ece39a7404db27d901c7690b94daec17ab9de5b0f783ba6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4428b4dd-66e9-42b0-baf1-1bc6a25a8080", "node_type": "1", "metadata": {}, "hash": "772a2096049ebf732f25e2044ee9b9a475a217d11a4d346e61c4322554e33c6c", "class_name": "RelatedNodeInfo"}}, "text": "Each of our prompts train on a single Super-\nGLUE task; there was no multi-task setup or mix-\ning of training data across tasks. We translate each\nSuperGLUE dataset into a text-to-text format fol-\nlowing Raffel et al. (2020), except that we omit the\ntask names prepended to inputs indicating which\nSuperGLUE task an example belongs to.\nWe train our prompts for 30,000steps using T5\u2019s\nstandard cross-entropy loss, with a constant learn-\n1These improvements are (1) the removal of all supervised\ndata from pre-training, (2) adjustments to hyperparameters\ndmodel anddff, and (3) the use of GeGLU (Shazeer, 2020)\nover ReLU (Nair and Hinton, 2010) activations.\n2The tasks are BoolQ (Clark et al., 2019), CB (De Marn-\neff et al., 2019), COPA (Roemmele et al., 2011), MultiRC\n(Khashabi et al., 2018), ReCoRD (Zhang et al., 2018), RTE\n(Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al.,\n2007; Bentivogli et al., 2009), WiC (Pilehvar and Camacho-\nCollados, 2018), and WSC (Levesque et al., 2012).ing rate of 0.3and a batch size of 32.", "start_char_idx": 1550, "end_char_idx": 2587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4428b4dd-66e9-42b0-baf1-1bc6a25a8080": {"__data__": {"id_": "4428b4dd-66e9-42b0-baf1-1bc6a25a8080", "embedding": null, "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "912952e8dcf723d716d4fce59f8fa980aa1e52002151621c8ab589d21bbd5c11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "178f6a48-9367-44b2-a33e-049035c423fb", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "346bff67e0753f599473aa3e88f4de883d008389f7018795a0f300937e117094", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1bf8130-a00d-48b4-81e3-6a53b4fd579a", "node_type": "1", "metadata": {}, "hash": "775be0989ceb4c4400579aecac2b8ff2558bcaa46c18dbf670bd06ace93f5691", "class_name": "RelatedNodeInfo"}}, "text": "2The tasks are BoolQ (Clark et al., 2019), CB (De Marn-\neff et al., 2019), COPA (Roemmele et al., 2011), MultiRC\n(Khashabi et al., 2018), ReCoRD (Zhang et al., 2018), RTE\n(Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al.,\n2007; Bentivogli et al., 2009), WiC (Pilehvar and Camacho-\nCollados, 2018), and WSC (Levesque et al., 2012).ing rate of 0.3and a batch size of 32. Checkpoints\nare selected via early stopping on the development\nset, where the stopping metric is the default met-\nric for the dataset, or the average of metrics for\ndatasets evaluated with multiple metrics. All ex-\nperiments were run in JAX (Bradbury et al., 2018)\nusing the Adafactor optimizer (Shazeer and Stern,\n2018) with weight decay 1e\u22125,\u03b22decay 0.8, and\nparameter scaling off.", "start_char_idx": 2206, "end_char_idx": 2971, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1bf8130-a00d-48b4-81e3-6a53b4fd579a": {"__data__": {"id_": "e1bf8130-a00d-48b4-81e3-6a53b4fd579a", "embedding": null, "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "912952e8dcf723d716d4fce59f8fa980aa1e52002151621c8ab589d21bbd5c11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4428b4dd-66e9-42b0-baf1-1bc6a25a8080", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "9e665ac86fb0ad6f667a50d2891f24d7fcae918dd500a134fbc928593a148913", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc7234a0-cb8b-4648-804c-fbeb6daf4489", "node_type": "1", "metadata": {}, "hash": "613243067ae8bc34ae58ce4262d4125a3a84ef556319554fe23eb131de4434a2", "class_name": "RelatedNodeInfo"}}, "text": "Checkpoints\nare selected via early stopping on the development\nset, where the stopping metric is the default met-\nric for the dataset, or the average of metrics for\ndatasets evaluated with multiple metrics. All ex-\nperiments were run in JAX (Bradbury et al., 2018)\nusing the Adafactor optimizer (Shazeer and Stern,\n2018) with weight decay 1e\u22125,\u03b22decay 0.8, and\nparameter scaling off. The models were imple-\nmented in Flax (Heek et al., 2020). More details\nare available in Appendix A.\n3.1 Closing the Gap\nTo compare our method with standard model tun-\ning, we tune the public T5.1.1 checkpoints on\nSuperGLUE using the default hyperparameters\nspeci\ufb01ed in the T5 library (learning rate 0.001,\nand Adafactor optimizer with pre-training param-\neter states restored). We consider two baselines.", "start_char_idx": 2588, "end_char_idx": 3377, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc7234a0-cb8b-4648-804c-fbeb6daf4489": {"__data__": {"id_": "fc7234a0-cb8b-4648-804c-fbeb6daf4489", "embedding": null, "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "912952e8dcf723d716d4fce59f8fa980aa1e52002151621c8ab589d21bbd5c11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1bf8130-a00d-48b4-81e3-6a53b4fd579a", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "b1f9f123a0e094c6bf901457d0dc3a06efa523eac0a70c271db85c16f794ce15", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27dfb214-c52b-42e4-882a-b285da59f623", "node_type": "1", "metadata": {}, "hash": "8186205bc9ddd8a3eaa08f1825b0b94610ba31bdc595ddd069165538206667fe", "class_name": "RelatedNodeInfo"}}, "text": "All ex-\nperiments were run in JAX (Bradbury et al., 2018)\nusing the Adafactor optimizer (Shazeer and Stern,\n2018) with weight decay 1e\u22125,\u03b22decay 0.8, and\nparameter scaling off. The models were imple-\nmented in Flax (Heek et al., 2020). More details\nare available in Appendix A.\n3.1 Closing the Gap\nTo compare our method with standard model tun-\ning, we tune the public T5.1.1 checkpoints on\nSuperGLUE using the default hyperparameters\nspeci\ufb01ed in the T5 library (learning rate 0.001,\nand Adafactor optimizer with pre-training param-\neter states restored). We consider two baselines.\n(1) \u201cModel Tuning\u201d: For an apples-to-apples com-\nparison, we tune on each task separately, as in our\nprompt tuning setup.3(2) \u201cModel Tuning (Multi-\ntask)\u201d: We use T5\u2019s multi-task tuning setup to\nachieve a more competitive baseline.4In this case,\na single model is tuned on all tasks jointly, with a\ntext pre\ufb01x indicating the task name.", "start_char_idx": 2795, "end_char_idx": 3713, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27dfb214-c52b-42e4-882a-b285da59f623": {"__data__": {"id_": "27dfb214-c52b-42e4-882a-b285da59f623", "embedding": null, "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "912952e8dcf723d716d4fce59f8fa980aa1e52002151621c8ab589d21bbd5c11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc7234a0-cb8b-4648-804c-fbeb6daf4489", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "0935f4d57dae15961b9ecd696af2f457cbd6d5b5457831286b5e16c15ac783cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5dd264e-ae72-4dd0-83d8-1cfa9ce6dbc5", "node_type": "1", "metadata": {}, "hash": "bdeaab0159218a7b030b363f5c9a99ff503603677e7387261624ffccf0584f06", "class_name": "RelatedNodeInfo"}}, "text": "More details\nare available in Appendix A.\n3.1 Closing the Gap\nTo compare our method with standard model tun-\ning, we tune the public T5.1.1 checkpoints on\nSuperGLUE using the default hyperparameters\nspeci\ufb01ed in the T5 library (learning rate 0.001,\nand Adafactor optimizer with pre-training param-\neter states restored). We consider two baselines.\n(1) \u201cModel Tuning\u201d: For an apples-to-apples com-\nparison, we tune on each task separately, as in our\nprompt tuning setup.3(2) \u201cModel Tuning (Multi-\ntask)\u201d: We use T5\u2019s multi-task tuning setup to\nachieve a more competitive baseline.4In this case,\na single model is tuned on all tasks jointly, with a\ntext pre\ufb01x indicating the task name.\nIn Figure 1 (p. 1), we see that prompt tuning\nbecomes more competitive with model tuning as\nscale increases. At the XXL size (11 billion param-\neters), prompt tuning matches even the stronger\nmulti-task model tuning baseline, despite having\nover20,000times fewer task-speci\ufb01c parameters.", "start_char_idx": 3031, "end_char_idx": 4001, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5dd264e-ae72-4dd0-83d8-1cfa9ce6dbc5": {"__data__": {"id_": "a5dd264e-ae72-4dd0-83d8-1cfa9ce6dbc5", "embedding": null, "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "912952e8dcf723d716d4fce59f8fa980aa1e52002151621c8ab589d21bbd5c11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27dfb214-c52b-42e4-882a-b285da59f623", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d7a65ca96024531ff462e14670d55645509261ede013f614931e18c649e0ed12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e371f207-1421-4299-81ac-26503a01c555", "node_type": "1", "metadata": {}, "hash": "abe228975f1b0adf2b51aa581e8491d7b239226aa1af5144933c44c5926b19fc", "class_name": "RelatedNodeInfo"}}, "text": "We consider two baselines.\n(1) \u201cModel Tuning\u201d: For an apples-to-apples com-\nparison, we tune on each task separately, as in our\nprompt tuning setup.3(2) \u201cModel Tuning (Multi-\ntask)\u201d: We use T5\u2019s multi-task tuning setup to\nachieve a more competitive baseline.4In this case,\na single model is tuned on all tasks jointly, with a\ntext pre\ufb01x indicating the task name.\nIn Figure 1 (p. 1), we see that prompt tuning\nbecomes more competitive with model tuning as\nscale increases. At the XXL size (11 billion param-\neters), prompt tuning matches even the stronger\nmulti-task model tuning baseline, despite having\nover20,000times fewer task-speci\ufb01c parameters.\nTo compare with prompt design, we include\nGPT-3 few-shot performance on the SuperGLUE\ndev split, as reported by Brown et al.", "start_char_idx": 3351, "end_char_idx": 4126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e371f207-1421-4299-81ac-26503a01c555": {"__data__": {"id_": "e371f207-1421-4299-81ac-26503a01c555", "embedding": null, "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "912952e8dcf723d716d4fce59f8fa980aa1e52002151621c8ab589d21bbd5c11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5dd264e-ae72-4dd0-83d8-1cfa9ce6dbc5", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "25360a8047cb2bfbeee2e5a692a9401407d104b81114c00227dbee7e7b0a1005", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31a72be0-7df7-43a2-b84e-cc1979ea92dc", "node_type": "1", "metadata": {}, "hash": "88bf30930f868bda94a21c3089fd21a126e4a05b26a77ad1d669e5b21ff3ca76", "class_name": "RelatedNodeInfo"}}, "text": "(1) \u201cModel Tuning\u201d: For an apples-to-apples com-\nparison, we tune on each task separately, as in our\nprompt tuning setup.3(2) \u201cModel Tuning (Multi-\ntask)\u201d: We use T5\u2019s multi-task tuning setup to\nachieve a more competitive baseline.4In this case,\na single model is tuned on all tasks jointly, with a\ntext pre\ufb01x indicating the task name.\nIn Figure 1 (p. 1), we see that prompt tuning\nbecomes more competitive with model tuning as\nscale increases. At the XXL size (11 billion param-\neters), prompt tuning matches even the stronger\nmulti-task model tuning baseline, despite having\nover20,000times fewer task-speci\ufb01c parameters.\nTo compare with prompt design, we include\nGPT-3 few-shot performance on the SuperGLUE\ndev split, as reported by Brown et al. (2020).5\nFigure 1 shows that prompt tuning beats GPT-3\nprompt design by a large margin, with prompt-\ntuned T5-Small matching GPT-3 XL (over 16\ntimes larger), and prompt-tuned T5-Large beating\nGPT-3 175B (over 220times larger).", "start_char_idx": 3378, "end_char_idx": 4353, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31a72be0-7df7-43a2-b84e-cc1979ea92dc": {"__data__": {"id_": "31a72be0-7df7-43a2-b84e-cc1979ea92dc", "embedding": null, "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "912952e8dcf723d716d4fce59f8fa980aa1e52002151621c8ab589d21bbd5c11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e371f207-1421-4299-81ac-26503a01c555", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "54102d8a5deab35b542dc66e03a7b3f0932a2f6a75fc61940eebdd2a40c736e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d51c778-a178-4e67-b40b-99059cd9f1e8", "node_type": "1", "metadata": {}, "hash": "a777dbdb342aaab1471fc84725bc64b9f7191d1f647e351ab26d858b80e82073", "class_name": "RelatedNodeInfo"}}, "text": "In Figure 1 (p. 1), we see that prompt tuning\nbecomes more competitive with model tuning as\nscale increases. At the XXL size (11 billion param-\neters), prompt tuning matches even the stronger\nmulti-task model tuning baseline, despite having\nover20,000times fewer task-speci\ufb01c parameters.\nTo compare with prompt design, we include\nGPT-3 few-shot performance on the SuperGLUE\ndev split, as reported by Brown et al. (2020).5\nFigure 1 shows that prompt tuning beats GPT-3\nprompt design by a large margin, with prompt-\ntuned T5-Small matching GPT-3 XL (over 16\ntimes larger), and prompt-tuned T5-Large beating\nGPT-3 175B (over 220times larger).\n3To improve this baseline, we performed a sweep over the\nbatch size hyperparameter and selected 216tokens per batch.\n4The T5 SuperGLUE submission used a more complex\nsetup, \ufb01rst mixing multi-task supervised data into pre-training,\nand then performing single-task \ufb01ne-tuning.", "start_char_idx": 3714, "end_char_idx": 4628, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d51c778-a178-4e67-b40b-99059cd9f1e8": {"__data__": {"id_": "2d51c778-a178-4e67-b40b-99059cd9f1e8", "embedding": null, "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "912952e8dcf723d716d4fce59f8fa980aa1e52002151621c8ab589d21bbd5c11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31a72be0-7df7-43a2-b84e-cc1979ea92dc", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "702d8d482688d3a925b9b61b84812dde6c3f18351e3e10882c457d7be827f9d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2db946e8-2f2c-45df-90dd-549c66cb2764", "node_type": "1", "metadata": {}, "hash": "00f3312da44d21640ca3f5ce62c611295ed36a759110435da28dd8a9652d3d3a", "class_name": "RelatedNodeInfo"}}, "text": "To compare with prompt design, we include\nGPT-3 few-shot performance on the SuperGLUE\ndev split, as reported by Brown et al. (2020).5\nFigure 1 shows that prompt tuning beats GPT-3\nprompt design by a large margin, with prompt-\ntuned T5-Small matching GPT-3 XL (over 16\ntimes larger), and prompt-tuned T5-Large beating\nGPT-3 175B (over 220times larger).\n3To improve this baseline, we performed a sweep over the\nbatch size hyperparameter and selected 216tokens per batch.\n4The T5 SuperGLUE submission used a more complex\nsetup, \ufb01rst mixing multi-task supervised data into pre-training,\nand then performing single-task \ufb01ne-tuning. Since we use\nT5.1.1 throughout, this setup is unavailable, as the pre-training\nphase is fully self-supervised. We follow Raffel et al. (2020)\nin using 220tokens per batch and including DPR data in\nthe multi-task mixture, which is known to boost WSC task\nperformance (Kocijan et al., 2019).", "start_char_idx": 4002, "end_char_idx": 4918, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2db946e8-2f2c-45df-90dd-549c66cb2764": {"__data__": {"id_": "2db946e8-2f2c-45df-90dd-549c66cb2764", "embedding": null, "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f47bd05-4993-421c-a4de-1d820c1ecea4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "912952e8dcf723d716d4fce59f8fa980aa1e52002151621c8ab589d21bbd5c11", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d51c778-a178-4e67-b40b-99059cd9f1e8", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "732cdf19ea01770e0e2099ad0dd28cd249d0821ea134ff76ff57840a734ccb6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b56ecdb-6a66-4781-bb4e-4e01f4bec81c", "node_type": "1", "metadata": {}, "hash": "6416c4948c03a8f67eb5e2195863dce02f75da129fdc898a0a443c9faa4b2baa", "class_name": "RelatedNodeInfo"}}, "text": "3To improve this baseline, we performed a sweep over the\nbatch size hyperparameter and selected 216tokens per batch.\n4The T5 SuperGLUE submission used a more complex\nsetup, \ufb01rst mixing multi-task supervised data into pre-training,\nand then performing single-task \ufb01ne-tuning. Since we use\nT5.1.1 throughout, this setup is unavailable, as the pre-training\nphase is fully self-supervised. We follow Raffel et al. (2020)\nin using 220tokens per batch and including DPR data in\nthe multi-task mixture, which is known to boost WSC task\nperformance (Kocijan et al., 2019).\n5We also experimented with using GPT-3\u2019s manual text\nprompts directly with our LM-adapted T5 checkpoints. How-\never performance was far below GPT-3 for comparable model\nsizes. This may be due to differences in pre-training data and\nmodel architecture, as well as T5\u2019s shorter sequence length.", "start_char_idx": 4354, "end_char_idx": 5211, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b56ecdb-6a66-4781-bb4e-4e01f4bec81c": {"__data__": {"id_": "0b56ecdb-6a66-4781-bb4e-4e01f4bec81c", "embedding": null, "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "504434b7-d7ac-44ee-b82f-d66314dc879f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "69eba67d3fcec2cb381aff2d47aea3fcf7dacb148b6e2878bbc4a68b27aaea75", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2db946e8-2f2c-45df-90dd-549c66cb2764", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "155b0b36b9a117bd23e601dbfaae027df6e7eaff5b64978a364afe448475dc30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4678b6aa-f137-4bf1-834d-8afc2add4a16", "node_type": "1", "metadata": {}, "hash": "38636514c51586f56729b9f8eb938e7f8b7fd4efc10c6e8800c1e0bbc2d35ed4", "class_name": "RelatedNodeInfo"}}, "text": "1081091010\nModel Parameters5060708090100SuperGLUE Score\n1\n5\n20\n100\n150(a) Prompt length\n1081091010\nModel Parameters5060708090100SuperGLUE Score\nRandom Uniform\nSampled Vocab\nClass Label (b) Prompt initialization\n1081091010\nModel Parameters102030405060708090100SuperGLUE Score\nSpan Corruption\nSpan Corruption\n+ Sentinel\nLM Adaptation\n(100K)\n(c) Pre-training method\n1081091010\nModel Parameters102030405060708090100SuperGLUE Score\n0K\n10K\n50K\n100K (d) LM adaptation steps\nFigure 3: Ablations of various hyperparameters on prompt tuning performance (mean and stddev across 3runs). In\nour \u201cdefault\u201d (\n ) con\ufb01guration, quality improves stably with model size. Across all ablations, the largest (XXL)\nmodel is the most robust to hyperparameter choice . (a) Prompt length : Increasing to 20+ tokens generally confers\na large boost, but XXL performs well even with single-token prompts.", "start_char_idx": 0, "end_char_idx": 875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4678b6aa-f137-4bf1-834d-8afc2add4a16": {"__data__": {"id_": "4678b6aa-f137-4bf1-834d-8afc2add4a16", "embedding": null, "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "504434b7-d7ac-44ee-b82f-d66314dc879f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "69eba67d3fcec2cb381aff2d47aea3fcf7dacb148b6e2878bbc4a68b27aaea75", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b56ecdb-6a66-4781-bb4e-4e01f4bec81c", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "26a364ad4ec1a36a08072c7305f3d9e4efa8c97b548a41225bc685af3de731af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8d5c18f-d5b8-43b0-a3e7-9f5f01b2a177", "node_type": "1", "metadata": {}, "hash": "7199d7cbf992b9720d52286272296d6499361110c371489d0edab98704f71b69", "class_name": "RelatedNodeInfo"}}, "text": "In\nour \u201cdefault\u201d (\n ) con\ufb01guration, quality improves stably with model size. Across all ablations, the largest (XXL)\nmodel is the most robust to hyperparameter choice . (a) Prompt length : Increasing to 20+ tokens generally confers\na large boost, but XXL performs well even with single-token prompts. (b) Prompt initialization : Random uniform\ninitialization lags behind more \u201cadvanced\u201d initializations using sampled vocabulary or class label embeddings, but\nthe difference vanishes at XXL size. (c) Pre-training objective : LM adaptation outperforms span corruption, even\nwhen a sentinel is added to downstream task targets, but XXL works well with any method. (d) LM adaptation :\nLonger adaptation generally gives larger gains, but XXL is robust to even short adaptation.\n3.2 Ablation Study\nPrompt Length We train prompts for each\nmodel size while varying the prompt length in\n{1,5,20,100,150}and \ufb01xing other settings to our\ndefault con\ufb01guration. Figure 3(a) shows that for\nmost model sizes, increasing prompt length beyond\na single token is critical to achieve good perfor-\nmance.", "start_char_idx": 575, "end_char_idx": 1658, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8d5c18f-d5b8-43b0-a3e7-9f5f01b2a177": {"__data__": {"id_": "f8d5c18f-d5b8-43b0-a3e7-9f5f01b2a177", "embedding": null, "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "504434b7-d7ac-44ee-b82f-d66314dc879f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "69eba67d3fcec2cb381aff2d47aea3fcf7dacb148b6e2878bbc4a68b27aaea75", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4678b6aa-f137-4bf1-834d-8afc2add4a16", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "be36ec16b99db1efee241e754461d396a1d3df08d732a9af209cf117744997a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c92efd4-ba54-499c-8f74-383c7f0bf8f4", "node_type": "1", "metadata": {}, "hash": "c8c25f786480a22a3b2d557c6af893484c52e5def5f4c0c81a4a26a118435dcd", "class_name": "RelatedNodeInfo"}}, "text": "(b) Prompt initialization : Random uniform\ninitialization lags behind more \u201cadvanced\u201d initializations using sampled vocabulary or class label embeddings, but\nthe difference vanishes at XXL size. (c) Pre-training objective : LM adaptation outperforms span corruption, even\nwhen a sentinel is added to downstream task targets, but XXL works well with any method. (d) LM adaptation :\nLonger adaptation generally gives larger gains, but XXL is robust to even short adaptation.\n3.2 Ablation Study\nPrompt Length We train prompts for each\nmodel size while varying the prompt length in\n{1,5,20,100,150}and \ufb01xing other settings to our\ndefault con\ufb01guration. Figure 3(a) shows that for\nmost model sizes, increasing prompt length beyond\na single token is critical to achieve good perfor-\nmance. Notably, the XXL model still gives strong\nresults with a single-token prompt, suggesting that\nthe larger the model, the less conditioning signal\nis needed to achieve a target behavior.", "start_char_idx": 876, "end_char_idx": 1843, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c92efd4-ba54-499c-8f74-383c7f0bf8f4": {"__data__": {"id_": "7c92efd4-ba54-499c-8f74-383c7f0bf8f4", "embedding": null, "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "504434b7-d7ac-44ee-b82f-d66314dc879f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "69eba67d3fcec2cb381aff2d47aea3fcf7dacb148b6e2878bbc4a68b27aaea75", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8d5c18f-d5b8-43b0-a3e7-9f5f01b2a177", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "ad3a2443b2d98451e9cfeb3a0ec28718faaba73ebff72ca9f7999afcc39bb921", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "410c0ab8-7f89-4670-bdc0-c166c1f44715", "node_type": "1", "metadata": {}, "hash": "03b65767f94e8e75b557ae2ce8231c7d910bbd3de27865a8a39f7029e8174d8a", "class_name": "RelatedNodeInfo"}}, "text": "(c) Pre-training objective : LM adaptation outperforms span corruption, even\nwhen a sentinel is added to downstream task targets, but XXL works well with any method. (d) LM adaptation :\nLonger adaptation generally gives larger gains, but XXL is robust to even short adaptation.\n3.2 Ablation Study\nPrompt Length We train prompts for each\nmodel size while varying the prompt length in\n{1,5,20,100,150}and \ufb01xing other settings to our\ndefault con\ufb01guration. Figure 3(a) shows that for\nmost model sizes, increasing prompt length beyond\na single token is critical to achieve good perfor-\nmance. Notably, the XXL model still gives strong\nresults with a single-token prompt, suggesting that\nthe larger the model, the less conditioning signal\nis needed to achieve a target behavior. Across all\nmodels, increasing beyond 20tokens only yields\nmarginal gains.6\nPrompt Initialization We ablate the effect of\nprompt initialization by training models at all sizes\nwhile \ufb01xing other hyperparameters to their default\nvalues. For random initialization, we sample uni-\n6Going past 100 tokens appears mildly detrimental for\nlarger models.", "start_char_idx": 1071, "end_char_idx": 2188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "410c0ab8-7f89-4670-bdc0-c166c1f44715": {"__data__": {"id_": "410c0ab8-7f89-4670-bdc0-c166c1f44715", "embedding": null, "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "504434b7-d7ac-44ee-b82f-d66314dc879f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "69eba67d3fcec2cb381aff2d47aea3fcf7dacb148b6e2878bbc4a68b27aaea75", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c92efd4-ba54-499c-8f74-383c7f0bf8f4", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "98554ff97ed56c995bbfa8b4d609c3e6794cb257ef4616d20962ea4ee3ee378f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c30f3b3e-c08b-4991-8127-a8315df260b6", "node_type": "1", "metadata": {}, "hash": "3a307184ff83b2992d321762e29c8b913d7e66a9c2c99c6e898fcc0daba76dcc", "class_name": "RelatedNodeInfo"}}, "text": "Figure 3(a) shows that for\nmost model sizes, increasing prompt length beyond\na single token is critical to achieve good perfor-\nmance. Notably, the XXL model still gives strong\nresults with a single-token prompt, suggesting that\nthe larger the model, the less conditioning signal\nis needed to achieve a target behavior. Across all\nmodels, increasing beyond 20tokens only yields\nmarginal gains.6\nPrompt Initialization We ablate the effect of\nprompt initialization by training models at all sizes\nwhile \ufb01xing other hyperparameters to their default\nvalues. For random initialization, we sample uni-\n6Going past 100 tokens appears mildly detrimental for\nlarger models. A similar pattern of diminishing performance\npast a certain pre\ufb01x length is observed by Li and Liang (2021).formly from the range [ \u22120.5,0.5]. When initial-\nizing from sampled vocabulary, we restrict to the\n5,000most \u201ccommon\u201d tokens in T5\u2019s Sentence-\nPiece vocabulary (Kudo and Richardson, 2018),\nwhich is ordered by likelihood in the pre-training\ncorpus.", "start_char_idx": 1524, "end_char_idx": 2544, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c30f3b3e-c08b-4991-8127-a8315df260b6": {"__data__": {"id_": "c30f3b3e-c08b-4991-8127-a8315df260b6", "embedding": null, "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "504434b7-d7ac-44ee-b82f-d66314dc879f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "69eba67d3fcec2cb381aff2d47aea3fcf7dacb148b6e2878bbc4a68b27aaea75", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "410c0ab8-7f89-4670-bdc0-c166c1f44715", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "29e3b41fe6d958b7df279241332d0d8db1e9d0db253b542f710239705709527d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "edfaaeec-115d-4194-8233-7b72f4f85843", "node_type": "1", "metadata": {}, "hash": "84a109ecb8b8605f75c761e40cc4e5df5716403de83b397aa1bccffce689ba8f", "class_name": "RelatedNodeInfo"}}, "text": "Across all\nmodels, increasing beyond 20tokens only yields\nmarginal gains.6\nPrompt Initialization We ablate the effect of\nprompt initialization by training models at all sizes\nwhile \ufb01xing other hyperparameters to their default\nvalues. For random initialization, we sample uni-\n6Going past 100 tokens appears mildly detrimental for\nlarger models. A similar pattern of diminishing performance\npast a certain pre\ufb01x length is observed by Li and Liang (2021).formly from the range [ \u22120.5,0.5]. When initial-\nizing from sampled vocabulary, we restrict to the\n5,000most \u201ccommon\u201d tokens in T5\u2019s Sentence-\nPiece vocabulary (Kudo and Richardson, 2018),\nwhich is ordered by likelihood in the pre-training\ncorpus. For \u201cclass label\u201d initialization, we take\nthe embeddings for the string representations of\neach class in the downstream task and use them to\ninitialize one of the tokens in the prompt. When\na class label is multi-token, we average the token\nembeddings. At longer prompt lengths, we often\nrun out of class labels before we have initialized all\nof the prompt tokens.", "start_char_idx": 1844, "end_char_idx": 2909, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "edfaaeec-115d-4194-8233-7b72f4f85843": {"__data__": {"id_": "edfaaeec-115d-4194-8233-7b72f4f85843", "embedding": null, "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "504434b7-d7ac-44ee-b82f-d66314dc879f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "69eba67d3fcec2cb381aff2d47aea3fcf7dacb148b6e2878bbc4a68b27aaea75", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c30f3b3e-c08b-4991-8127-a8315df260b6", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "733c2877498b26b7bb8f470f80af2bc4ec77c1cfe4d6e64c63a3d66164edaf07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a3a5af5-513e-4f79-8372-7c640f019673", "node_type": "1", "metadata": {}, "hash": "ea47781c268fc9823ded791e504ccd2a10ab06905db640badc65490dc807c8c7", "class_name": "RelatedNodeInfo"}}, "text": "A similar pattern of diminishing performance\npast a certain pre\ufb01x length is observed by Li and Liang (2021).formly from the range [ \u22120.5,0.5]. When initial-\nizing from sampled vocabulary, we restrict to the\n5,000most \u201ccommon\u201d tokens in T5\u2019s Sentence-\nPiece vocabulary (Kudo and Richardson, 2018),\nwhich is ordered by likelihood in the pre-training\ncorpus. For \u201cclass label\u201d initialization, we take\nthe embeddings for the string representations of\neach class in the downstream task and use them to\ninitialize one of the tokens in the prompt. When\na class label is multi-token, we average the token\nembeddings. At longer prompt lengths, we often\nrun out of class labels before we have initialized all\nof the prompt tokens. In this case we fall back to\nour sampled vocab strategy to \ufb01ll in the prompt.7\nFigure 3(b) shows our ablation of initialization\nstrategy across model sizes, where we \ufb01nd that\n7T5\u2019s handling of the ReCoRD and WSC tasks requires\nthe model to generate short, free-form text.", "start_char_idx": 2189, "end_char_idx": 3181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a3a5af5-513e-4f79-8372-7c640f019673": {"__data__": {"id_": "7a3a5af5-513e-4f79-8372-7c640f019673", "embedding": null, "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "504434b7-d7ac-44ee-b82f-d66314dc879f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "69eba67d3fcec2cb381aff2d47aea3fcf7dacb148b6e2878bbc4a68b27aaea75", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "edfaaeec-115d-4194-8233-7b72f4f85843", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "1426e7d0b86e616c7dbc297b12dae796557ece5ae7a1369b95133b66ac45e1a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6cb09ba-958d-4be7-bbcb-41ed6e52f166", "node_type": "1", "metadata": {}, "hash": "486f44a965d664284fdcdbdd37fb0759f168b1b358d80bf82b606f6cb5b6908a", "class_name": "RelatedNodeInfo"}}, "text": "For \u201cclass label\u201d initialization, we take\nthe embeddings for the string representations of\neach class in the downstream task and use them to\ninitialize one of the tokens in the prompt. When\na class label is multi-token, we average the token\nembeddings. At longer prompt lengths, we often\nrun out of class labels before we have initialized all\nof the prompt tokens. In this case we fall back to\nour sampled vocab strategy to \ufb01ll in the prompt.7\nFigure 3(b) shows our ablation of initialization\nstrategy across model sizes, where we \ufb01nd that\n7T5\u2019s handling of the ReCoRD and WSC tasks requires\nthe model to generate short, free-form text. In these cases, we\ninitialize the prompts with words related to the task: common-\nsense ,reasoning ,reading , and comprehension for ReCoRD\nandcommonsense ,pronoun , and resolution for WSC.", "start_char_idx": 2545, "end_char_idx": 3370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6cb09ba-958d-4be7-bbcb-41ed6e52f166": {"__data__": {"id_": "f6cb09ba-958d-4be7-bbcb-41ed6e52f166", "embedding": null, "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4d4bd45d6c96facb0b607dfad2fd0a2f058b3356811b4734a5825ac1bceb8225", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a3a5af5-513e-4f79-8372-7c640f019673", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "15e239d23a0b216d7f465f489c5663362a9263512a43879d4a27fc5d21ee81f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfc3015e-c7ec-414b-9535-2fd4f42bfd90", "node_type": "1", "metadata": {}, "hash": "9a5463e13e2190cea0dfe54a9005e430d62b5aca02822dab3b5febfdb4cf98d1", "class_name": "RelatedNodeInfo"}}, "text": "the class based initialization performs best. At\nsmaller model sizes, there are large gaps between\nthe different initializations, but once the model is\nscaled to XXL size, those differences disappear.\nWith \u201cclass label\u201d initialization, we observe that\nthe class labels typically persist in the learned\nprompts, such that the nearest token embeddings\n(in cosine distance) match the tokens used for ini-\ntialization. Beyond this, we did not \ufb01nd our learned\nprompts to be interpretable, similar to those of Shin\net al. (2020). See Section 7 for details.\nPre-training Objective In Figures 3(c) and 3(d),\nwe see pre-training objective has a clear effect on\nprompt tuning quality. As hypothesized in Sec-\ntion 2.2, T5\u2019s default \u201cspan corruption\u201d objective\nis not well-suited for training frozen models to be\nlater conditioned by prompts. Intuitively, models\npre-trained to read and write sentinel tokens are\nhard to apply directly to tasks of reading and writ-\ning text without sentinels. As seen in Figure 3(c),\neven the \u201cworkaround\u201d of adding a sentinel to the\ndownstream targets has little bene\ufb01t.", "start_char_idx": 0, "end_char_idx": 1094, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfc3015e-c7ec-414b-9535-2fd4f42bfd90": {"__data__": {"id_": "cfc3015e-c7ec-414b-9535-2fd4f42bfd90", "embedding": null, "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4d4bd45d6c96facb0b607dfad2fd0a2f058b3356811b4734a5825ac1bceb8225", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6cb09ba-958d-4be7-bbcb-41ed6e52f166", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "91347e9122dedf3f36b3f1493f807c7a341f120ba17fed5d4b2667f9836710bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a985fc9-8ec1-49ce-8382-c2517ca4fba4", "node_type": "1", "metadata": {}, "hash": "dbfaa8a97a4589696aca2e2f5af85068d9cb3f45bde89342212ba3c7180268d8", "class_name": "RelatedNodeInfo"}}, "text": "Beyond this, we did not \ufb01nd our learned\nprompts to be interpretable, similar to those of Shin\net al. (2020). See Section 7 for details.\nPre-training Objective In Figures 3(c) and 3(d),\nwe see pre-training objective has a clear effect on\nprompt tuning quality. As hypothesized in Sec-\ntion 2.2, T5\u2019s default \u201cspan corruption\u201d objective\nis not well-suited for training frozen models to be\nlater conditioned by prompts. Intuitively, models\npre-trained to read and write sentinel tokens are\nhard to apply directly to tasks of reading and writ-\ning text without sentinels. As seen in Figure 3(c),\neven the \u201cworkaround\u201d of adding a sentinel to the\ndownstream targets has little bene\ufb01t. While LM\nadaptation adds value across all model sizes, we\nnote our largest XXL model is the most forgiving\nand gives strong results even with span corruption.\nGiven the bene\ufb01t of LM adaptation, we also\nexplore how long of an adaptation is helpful. Fig-\nure 3(d) shows that longer adaptation provides ad-\nditional gains, up to 100K steps.", "start_char_idx": 415, "end_char_idx": 1432, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a985fc9-8ec1-49ce-8382-c2517ca4fba4": {"__data__": {"id_": "3a985fc9-8ec1-49ce-8382-c2517ca4fba4", "embedding": null, "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4d4bd45d6c96facb0b607dfad2fd0a2f058b3356811b4734a5825ac1bceb8225", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfc3015e-c7ec-414b-9535-2fd4f42bfd90", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "8cad636fc41d323a4485c00c6e790ad5eed2f42a402af00716a5bb1920ffb67f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "271b6b37-0c1b-4b6e-ab63-0bc5ff989a1a", "node_type": "1", "metadata": {}, "hash": "6ec390b3e799aba15a4d56080a9841831b77bd1509e095952b6582a9f0021917", "class_name": "RelatedNodeInfo"}}, "text": "As hypothesized in Sec-\ntion 2.2, T5\u2019s default \u201cspan corruption\u201d objective\nis not well-suited for training frozen models to be\nlater conditioned by prompts. Intuitively, models\npre-trained to read and write sentinel tokens are\nhard to apply directly to tasks of reading and writ-\ning text without sentinels. As seen in Figure 3(c),\neven the \u201cworkaround\u201d of adding a sentinel to the\ndownstream targets has little bene\ufb01t. While LM\nadaptation adds value across all model sizes, we\nnote our largest XXL model is the most forgiving\nand gives strong results even with span corruption.\nGiven the bene\ufb01t of LM adaptation, we also\nexplore how long of an adaptation is helpful. Fig-\nure 3(d) shows that longer adaptation provides ad-\nditional gains, up to 100K steps. This suggests\nthat the \u201ctransition\u201d from span corruption to a lan-\nguage modeling objective is not a trivial change,\nand making an effective switch takes an investment\nof training resources ( 10% of the steps of the orig-\ninal T5 pre-training).", "start_char_idx": 675, "end_char_idx": 1677, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "271b6b37-0c1b-4b6e-ab63-0bc5ff989a1a": {"__data__": {"id_": "271b6b37-0c1b-4b6e-ab63-0bc5ff989a1a", "embedding": null, "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4d4bd45d6c96facb0b607dfad2fd0a2f058b3356811b4734a5825ac1bceb8225", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a985fc9-8ec1-49ce-8382-c2517ca4fba4", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e06a0754d6b26e7c5c34dcaff22ee253745f39e695bf545231f4325f1dc2922e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ff0eb82-2737-43af-9f43-651f258b8f51", "node_type": "1", "metadata": {}, "hash": "f8f0951886ff884b23108caa46e99a0d3c8b149f55b0c2437642bfe6ebd238d3", "class_name": "RelatedNodeInfo"}}, "text": "As seen in Figure 3(c),\neven the \u201cworkaround\u201d of adding a sentinel to the\ndownstream targets has little bene\ufb01t. While LM\nadaptation adds value across all model sizes, we\nnote our largest XXL model is the most forgiving\nand gives strong results even with span corruption.\nGiven the bene\ufb01t of LM adaptation, we also\nexplore how long of an adaptation is helpful. Fig-\nure 3(d) shows that longer adaptation provides ad-\nditional gains, up to 100K steps. This suggests\nthat the \u201ctransition\u201d from span corruption to a lan-\nguage modeling objective is not a trivial change,\nand making an effective switch takes an investment\nof training resources ( 10% of the steps of the orig-\ninal T5 pre-training). At the same time, as in our\nother ablations, we observe that the XXL model\nis robust to even non-ideal con\ufb01gurations. At this\nsize, the gains from adaptation are quite modest.\nIn the non-optimal \u201cspan corruption\u201d setting, we\nobserve instability across model sizes, with the\nSmall model outperforming the larger Base, Large,\nand XL models.", "start_char_idx": 983, "end_char_idx": 2016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ff0eb82-2737-43af-9f43-651f258b8f51": {"__data__": {"id_": "2ff0eb82-2737-43af-9f43-651f258b8f51", "embedding": null, "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4d4bd45d6c96facb0b607dfad2fd0a2f058b3356811b4734a5825ac1bceb8225", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "271b6b37-0c1b-4b6e-ab63-0bc5ff989a1a", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "025209a5cf64fb178172ffafe53a0089d01d9e60e70d4b14d2ab5c59b05b09ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f305f1e-513a-4372-8911-57d407d90cc9", "node_type": "1", "metadata": {}, "hash": "ae7c92165450afb153d7d485509991b6aaf31409227b55dc7b913127899c17f3", "class_name": "RelatedNodeInfo"}}, "text": "Given the bene\ufb01t of LM adaptation, we also\nexplore how long of an adaptation is helpful. Fig-\nure 3(d) shows that longer adaptation provides ad-\nditional gains, up to 100K steps. This suggests\nthat the \u201ctransition\u201d from span corruption to a lan-\nguage modeling objective is not a trivial change,\nand making an effective switch takes an investment\nof training resources ( 10% of the steps of the orig-\ninal T5 pre-training). At the same time, as in our\nother ablations, we observe that the XXL model\nis robust to even non-ideal con\ufb01gurations. At this\nsize, the gains from adaptation are quite modest.\nIn the non-optimal \u201cspan corruption\u201d setting, we\nobserve instability across model sizes, with the\nSmall model outperforming the larger Base, Large,\nand XL models. On inspection, we \ufb01nd that for\nmany tasks, these mid-sized models never learn to\noutput a legal class label and thus score 0%. The\ntwo most common error modes are copying sub-\nspans from the input and predicting an empty string.", "start_char_idx": 1254, "end_char_idx": 2245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f305f1e-513a-4372-8911-57d407d90cc9": {"__data__": {"id_": "1f305f1e-513a-4372-8911-57d407d90cc9", "embedding": null, "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4d4bd45d6c96facb0b607dfad2fd0a2f058b3356811b4734a5825ac1bceb8225", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ff0eb82-2737-43af-9f43-651f258b8f51", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "3d8e5caa5dfbb1008bbe5809ba67b3e6d940b1a767be5207c87c5fbed05feb66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66ebaa18-f6a4-4ce8-aeb3-066261e8ce81", "node_type": "1", "metadata": {}, "hash": "8bcaac48c884294335126c5248b2d8c0baa27f8a1b4e09568312de1c15a654ba", "class_name": "RelatedNodeInfo"}}, "text": "This suggests\nthat the \u201ctransition\u201d from span corruption to a lan-\nguage modeling objective is not a trivial change,\nand making an effective switch takes an investment\nof training resources ( 10% of the steps of the orig-\ninal T5 pre-training). At the same time, as in our\nother ablations, we observe that the XXL model\nis robust to even non-ideal con\ufb01gurations. At this\nsize, the gains from adaptation are quite modest.\nIn the non-optimal \u201cspan corruption\u201d setting, we\nobserve instability across model sizes, with the\nSmall model outperforming the larger Base, Large,\nand XL models. On inspection, we \ufb01nd that for\nmany tasks, these mid-sized models never learn to\noutput a legal class label and thus score 0%. The\ntwo most common error modes are copying sub-\nspans from the input and predicting an empty string.\nFurthermore, this poor performance is not due to\nrandom variance in prompt tuning, as we observe\nlow variance across 3runs for each size.", "start_char_idx": 1433, "end_char_idx": 2383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66ebaa18-f6a4-4ce8-aeb3-066261e8ce81": {"__data__": {"id_": "66ebaa18-f6a4-4ce8-aeb3-066261e8ce81", "embedding": null, "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4d4bd45d6c96facb0b607dfad2fd0a2f058b3356811b4734a5825ac1bceb8225", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f305f1e-513a-4372-8911-57d407d90cc9", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "1a9173d3106f7820fd2d0f8f4dbb322303b34778f7107171e3d7ae7b9146d45a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5c48b96-117e-411d-9fd8-b1c0866f4781", "node_type": "1", "metadata": {}, "hash": "d219dcbc21fb70b9bec8d88b3f6a24571ae0af504de305554785dd1691b7f43e", "class_name": "RelatedNodeInfo"}}, "text": "At the same time, as in our\nother ablations, we observe that the XXL model\nis robust to even non-ideal con\ufb01gurations. At this\nsize, the gains from adaptation are quite modest.\nIn the non-optimal \u201cspan corruption\u201d setting, we\nobserve instability across model sizes, with the\nSmall model outperforming the larger Base, Large,\nand XL models. On inspection, we \ufb01nd that for\nmany tasks, these mid-sized models never learn to\noutput a legal class label and thus score 0%. The\ntwo most common error modes are copying sub-\nspans from the input and predicting an empty string.\nFurthermore, this poor performance is not due to\nrandom variance in prompt tuning, as we observe\nlow variance across 3runs for each size. These\nresults indicate that using models pre-trained with\nthe \u201cspan corruption\u201d objective can be unreliable,\nwith only 2out of 5models working well, whereas\n1081091010\nModel Parameters103105107109Task Parameters\nModel Tuning\nPrefix Tuning (Train)\nPrefix Tuning (Infer)WARP\nPrompt Tuning\nPrompt Design\n0.001%0.01%0.1%1%10%100%Task Parameters (%)Figure 4: Parameter usage of various adaptation tech-\nniques, \ufb01xing architecture to T5.1.1 and prompt/pre\ufb01x\nlength to 1\u2013100tokens (bands show mean and stddev).", "start_char_idx": 1678, "end_char_idx": 2887, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5c48b96-117e-411d-9fd8-b1c0866f4781": {"__data__": {"id_": "c5c48b96-117e-411d-9fd8-b1c0866f4781", "embedding": null, "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4d4bd45d6c96facb0b607dfad2fd0a2f058b3356811b4734a5825ac1bceb8225", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66ebaa18-f6a4-4ce8-aeb3-066261e8ce81", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d52e269e2dbcd2b3fb91abfd5dad177ac45ed729ddf562e269633defb8c7d58c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "158031f8-feeb-4b5b-a25a-76e77de0418c", "node_type": "1", "metadata": {}, "hash": "d2f055718446e8739e144afffdc827144a1b6c85698d06e703b49fba2cc0dae6", "class_name": "RelatedNodeInfo"}}, "text": "The\ntwo most common error modes are copying sub-\nspans from the input and predicting an empty string.\nFurthermore, this poor performance is not due to\nrandom variance in prompt tuning, as we observe\nlow variance across 3runs for each size. These\nresults indicate that using models pre-trained with\nthe \u201cspan corruption\u201d objective can be unreliable,\nwith only 2out of 5models working well, whereas\n1081091010\nModel Parameters103105107109Task Parameters\nModel Tuning\nPrefix Tuning (Train)\nPrefix Tuning (Infer)WARP\nPrompt Tuning\nPrompt Design\n0.001%0.01%0.1%1%10%100%Task Parameters (%)Figure 4: Parameter usage of various adaptation tech-\nniques, \ufb01xing architecture to T5.1.1 and prompt/pre\ufb01x\nlength to 1\u2013100tokens (bands show mean and stddev).\nModel Tuning : All parameters are task-speci\ufb01c. Pre-\n\ufb01x Tuning : Activations are tuned in the pre\ufb01x of each\nlayer, requiring 0.1\u20131% task-speci\ufb01c parameters for in-\nference, but more are used for training.", "start_char_idx": 2144, "end_char_idx": 3092, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "158031f8-feeb-4b5b-a25a-76e77de0418c": {"__data__": {"id_": "158031f8-feeb-4b5b-a25a-76e77de0418c", "embedding": null, "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4d4bd45d6c96facb0b607dfad2fd0a2f058b3356811b4734a5825ac1bceb8225", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5c48b96-117e-411d-9fd8-b1c0866f4781", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "73439da66da8955ee71e91af18517aa0f2494bff2a104532bdcf55e72001b896", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23edd47c-2e1c-4975-a3bd-6603995ec8fc", "node_type": "1", "metadata": {}, "hash": "3bf0ccc4a0d3eb27226c7700d93441323b98ab72a03669d530c4f4fc3537e793", "class_name": "RelatedNodeInfo"}}, "text": "Model Tuning : All parameters are task-speci\ufb01c. Pre-\n\ufb01x Tuning : Activations are tuned in the pre\ufb01x of each\nlayer, requiring 0.1\u20131% task-speci\ufb01c parameters for in-\nference, but more are used for training. WARP : Task\nparameters are reduced to under 0.1% by only tuning\ninput and output layers. Prompt Tuning : Only prompt\nembeddings are tuned, reaching under 0.01% for most\nmodel sizes. Prompt Design : Only a sequence of\nprompt IDs ( 500\u20132000 tokens) is required.\nthe LM adapated versions work reliably across all\nmodel sizes.\nWe have released T5 1.1 checkpoints adapted\nusing the LM objective for 100K steps for all model\nsizes.8\n4 Comparison to Similar Approaches\nIn this section, we review recent work on learn-\ning continuous prompts, and draw comparisons\nwith our method. One important axis of compari-\nson is the number of task-speci\ufb01c parameters each\nmethod requires, as shown in Figure 4.", "start_char_idx": 2888, "end_char_idx": 3785, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23edd47c-2e1c-4975-a3bd-6603995ec8fc": {"__data__": {"id_": "23edd47c-2e1c-4975-a3bd-6603995ec8fc", "embedding": null, "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4d4bd45d6c96facb0b607dfad2fd0a2f058b3356811b4734a5825ac1bceb8225", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "158031f8-feeb-4b5b-a25a-76e77de0418c", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "6cdff4ea2d3ce87882504dd2f74455e56839e38fbc72a357bba3b09272f9283e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "678fbc52-5b8c-4a9d-a131-b7a5151582bb", "node_type": "1", "metadata": {}, "hash": "b533168360f29aa969b267279dfecc07b38d92cfa2f784c87fedfe53ebea26d8", "class_name": "RelatedNodeInfo"}}, "text": "WARP : Task\nparameters are reduced to under 0.1% by only tuning\ninput and output layers. Prompt Tuning : Only prompt\nembeddings are tuned, reaching under 0.01% for most\nmodel sizes. Prompt Design : Only a sequence of\nprompt IDs ( 500\u20132000 tokens) is required.\nthe LM adapated versions work reliably across all\nmodel sizes.\nWe have released T5 1.1 checkpoints adapted\nusing the LM objective for 100K steps for all model\nsizes.8\n4 Comparison to Similar Approaches\nIn this section, we review recent work on learn-\ning continuous prompts, and draw comparisons\nwith our method. One important axis of compari-\nson is the number of task-speci\ufb01c parameters each\nmethod requires, as shown in Figure 4. Among\nmethods with learnable parameters, prompt tuning\nis the most parameter ef\ufb01cient, requiring less than\n0.01% task-speci\ufb01c parameters for models over a\nbillion parameters.9\nLi and Liang (2021) propose \u201cpre\ufb01x tuning\u201d:\nlearning a sequence of pre\ufb01xes that are prepended\n8https://github.com/google-research/\ntext-to-text-transfer-transformer/\nblob/main/released_checkpoints.md#\nlm-adapted-t511lm100k\n9To compare with prompt design, we count each token\nID in the prompt as a parameter, and assume a prompt of\nbetween 500\u20132000 tokens to match the GPT-3 setting.", "start_char_idx": 3093, "end_char_idx": 4344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "678fbc52-5b8c-4a9d-a131-b7a5151582bb": {"__data__": {"id_": "678fbc52-5b8c-4a9d-a131-b7a5151582bb", "embedding": null, "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4d4bd45d6c96facb0b607dfad2fd0a2f058b3356811b4734a5825ac1bceb8225", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23edd47c-2e1c-4975-a3bd-6603995ec8fc", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "6bb203386e18e659838df71079a669aa1b092fe961369dbd7bf5491dfaf60bfb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9674adea-790e-404a-b66e-df9ebe95ff68", "node_type": "1", "metadata": {}, "hash": "53dbeff7649ec737b5ea387c495cb99a932493d5b9139b3bccfcbb25926f346b", "class_name": "RelatedNodeInfo"}}, "text": "One important axis of compari-\nson is the number of task-speci\ufb01c parameters each\nmethod requires, as shown in Figure 4. Among\nmethods with learnable parameters, prompt tuning\nis the most parameter ef\ufb01cient, requiring less than\n0.01% task-speci\ufb01c parameters for models over a\nbillion parameters.9\nLi and Liang (2021) propose \u201cpre\ufb01x tuning\u201d:\nlearning a sequence of pre\ufb01xes that are prepended\n8https://github.com/google-research/\ntext-to-text-transfer-transformer/\nblob/main/released_checkpoints.md#\nlm-adapted-t511lm100k\n9To compare with prompt design, we count each token\nID in the prompt as a parameter, and assume a prompt of\nbetween 500\u20132000 tokens to match the GPT-3 setting. While\nthis technique is by far the most parameter ef\ufb01cient, it comes\nat the cost of task quality.", "start_char_idx": 3666, "end_char_idx": 4442, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9674adea-790e-404a-b66e-df9ebe95ff68": {"__data__": {"id_": "9674adea-790e-404a-b66e-df9ebe95ff68", "embedding": null, "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4b364895e6d406da7a27def36e3c3eb08ef5616f522e8a807332c8d1b0e9ad8d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "678fbc52-5b8c-4a9d-a131-b7a5151582bb", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "165e8ca8cf515aba29484f7158ecebd11ff0c92564a5dd1bca20dd4af842e9b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14ce6290-a6d5-402f-821e-f631931a0fb2", "node_type": "1", "metadata": {}, "hash": "d648afeca7231503ea36467edfbaf49e7007929cb3c7111fc790305ce67202fa", "class_name": "RelatedNodeInfo"}}, "text": "at every transformer layer. This is akin to learning\ntransformer activations that are \ufb01xed across exam-\nples at every network layer. In contrast, prompt\ntuning uses a single prompt representation that\nis prepended to the embedded input. Beyond re-\nquiring fewer parameters, our approach allows the\ntransformer to update the intermediate-layer task\nrepresentations, as contextualized by an input ex-\nample. Their work builds on GPT-2 (Radford et al.,\n2019) and BART (Lewis et al., 2020), while ours fo-\ncuses on T5 and examines changes in performance\nand robustness to design choices as model size in-\ncreases. When using BART, pre\ufb01x tuning includes\npre\ufb01xes on both the encoder and decoder network,\nwhile prompt tuning only requires prompts on the\nencoder. Li and Liang (2021) also rely on a repa-\nrameterization of the pre\ufb01x to stabilize learning,\nwhich adds a large number of parameters during\ntraining, whereas our con\ufb01guration does not re-\nquire this reparameterization and is robust across\nSuperGLUE tasks and model sizes.\nHambardzumyan et al.", "start_char_idx": 0, "end_char_idx": 1047, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14ce6290-a6d5-402f-821e-f631931a0fb2": {"__data__": {"id_": "14ce6290-a6d5-402f-821e-f631931a0fb2", "embedding": null, "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4b364895e6d406da7a27def36e3c3eb08ef5616f522e8a807332c8d1b0e9ad8d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9674adea-790e-404a-b66e-df9ebe95ff68", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "ed3f2f5dbeb3e270b84be52fae7c0bc35152f16615d538f582cac5296cad1a95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad39720b-3e57-4eda-9f64-1d0f1527bc27", "node_type": "1", "metadata": {}, "hash": "328b91de9f40ff90a1923c0ef8e4e5368347b2ea1743aa98ac4ad3879f7720a4", "class_name": "RelatedNodeInfo"}}, "text": "Their work builds on GPT-2 (Radford et al.,\n2019) and BART (Lewis et al., 2020), while ours fo-\ncuses on T5 and examines changes in performance\nand robustness to design choices as model size in-\ncreases. When using BART, pre\ufb01x tuning includes\npre\ufb01xes on both the encoder and decoder network,\nwhile prompt tuning only requires prompts on the\nencoder. Li and Liang (2021) also rely on a repa-\nrameterization of the pre\ufb01x to stabilize learning,\nwhich adds a large number of parameters during\ntraining, whereas our con\ufb01guration does not re-\nquire this reparameterization and is robust across\nSuperGLUE tasks and model sizes.\nHambardzumyan et al. (2021) propose \u201cWARP\u201d,\nwhere prompt parameters are added to the input\nlayer. This method works with masked language\nmodels, relying on a [MASK] token and a learn-\nable output layer to project the mask to class logits.\nThis formulation restricts the model to producing a\nsingle output, limiting it to classi\ufb01cation.", "start_char_idx": 406, "end_char_idx": 1362, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad39720b-3e57-4eda-9f64-1d0f1527bc27": {"__data__": {"id_": "ad39720b-3e57-4eda-9f64-1d0f1527bc27", "embedding": null, "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4b364895e6d406da7a27def36e3c3eb08ef5616f522e8a807332c8d1b0e9ad8d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14ce6290-a6d5-402f-821e-f631931a0fb2", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "cfcb5dce54306c44fce93b4bd427ada582fec9f961ed9f20709ebe3addec3153", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "531b4297-4bb5-46a8-aab1-969ad39503a7", "node_type": "1", "metadata": {}, "hash": "53e43510237d7f8e983713d91cd72690601241f0195c847553bec4a88b6b0827", "class_name": "RelatedNodeInfo"}}, "text": "When using BART, pre\ufb01x tuning includes\npre\ufb01xes on both the encoder and decoder network,\nwhile prompt tuning only requires prompts on the\nencoder. Li and Liang (2021) also rely on a repa-\nrameterization of the pre\ufb01x to stabilize learning,\nwhich adds a large number of parameters during\ntraining, whereas our con\ufb01guration does not re-\nquire this reparameterization and is robust across\nSuperGLUE tasks and model sizes.\nHambardzumyan et al. (2021) propose \u201cWARP\u201d,\nwhere prompt parameters are added to the input\nlayer. This method works with masked language\nmodels, relying on a [MASK] token and a learn-\nable output layer to project the mask to class logits.\nThis formulation restricts the model to producing a\nsingle output, limiting it to classi\ufb01cation. Prompt\ntuning does not require any changes to the input or\na task-speci\ufb01c head. The performance of prompt\ntuning is also considerably closer to the strong per-\nformance of model tuning.\nLiu et al.", "start_char_idx": 610, "end_char_idx": 1559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "531b4297-4bb5-46a8-aab1-969ad39503a7": {"__data__": {"id_": "531b4297-4bb5-46a8-aab1-969ad39503a7", "embedding": null, "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4b364895e6d406da7a27def36e3c3eb08ef5616f522e8a807332c8d1b0e9ad8d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad39720b-3e57-4eda-9f64-1d0f1527bc27", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "eb68793cd51e5e1ad80c174c5f1764f3415d3c62a571495c2dcbc9b5b0ca477d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3933681c-f4f2-451f-bff3-43d1ea83f0ac", "node_type": "1", "metadata": {}, "hash": "e1cae04a065a82a4d9e8aa7c21c61a410809178d7c9bc873e7a221edaf1df631", "class_name": "RelatedNodeInfo"}}, "text": "Hambardzumyan et al. (2021) propose \u201cWARP\u201d,\nwhere prompt parameters are added to the input\nlayer. This method works with masked language\nmodels, relying on a [MASK] token and a learn-\nable output layer to project the mask to class logits.\nThis formulation restricts the model to producing a\nsingle output, limiting it to classi\ufb01cation. Prompt\ntuning does not require any changes to the input or\na task-speci\ufb01c head. The performance of prompt\ntuning is also considerably closer to the strong per-\nformance of model tuning.\nLiu et al. (2021) propose \u201cP-tuning\u201d where learn-\nable continuous prompts are interleaved throughout\nthe embedded input, using patterns based on human\ndesign. Our approach removes this complication\nby simply prepending the prompt to the input.", "start_char_idx": 1027, "end_char_idx": 1792, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3933681c-f4f2-451f-bff3-43d1ea83f0ac": {"__data__": {"id_": "3933681c-f4f2-451f-bff3-43d1ea83f0ac", "embedding": null, "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4b364895e6d406da7a27def36e3c3eb08ef5616f522e8a807332c8d1b0e9ad8d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "531b4297-4bb5-46a8-aab1-969ad39503a7", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "f10a343ebc090cc7260635a1429b8215005c353073f567506eb268526394fd54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "019c7249-24d1-434a-ab38-5f569a61dc87", "node_type": "1", "metadata": {}, "hash": "5e509e223cfd1df844f66aa54f60739bf20a0a66630650f35b5c11ec7fc892bc", "class_name": "RelatedNodeInfo"}}, "text": "Hambardzumyan et al. (2021) propose \u201cWARP\u201d,\nwhere prompt parameters are added to the input\nlayer. This method works with masked language\nmodels, relying on a [MASK] token and a learn-\nable output layer to project the mask to class logits.\nThis formulation restricts the model to producing a\nsingle output, limiting it to classi\ufb01cation. Prompt\ntuning does not require any changes to the input or\na task-speci\ufb01c head. The performance of prompt\ntuning is also considerably closer to the strong per-\nformance of model tuning.\nLiu et al. (2021) propose \u201cP-tuning\u201d where learn-\nable continuous prompts are interleaved throughout\nthe embedded input, using patterns based on human\ndesign. Our approach removes this complication\nby simply prepending the prompt to the input. To\nachieve strong SuperGLUE results, P-tuning has to\nbe used in conjunction with model tuning, that is,\nmodels jointly update both the prompt and the main\nmodel parameters, whereas our approach keeps the\noriginal language model frozen.10\nQin and Eisner (2021) use \u201csoft words\u201d to learn\nprompts to extract knowledge from pre-trained\nLMs.", "start_char_idx": 1027, "end_char_idx": 2129, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "019c7249-24d1-434a-ab38-5f569a61dc87": {"__data__": {"id_": "019c7249-24d1-434a-ab38-5f569a61dc87", "embedding": null, "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4b364895e6d406da7a27def36e3c3eb08ef5616f522e8a807332c8d1b0e9ad8d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3933681c-f4f2-451f-bff3-43d1ea83f0ac", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "778b21e25e580e707884c8df98a4ecf17a4e200e1537587a1f152e0463c99b65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "baaf5bc6-958b-4e75-a1c0-dccca96eed11", "node_type": "1", "metadata": {}, "hash": "72fe653711c1458c7d3bb032fef8d01335db933548bfd1e1134ea3456a5430b4", "class_name": "RelatedNodeInfo"}}, "text": "Prompt\ntuning does not require any changes to the input or\na task-speci\ufb01c head. The performance of prompt\ntuning is also considerably closer to the strong per-\nformance of model tuning.\nLiu et al. (2021) propose \u201cP-tuning\u201d where learn-\nable continuous prompts are interleaved throughout\nthe embedded input, using patterns based on human\ndesign. Our approach removes this complication\nby simply prepending the prompt to the input. To\nachieve strong SuperGLUE results, P-tuning has to\nbe used in conjunction with model tuning, that is,\nmodels jointly update both the prompt and the main\nmodel parameters, whereas our approach keeps the\noriginal language model frozen.10\nQin and Eisner (2021) use \u201csoft words\u201d to learn\nprompts to extract knowledge from pre-trained\nLMs. Prompts are positioned in relation to the\ninput based on hand-designed prompt prototypes,\nand a learned \u2206\u2113\niparameter is included for each\nlayer, so parameter cost scales with model depth.\n10As another difference, P-tuning requires the addition of\n\u201canchor\u201d tokens in the input (e.g.", "start_char_idx": 1363, "end_char_idx": 2412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "baaf5bc6-958b-4e75-a1c0-dccca96eed11": {"__data__": {"id_": "baaf5bc6-958b-4e75-a1c0-dccca96eed11", "embedding": null, "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4b364895e6d406da7a27def36e3c3eb08ef5616f522e8a807332c8d1b0e9ad8d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "019c7249-24d1-434a-ab38-5f569a61dc87", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "ffbfeae40529dddf1e93ecdbac50ade3e6a2ac70786142d0d1d5a56ef3de91e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f53e45aa-787b-4b90-b28d-2e56e93f3b35", "node_type": "1", "metadata": {}, "hash": "4853bd21a1d99bba56cc27752cebb8a345f0a3c404381779f9a8ee93d3076a04", "class_name": "RelatedNodeInfo"}}, "text": "Our approach removes this complication\nby simply prepending the prompt to the input. To\nachieve strong SuperGLUE results, P-tuning has to\nbe used in conjunction with model tuning, that is,\nmodels jointly update both the prompt and the main\nmodel parameters, whereas our approach keeps the\noriginal language model frozen.10\nQin and Eisner (2021) use \u201csoft words\u201d to learn\nprompts to extract knowledge from pre-trained\nLMs. Prompts are positioned in relation to the\ninput based on hand-designed prompt prototypes,\nand a learned \u2206\u2113\niparameter is included for each\nlayer, so parameter cost scales with model depth.\n10As another difference, P-tuning requires the addition of\n\u201canchor\u201d tokens in the input (e.g. a question mark following\nthe hypothesis in the RTE task) to achieve strong performance,\nwhile prompt tuning leaves inputs untouched.Dataset Domain Model Prompt \u2206\nSQuAD Wiki 94.9\u00b10.2 94.8\u00b10.1\u22120.1\nTextbookQA Book 54.3\u00b13.7 66.8\u00b12.9 +12.5\nBioASQ Bio 77.9\u00b10.4 79.1\u00b10.3 +1.2\nRACE Exam 59.8\u00b10.6 60.7\u00b10.5 +0.9\nRE Wiki 88.4\u00b10.1 88.8\u00b10.2 +0.4\nDuoRC Movie 68.9\u00b10.7 67.7\u00b11.1\u22121.2\nDROP Wiki 68.9\u00b11.7 67.1\u00b11.9\u22121.8\nTable 1: F1 mean and stddev for models trained on\nSQuAD and evaluated on out-of-domain datasets from\nthe MRQA 2019 shared task.", "start_char_idx": 1708, "end_char_idx": 2940, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f53e45aa-787b-4b90-b28d-2e56e93f3b35": {"__data__": {"id_": "f53e45aa-787b-4b90-b28d-2e56e93f3b35", "embedding": null, "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4b364895e6d406da7a27def36e3c3eb08ef5616f522e8a807332c8d1b0e9ad8d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "baaf5bc6-958b-4e75-a1c0-dccca96eed11", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "655bee70fff591d05680afeeff1fdc5d14e769a5478ec5c0761c8110a8a2e83d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1341f585-eafd-453c-a502-a412336daa81", "node_type": "1", "metadata": {}, "hash": "94c3fa5af6f7a9ddeef0b80373b63f9d13338fc7c606aa51e63d42485ac1e86b", "class_name": "RelatedNodeInfo"}}, "text": "Prompt tuning tends to\ngive stronger zero-shot performance than model tun-\ning, especially on datasets with large domain shifts like\nTextbookQA.\nLogeswaran et al. (2020) use a learnable\nprepended token to adapt transformer models to var-\nious tasks, but focus on small synthetic datasets de-\nsigned to accommodate a compositional task repre-\nsentation, as opposed to larger real-world datasets.\nTheir base models are small transformers trained\nfrom scratch jointly with the task representations,\nwhereas we keep the base model frozen and inves-\ntigate scaling laws using larger transformers.\nMore generally, work on task prompts is closely\naligned with work on \u201cadapters\u201d (Rebuf\ufb01 et al.,\n2017; Houlsby et al., 2019), small bottleneck lay-\ners inserted between frozen pre-trained network\nlayers. Adapters offer another means of reduc-\ning task-speci\ufb01c parameters, with Houlsby et al.\n(2019) achieving GLUE performance close to full\nmodel tuning when freezing BERT-Large and only\nadding 2\u20134% additional parameters. Pfeiffer et al.", "start_char_idx": 2941, "end_char_idx": 3969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1341f585-eafd-453c-a502-a412336daa81": {"__data__": {"id_": "1341f585-eafd-453c-a502-a412336daa81", "embedding": null, "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4b364895e6d406da7a27def36e3c3eb08ef5616f522e8a807332c8d1b0e9ad8d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f53e45aa-787b-4b90-b28d-2e56e93f3b35", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "25c2e4673608673c6a289464250a53cf7cf6cb665a20710f825bed340daf9d98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8275cf49-52b2-444c-ae80-56f1b4dfa854", "node_type": "1", "metadata": {}, "hash": "ac732cdac84d60c4328264e37894e3d2f6bc4276664cb5f21d16a4ce0d4b1ce7", "class_name": "RelatedNodeInfo"}}, "text": "Their base models are small transformers trained\nfrom scratch jointly with the task representations,\nwhereas we keep the base model frozen and inves-\ntigate scaling laws using larger transformers.\nMore generally, work on task prompts is closely\naligned with work on \u201cadapters\u201d (Rebuf\ufb01 et al.,\n2017; Houlsby et al., 2019), small bottleneck lay-\ners inserted between frozen pre-trained network\nlayers. Adapters offer another means of reduc-\ning task-speci\ufb01c parameters, with Houlsby et al.\n(2019) achieving GLUE performance close to full\nmodel tuning when freezing BERT-Large and only\nadding 2\u20134% additional parameters. Pfeiffer et al.\n(2020) use multiple adapters in a multilingual con-\ntext to explicitly separate language understanding\nfrom task speci\ufb01cation, similar to our approach. A\ncore difference between adapters and prompt tun-\ning is how the approaches change model behavior.\nAdapters modify the actual function that acts on the\ninput representation, parameterized by the neural\nnetwork, by allowing the rewriting of activations at\nany given layer.", "start_char_idx": 3336, "end_char_idx": 4394, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8275cf49-52b2-444c-ae80-56f1b4dfa854": {"__data__": {"id_": "8275cf49-52b2-444c-ae80-56f1b4dfa854", "embedding": null, "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e4937f0c-b264-411d-8d2e-dbca9b578b6e", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4b364895e6d406da7a27def36e3c3eb08ef5616f522e8a807332c8d1b0e9ad8d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1341f585-eafd-453c-a502-a412336daa81", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "c8346ecb0d918fe1257204c6fe5035a90c4816283c57e54b5256f0769d3e53a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd04cbd9-f8a2-415e-87ee-31fae4d25536", "node_type": "1", "metadata": {}, "hash": "1e3e4450e3bb8d4f539c2e2dc447edbf4f24e0dc11e24fafac4920d5eb19c108", "class_name": "RelatedNodeInfo"}}, "text": "Adapters offer another means of reduc-\ning task-speci\ufb01c parameters, with Houlsby et al.\n(2019) achieving GLUE performance close to full\nmodel tuning when freezing BERT-Large and only\nadding 2\u20134% additional parameters. Pfeiffer et al.\n(2020) use multiple adapters in a multilingual con-\ntext to explicitly separate language understanding\nfrom task speci\ufb01cation, similar to our approach. A\ncore difference between adapters and prompt tun-\ning is how the approaches change model behavior.\nAdapters modify the actual function that acts on the\ninput representation, parameterized by the neural\nnetwork, by allowing the rewriting of activations at\nany given layer. Prompt tuning modi\ufb01es behavior\nby leaving the function \ufb01xed and adding new in-\nput representations that can affect how subsequent\ninput is processed.\n5 Resilience to Domain Shift\nBy freezing the core language model parameters,\nprompt tuning prevents the model from modify-\ning its general understanding of language. Instead,\nprompt representations indirectly modulate the rep-\nresentation of the input. This reduces the model\u2019s\nability to over\ufb01t to a dataset by memorizing spe-", "start_char_idx": 3736, "end_char_idx": 4872, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd04cbd9-f8a2-415e-87ee-31fae4d25536": {"__data__": {"id_": "bd04cbd9-f8a2-415e-87ee-31fae4d25536", "embedding": null, "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "379c133f-b178-41b8-b255-09fb40527f5b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e22983e0a515690fe60d55f5b048f5e6039f18641bb488a10a660dd85d8dcb3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8275cf49-52b2-444c-ae80-56f1b4dfa854", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d790fc84cb8a81e7d51dba04d2acc00ed27f08d6f1d47dbebb6f1c59664cd4a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2186272-28e4-4876-9cab-21a157fedbf7", "node_type": "1", "metadata": {}, "hash": "b5fb7aa19489d9e06bf7fc42fec98680a12c60e3c8b72635c841df541c2a6381", "class_name": "RelatedNodeInfo"}}, "text": "Train Eval Tuning Accuracy F1\nQQP MRPC Model 73.1\u00b10.9 81.2\u00b12.1\nPrompt 76.3\u00b10.1 84.3\u00b10.3\nMRPC QQP Model 74.9\u00b11.3 70.9\u00b11.2\nPrompt 75.4\u00b10.8 69.7\u00b10.3\nTable 2: Mean and stddev of zero-shot domain transfer\nbetween two paraphrase detection tasks.\nci\ufb01c lexical cues and spurious correlations. This re-\nstriction suggests that prompt tuning may improve\nrobustness to domain shifts, where the distribution\nof inputs differs between training and evaluation.\nWe investigate zero-shot domain transfer on two\ntasks: question answering (QA) and paraphrase de-\ntection. For question answering, we use the MRQA\n2019 shared task on generalization (Fisch et al.,\n2019). This task collects extractive QA datasets\nin a uni\ufb01ed format and tests how models trained\non \u201cin-domain\u201d datasets perform when evaluated\non \u201cout-of-domain\u201d datasets.", "start_char_idx": 0, "end_char_idx": 816, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2186272-28e4-4876-9cab-21a157fedbf7": {"__data__": {"id_": "a2186272-28e4-4876-9cab-21a157fedbf7", "embedding": null, "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "379c133f-b178-41b8-b255-09fb40527f5b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e22983e0a515690fe60d55f5b048f5e6039f18641bb488a10a660dd85d8dcb3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd04cbd9-f8a2-415e-87ee-31fae4d25536", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4383ca70e66e29ce9ec02fffbee2f61ae21e4bef3f792b3c7b772de7ec4df20d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f2c5c93-2e83-4da1-8f8a-f3debbc13a1e", "node_type": "1", "metadata": {}, "hash": "e7fccbe3b80ad754e24e986468ca6a12ae426ff13295c6c5e8f2e805e39a5e39", "class_name": "RelatedNodeInfo"}}, "text": "ci\ufb01c lexical cues and spurious correlations. This re-\nstriction suggests that prompt tuning may improve\nrobustness to domain shifts, where the distribution\nof inputs differs between training and evaluation.\nWe investigate zero-shot domain transfer on two\ntasks: question answering (QA) and paraphrase de-\ntection. For question answering, we use the MRQA\n2019 shared task on generalization (Fisch et al.,\n2019). This task collects extractive QA datasets\nin a uni\ufb01ed format and tests how models trained\non \u201cin-domain\u201d datasets perform when evaluated\non \u201cout-of-domain\u201d datasets. For our experiments,\nwe train on SQuAD (Rajpurkar et al., 2016) and\nevaluate on each of the out-of-domain datasets.11\nTable 1 shows that prompt tuning outperforms\nmodel tuning on the majority of out-of-domain\ndatasets, with a remarkable 12.5point F1 gap be-\ntween the two approaches on TextbookQA. We ob-\nserve larger gains from prompt tuning in cases of\nlarger domain shifts (e.g. to Biomedical in BioASQ\nor to Textbooks in TextbookQA).", "start_char_idx": 240, "end_char_idx": 1254, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f2c5c93-2e83-4da1-8f8a-f3debbc13a1e": {"__data__": {"id_": "9f2c5c93-2e83-4da1-8f8a-f3debbc13a1e", "embedding": null, "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "379c133f-b178-41b8-b255-09fb40527f5b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e22983e0a515690fe60d55f5b048f5e6039f18641bb488a10a660dd85d8dcb3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2186272-28e4-4876-9cab-21a157fedbf7", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "ae1fc2abf7e689435bffaaa5b5cf7308624702a2f37b91982ccf0ae5f100fbc3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cb7a4b8-a71d-4315-9ca3-03d58900a664", "node_type": "1", "metadata": {}, "hash": "7b1ba9521b091d385445acf070e9b2d86860f6f571b7f3a9d47f260ce916eace", "class_name": "RelatedNodeInfo"}}, "text": "For question answering, we use the MRQA\n2019 shared task on generalization (Fisch et al.,\n2019). This task collects extractive QA datasets\nin a uni\ufb01ed format and tests how models trained\non \u201cin-domain\u201d datasets perform when evaluated\non \u201cout-of-domain\u201d datasets. For our experiments,\nwe train on SQuAD (Rajpurkar et al., 2016) and\nevaluate on each of the out-of-domain datasets.11\nTable 1 shows that prompt tuning outperforms\nmodel tuning on the majority of out-of-domain\ndatasets, with a remarkable 12.5point F1 gap be-\ntween the two approaches on TextbookQA. We ob-\nserve larger gains from prompt tuning in cases of\nlarger domain shifts (e.g. to Biomedical in BioASQ\nor to Textbooks in TextbookQA). Of the datasets\nwhere model tuning is better, we see that DROP\nshares a domain (Wikipedia) with SQuAD and is\nthus one of the smallest domain transfers.\nAs a second test of robustness to domain shift,\nwe explore transfer between two paraphrase detec-\ntion tasks from GLUE (Wang et al., 2019b).", "start_char_idx": 554, "end_char_idx": 1547, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2cb7a4b8-a71d-4315-9ca3-03d58900a664": {"__data__": {"id_": "2cb7a4b8-a71d-4315-9ca3-03d58900a664", "embedding": null, "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "379c133f-b178-41b8-b255-09fb40527f5b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e22983e0a515690fe60d55f5b048f5e6039f18641bb488a10a660dd85d8dcb3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f2c5c93-2e83-4da1-8f8a-f3debbc13a1e", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "72df2af1889aad08af37f1b5d19880e83e221704f5957cd0fd582c06b9a434e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9d09769-019c-43b2-985a-054385bb1c24", "node_type": "1", "metadata": {}, "hash": "170b452f43d79fdd96646d8e656a9f6e40490df1e031648050ea32019c4129bd", "class_name": "RelatedNodeInfo"}}, "text": "For our experiments,\nwe train on SQuAD (Rajpurkar et al., 2016) and\nevaluate on each of the out-of-domain datasets.11\nTable 1 shows that prompt tuning outperforms\nmodel tuning on the majority of out-of-domain\ndatasets, with a remarkable 12.5point F1 gap be-\ntween the two approaches on TextbookQA. We ob-\nserve larger gains from prompt tuning in cases of\nlarger domain shifts (e.g. to Biomedical in BioASQ\nor to Textbooks in TextbookQA). Of the datasets\nwhere model tuning is better, we see that DROP\nshares a domain (Wikipedia) with SQuAD and is\nthus one of the smallest domain transfers.\nAs a second test of robustness to domain shift,\nwe explore transfer between two paraphrase detec-\ntion tasks from GLUE (Wang et al., 2019b). The\n\ufb01rst task is QQP (Iyer et al., 2017), which asks\nif two questions from the community Q&A site\nQuora are \u201cduplicates\u201d.", "start_char_idx": 817, "end_char_idx": 1669, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9d09769-019c-43b2-985a-054385bb1c24": {"__data__": {"id_": "b9d09769-019c-43b2-985a-054385bb1c24", "embedding": null, "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "379c133f-b178-41b8-b255-09fb40527f5b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e22983e0a515690fe60d55f5b048f5e6039f18641bb488a10a660dd85d8dcb3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2cb7a4b8-a71d-4315-9ca3-03d58900a664", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "7439f7d48bed0eb67064d90b6195399881f8bd0fd4e5428db4ad170973ce9510", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "456fd25f-8b30-46be-bc17-1756b1e0b9e8", "node_type": "1", "metadata": {}, "hash": "434d9bc28f79bb893aa9b91f4517e298fe99acf19fc3d586c84e18d95773afad", "class_name": "RelatedNodeInfo"}}, "text": "We ob-\nserve larger gains from prompt tuning in cases of\nlarger domain shifts (e.g. to Biomedical in BioASQ\nor to Textbooks in TextbookQA). Of the datasets\nwhere model tuning is better, we see that DROP\nshares a domain (Wikipedia) with SQuAD and is\nthus one of the smallest domain transfers.\nAs a second test of robustness to domain shift,\nwe explore transfer between two paraphrase detec-\ntion tasks from GLUE (Wang et al., 2019b). The\n\ufb01rst task is QQP (Iyer et al., 2017), which asks\nif two questions from the community Q&A site\nQuora are \u201cduplicates\u201d. The second task is MRPC\n(Dolan and Brockett, 2005), which asks if two sen-\ntences drawn from news articles are paraphrases.\nWe test transfer in both directions (QQP \u21d4MRPC).\nAs before, we train on the \u201cin-domain\u201d task, select\ncheckpoints using in-domain validation, and evalu-\nate zero-shot on the \u201cout-of-domain\u201d task.", "start_char_idx": 1115, "end_char_idx": 1988, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "456fd25f-8b30-46be-bc17-1756b1e0b9e8": {"__data__": {"id_": "456fd25f-8b30-46be-bc17-1756b1e0b9e8", "embedding": null, "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "379c133f-b178-41b8-b255-09fb40527f5b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e22983e0a515690fe60d55f5b048f5e6039f18641bb488a10a660dd85d8dcb3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9d09769-019c-43b2-985a-054385bb1c24", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "bf479a53883235ea161eeb5caa4eee5e3198ffe7c9bb31121fbea1fdeae6ffbf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbae576a-5efc-4ec3-a083-0741a8f722ed", "node_type": "1", "metadata": {}, "hash": "ab1d2b2ffa042edf5e17b387e8e439951dfedff9248a79ec9d11c3260bf28895", "class_name": "RelatedNodeInfo"}}, "text": "Of the datasets\nwhere model tuning is better, we see that DROP\nshares a domain (Wikipedia) with SQuAD and is\nthus one of the smallest domain transfers.\nAs a second test of robustness to domain shift,\nwe explore transfer between two paraphrase detec-\ntion tasks from GLUE (Wang et al., 2019b). The\n\ufb01rst task is QQP (Iyer et al., 2017), which asks\nif two questions from the community Q&A site\nQuora are \u201cduplicates\u201d. The second task is MRPC\n(Dolan and Brockett, 2005), which asks if two sen-\ntences drawn from news articles are paraphrases.\nWe test transfer in both directions (QQP \u21d4MRPC).\nAs before, we train on the \u201cin-domain\u201d task, select\ncheckpoints using in-domain validation, and evalu-\nate zero-shot on the \u201cout-of-domain\u201d task.\nTable 2 shows that training a lightweight prompt\non the QQP data and evaluating on MRPC gives\nmuch better performance than tuning the entire\n11We select checkpoints based on SQuAD validation F1.", "start_char_idx": 1255, "end_char_idx": 2183, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbae576a-5efc-4ec3-a083-0741a8f722ed": {"__data__": {"id_": "dbae576a-5efc-4ec3-a083-0741a8f722ed", "embedding": null, "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "379c133f-b178-41b8-b255-09fb40527f5b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e22983e0a515690fe60d55f5b048f5e6039f18641bb488a10a660dd85d8dcb3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "456fd25f-8b30-46be-bc17-1756b1e0b9e8", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "67438a79ee13e062b9c7e876a03ba3df5a808b3fa877440cdc50dd8b00933254", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63d6c4ab-20af-4de3-94e0-18cfd4dcbfa7", "node_type": "1", "metadata": {}, "hash": "e4c62105fb810e8119b9fe4e6b632c50ecd74d04928c7b44328f304eb8ce9a62", "class_name": "RelatedNodeInfo"}}, "text": "The\n\ufb01rst task is QQP (Iyer et al., 2017), which asks\nif two questions from the community Q&A site\nQuora are \u201cduplicates\u201d. The second task is MRPC\n(Dolan and Brockett, 2005), which asks if two sen-\ntences drawn from news articles are paraphrases.\nWe test transfer in both directions (QQP \u21d4MRPC).\nAs before, we train on the \u201cin-domain\u201d task, select\ncheckpoints using in-domain validation, and evalu-\nate zero-shot on the \u201cout-of-domain\u201d task.\nTable 2 shows that training a lightweight prompt\non the QQP data and evaluating on MRPC gives\nmuch better performance than tuning the entire\n11We select checkpoints based on SQuAD validation F1.\nThe out-of-domain datasets are TextbookQA (Kembhavi et al.,\n2017), RACE (Lai et al., 2017), BioASQ ( http://bioasq.\norg/ ), RE (Levy et al., 2017), DuoRC (Saha et al., 2018),\nand DROP (Dua et al., 2019).Dataset Metric Average Best Ensemble\nBoolQ acc.", "start_char_idx": 1548, "end_char_idx": 2434, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63d6c4ab-20af-4de3-94e0-18cfd4dcbfa7": {"__data__": {"id_": "63d6c4ab-20af-4de3-94e0-18cfd4dcbfa7", "embedding": null, "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "379c133f-b178-41b8-b255-09fb40527f5b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e22983e0a515690fe60d55f5b048f5e6039f18641bb488a10a660dd85d8dcb3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbae576a-5efc-4ec3-a083-0741a8f722ed", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "da1c53b0770a24eaa33311b7f098ad7b3f4aeee8862af0c3881c6bd136d61210", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f3644b9-ce8b-4b54-9f9d-eb060f0b3e58", "node_type": "1", "metadata": {}, "hash": "b7c5d771c20e4d90eba8ba4d5ba97c09695ea26aa071ec21d42a53acb951c3a1", "class_name": "RelatedNodeInfo"}}, "text": "We test transfer in both directions (QQP \u21d4MRPC).\nAs before, we train on the \u201cin-domain\u201d task, select\ncheckpoints using in-domain validation, and evalu-\nate zero-shot on the \u201cout-of-domain\u201d task.\nTable 2 shows that training a lightweight prompt\non the QQP data and evaluating on MRPC gives\nmuch better performance than tuning the entire\n11We select checkpoints based on SQuAD validation F1.\nThe out-of-domain datasets are TextbookQA (Kembhavi et al.,\n2017), RACE (Lai et al., 2017), BioASQ ( http://bioasq.\norg/ ), RE (Levy et al., 2017), DuoRC (Saha et al., 2018),\nand DROP (Dua et al., 2019).Dataset Metric Average Best Ensemble\nBoolQ acc. 91.1 91.3 91.7\nCB acc./F1 99.3 / 99.0 100.00 / 100.00 100.0 /100.0\nCOPA acc.", "start_char_idx": 1794, "end_char_idx": 2511, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f3644b9-ce8b-4b54-9f9d-eb060f0b3e58": {"__data__": {"id_": "9f3644b9-ce8b-4b54-9f9d-eb060f0b3e58", "embedding": null, "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "379c133f-b178-41b8-b255-09fb40527f5b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e22983e0a515690fe60d55f5b048f5e6039f18641bb488a10a660dd85d8dcb3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63d6c4ab-20af-4de3-94e0-18cfd4dcbfa7", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "45d9eedcf30ab54c58ff609b0daf2416fdc7a6a34f0b7aa3fa8d343501bd0f3b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "345050e3-f5a5-44de-9e89-d7627b5c89d9", "node_type": "1", "metadata": {}, "hash": "bb8097e768a842951338c2e41d72beed2c0ea1324f9d70369b0a2c8d0393fe74", "class_name": "RelatedNodeInfo"}}, "text": "Table 2 shows that training a lightweight prompt\non the QQP data and evaluating on MRPC gives\nmuch better performance than tuning the entire\n11We select checkpoints based on SQuAD validation F1.\nThe out-of-domain datasets are TextbookQA (Kembhavi et al.,\n2017), RACE (Lai et al., 2017), BioASQ ( http://bioasq.\norg/ ), RE (Levy et al., 2017), DuoRC (Saha et al., 2018),\nand DROP (Dua et al., 2019).Dataset Metric Average Best Ensemble\nBoolQ acc. 91.1 91.3 91.7\nCB acc./F1 99.3 / 99.0 100.00 / 100.00 100.0 /100.0\nCOPA acc. 98.8 100.0 100.0\nMultiRC EM/F1 a65.7 / 88.7 66.3 / 89.0 67.1 /89.4\nReCoRD EM/F1 92.7 / 93.4 92.9 / 93.5 93.2 /93.9\nRTE acc.", "start_char_idx": 1989, "end_char_idx": 2635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "345050e3-f5a5-44de-9e89-d7627b5c89d9": {"__data__": {"id_": "345050e3-f5a5-44de-9e89-d7627b5c89d9", "embedding": null, "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "379c133f-b178-41b8-b255-09fb40527f5b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e22983e0a515690fe60d55f5b048f5e6039f18641bb488a10a660dd85d8dcb3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f3644b9-ce8b-4b54-9f9d-eb060f0b3e58", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "9613e61157ece5950eef56cafde356d4a8fccf04eb6ca58038ede6ed79e77e65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "877bee80-fb5b-4b2c-ac7e-fb4abd7e3fdb", "node_type": "1", "metadata": {}, "hash": "eb70e1dc385b22a23f6d662f0ace7976a1caa75508a67cbcbda34231ebe2b124", "class_name": "RelatedNodeInfo"}}, "text": "org/ ), RE (Levy et al., 2017), DuoRC (Saha et al., 2018),\nand DROP (Dua et al., 2019).Dataset Metric Average Best Ensemble\nBoolQ acc. 91.1 91.3 91.7\nCB acc./F1 99.3 / 99.0 100.00 / 100.00 100.0 /100.0\nCOPA acc. 98.8 100.0 100.0\nMultiRC EM/F1 a65.7 / 88.7 66.3 / 89.0 67.1 /89.4\nReCoRD EM/F1 92.7 / 93.4 92.9 / 93.5 93.2 /93.9\nRTE acc. 92.6 93.5 93.5\nWiC acc. 76.2 76.6 77.4\nWSC acc.", "start_char_idx": 2300, "end_char_idx": 2683, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "877bee80-fb5b-4b2c-ac7e-fb4abd7e3fdb": {"__data__": {"id_": "877bee80-fb5b-4b2c-ac7e-fb4abd7e3fdb", "embedding": null, "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "379c133f-b178-41b8-b255-09fb40527f5b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e22983e0a515690fe60d55f5b048f5e6039f18641bb488a10a660dd85d8dcb3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "345050e3-f5a5-44de-9e89-d7627b5c89d9", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "87a3b5cb6ec8f1335185133784d2dc54383eac719d6298db4f91803a8e01c4e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51675636-1f9f-4468-823a-b8373cf1cc89", "node_type": "1", "metadata": {}, "hash": "8b356620f24fe7e9f0d16988f63593bb00d9f9832ad798365be0797cc25fbd63", "class_name": "RelatedNodeInfo"}}, "text": "91.1 91.3 91.7\nCB acc./F1 99.3 / 99.0 100.00 / 100.00 100.0 /100.0\nCOPA acc. 98.8 100.0 100.0\nMultiRC EM/F1 a65.7 / 88.7 66.3 / 89.0 67.1 /89.4\nReCoRD EM/F1 92.7 / 93.4 92.9 / 93.5 93.2 /93.9\nRTE acc. 92.6 93.5 93.5\nWiC acc. 76.2 76.6 77.4\nWSC acc. 95.8 96.2 96.2\nSuperGLUE (dev) 90.5 91.0 91.3\nTable 3: Performance of a \ufb01ve-prompt ensemble built\nfrom a single frozen T5-XXL model exceeds both the\naverage and the best among the \ufb01ve prompts.\nmodel (+ 3.2accuracy and + 3.1F1).", "start_char_idx": 2435, "end_char_idx": 2911, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51675636-1f9f-4468-823a-b8373cf1cc89": {"__data__": {"id_": "51675636-1f9f-4468-823a-b8373cf1cc89", "embedding": null, "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "379c133f-b178-41b8-b255-09fb40527f5b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e22983e0a515690fe60d55f5b048f5e6039f18641bb488a10a660dd85d8dcb3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "877bee80-fb5b-4b2c-ac7e-fb4abd7e3fdb", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "366b7b053282920c6844e35630c3bd8a1556510901dbc294e890325f08760cd1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6976ce7-6b05-4ca6-a3a2-bbb6d535b322", "node_type": "1", "metadata": {}, "hash": "bf1479c9b0f9616ae63fbd657153fde1c1dfc86dac8cf225c920bdfbdb7fdb05", "class_name": "RelatedNodeInfo"}}, "text": "92.6 93.5 93.5\nWiC acc. 76.2 76.6 77.4\nWSC acc. 95.8 96.2 96.2\nSuperGLUE (dev) 90.5 91.0 91.3\nTable 3: Performance of a \ufb01ve-prompt ensemble built\nfrom a single frozen T5-XXL model exceeds both the\naverage and the best among the \ufb01ve prompts.\nmodel (+ 3.2accuracy and + 3.1F1). The results\nare much closer in the other direction, with prompt\ntuning showing a small improvement in accuracy\nand a small drop in F1. These results support the\nview that model tuning may be over-parameterized\nand more prone to over\ufb01t the training task, to the\ndetriment of similar tasks in different domains.\n6 Prompt Ensembling\nEnsembles of neural models trained from different\ninitializations on the same data are widely observed\nto improve task performance (Hansen and Salamon,\n1990) and are useful for estimating model uncer-\ntainty (Lakshminarayanan et al., 2017).", "start_char_idx": 2636, "end_char_idx": 3482, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6976ce7-6b05-4ca6-a3a2-bbb6d535b322": {"__data__": {"id_": "f6976ce7-6b05-4ca6-a3a2-bbb6d535b322", "embedding": null, "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "379c133f-b178-41b8-b255-09fb40527f5b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e22983e0a515690fe60d55f5b048f5e6039f18641bb488a10a660dd85d8dcb3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51675636-1f9f-4468-823a-b8373cf1cc89", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a12e82f660804c3bf93edd9254a98d8201ba792075bc7e57c6fb62b8ff3b3535", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6044c37d-73e6-4d95-acd9-d294fe0b5967", "node_type": "1", "metadata": {}, "hash": "4a74d72945103aca2639fec67942f07949c58907da32baa7f9a3c044debd333f", "class_name": "RelatedNodeInfo"}}, "text": "model (+ 3.2accuracy and + 3.1F1). The results\nare much closer in the other direction, with prompt\ntuning showing a small improvement in accuracy\nand a small drop in F1. These results support the\nview that model tuning may be over-parameterized\nand more prone to over\ufb01t the training task, to the\ndetriment of similar tasks in different domains.\n6 Prompt Ensembling\nEnsembles of neural models trained from different\ninitializations on the same data are widely observed\nto improve task performance (Hansen and Salamon,\n1990) and are useful for estimating model uncer-\ntainty (Lakshminarayanan et al., 2017). However,\nas model size increases, ensembling can become\nimpractical. Beyond the space required to store N\nmodels (e.g. 42GiB for each copy of T5-XXL),\nthere is a substantial inference cost to running N\ndistinct models, whether in parallel or in series.\nPrompt tuning provides a more ef\ufb01cient way to\nensemble multiple adaptations of a pre-trained lan-\nguage model.", "start_char_idx": 2877, "end_char_idx": 3846, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6044c37d-73e6-4d95-acd9-d294fe0b5967": {"__data__": {"id_": "6044c37d-73e6-4d95-acd9-d294fe0b5967", "embedding": null, "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "379c133f-b178-41b8-b255-09fb40527f5b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e22983e0a515690fe60d55f5b048f5e6039f18641bb488a10a660dd85d8dcb3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6976ce7-6b05-4ca6-a3a2-bbb6d535b322", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4c85ad3e91381790dd0e32d2237380d6326775641cacb2ca97b9549873bc29e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96ede007-8026-44a3-8f92-1c7445e282df", "node_type": "1", "metadata": {}, "hash": "bb1da9a33e613391b86e3517f9f757d61b885ebeae81d6e84ea819a6f743af5a", "class_name": "RelatedNodeInfo"}}, "text": "These results support the\nview that model tuning may be over-parameterized\nand more prone to over\ufb01t the training task, to the\ndetriment of similar tasks in different domains.\n6 Prompt Ensembling\nEnsembles of neural models trained from different\ninitializations on the same data are widely observed\nto improve task performance (Hansen and Salamon,\n1990) and are useful for estimating model uncer-\ntainty (Lakshminarayanan et al., 2017). However,\nas model size increases, ensembling can become\nimpractical. Beyond the space required to store N\nmodels (e.g. 42GiB for each copy of T5-XXL),\nthere is a substantial inference cost to running N\ndistinct models, whether in parallel or in series.\nPrompt tuning provides a more ef\ufb01cient way to\nensemble multiple adaptations of a pre-trained lan-\nguage model. By training Nprompts on the same\ntask, we create Nseparate \u201cmodels\u201d for a task,\nwhile still sharing the core language modeling pa-\nrameters throughout. Beyond drastically reducing\nstorage costs, the prompt ensemble makes infer-\nence more ef\ufb01cient.", "start_char_idx": 3047, "end_char_idx": 4094, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96ede007-8026-44a3-8f92-1c7445e282df": {"__data__": {"id_": "96ede007-8026-44a3-8f92-1c7445e282df", "embedding": null, "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "379c133f-b178-41b8-b255-09fb40527f5b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e22983e0a515690fe60d55f5b048f5e6039f18641bb488a10a660dd85d8dcb3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6044c37d-73e6-4d95-acd9-d294fe0b5967", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "959e6b364d85d8419b30504f8a1f41338490ad3101b0842fc1283b1c9ae85e21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "773f6ba9-efb2-4139-b602-945ac5d85c77", "node_type": "1", "metadata": {}, "hash": "7eb32536465c2c779324f67648fe8f9f824f3c46b4eb9844d2e1bdead27f3860", "class_name": "RelatedNodeInfo"}}, "text": "However,\nas model size increases, ensembling can become\nimpractical. Beyond the space required to store N\nmodels (e.g. 42GiB for each copy of T5-XXL),\nthere is a substantial inference cost to running N\ndistinct models, whether in parallel or in series.\nPrompt tuning provides a more ef\ufb01cient way to\nensemble multiple adaptations of a pre-trained lan-\nguage model. By training Nprompts on the same\ntask, we create Nseparate \u201cmodels\u201d for a task,\nwhile still sharing the core language modeling pa-\nrameters throughout. Beyond drastically reducing\nstorage costs, the prompt ensemble makes infer-\nence more ef\ufb01cient. To process one example, rather\nthan computing forward passes of Ndifferent mod-\nels, we can execute a single forward pass with a\nbatch size of N, replicating the example across\nthe batch and varying the prompt. These savings\nmirror those seen for multi-tasking in Figure 2.\nTo demonstrate the viability of prompt ensem-\nbling, we train \ufb01ve prompts for each SuperGLUE\ntask, using a single frozen T5-XXL model with\nour default hyperparameters.", "start_char_idx": 3483, "end_char_idx": 4536, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "773f6ba9-efb2-4139-b602-945ac5d85c77": {"__data__": {"id_": "773f6ba9-efb2-4139-b602-945ac5d85c77", "embedding": null, "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "379c133f-b178-41b8-b255-09fb40527f5b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e22983e0a515690fe60d55f5b048f5e6039f18641bb488a10a660dd85d8dcb3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96ede007-8026-44a3-8f92-1c7445e282df", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "7ecc30f63aa9aa7e3e27ac80977fd78f0fa86af5dcf585eb83e60d12ab23496f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f746793b-4175-4b08-811a-d0cd1f17c7a6", "node_type": "1", "metadata": {}, "hash": "d2159c0daac32e9e6186cb5bb48d70ebc06f9d75db3b37e3544c0a8a8016569f", "class_name": "RelatedNodeInfo"}}, "text": "Prompt tuning provides a more ef\ufb01cient way to\nensemble multiple adaptations of a pre-trained lan-\nguage model. By training Nprompts on the same\ntask, we create Nseparate \u201cmodels\u201d for a task,\nwhile still sharing the core language modeling pa-\nrameters throughout. Beyond drastically reducing\nstorage costs, the prompt ensemble makes infer-\nence more ef\ufb01cient. To process one example, rather\nthan computing forward passes of Ndifferent mod-\nels, we can execute a single forward pass with a\nbatch size of N, replicating the example across\nthe batch and varying the prompt. These savings\nmirror those seen for multi-tasking in Figure 2.\nTo demonstrate the viability of prompt ensem-\nbling, we train \ufb01ve prompts for each SuperGLUE\ntask, using a single frozen T5-XXL model with\nour default hyperparameters. We use simple major-\nity voting to compute predictions from the ensem-\nble. Table 3 shows that across all tasks, the ensem-\nble beats the single-prompt average and beats, or", "start_char_idx": 3736, "end_char_idx": 4710, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f746793b-4175-4b08-811a-d0cd1f17c7a6": {"__data__": {"id_": "f746793b-4175-4b08-811a-d0cd1f17c7a6", "embedding": null, "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4832a0d177dd6918766af58eea501f73c529330643672681eb9b815871fc123f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "773f6ba9-efb2-4139-b602-945ac5d85c77", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "644d9bedb704477907d1220f855c6c209a7e57c5911e228ba909e509859749d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "527c9790-8f88-452c-8075-a1d50d4abb37", "node_type": "1", "metadata": {}, "hash": "0b87abf6918847157ebe58cbed6d79c81e4e8e7b67af0f8e9cbe32504d66ea2f", "class_name": "RelatedNodeInfo"}}, "text": "matches, the best individual prompt.\n7 Interpretability\nAn ideally interpretable prompt would consist of\nnatural language that clearly describes the task at\nhand, explicitly asks the model for some result or\naction, and makes it easy to understand why the\nprompt elicited such behavior from the model.\nAs prompt tuning works in the continuous em-\nbedding space rather than the discrete token space,\ninterpreting prompts becomes more dif\ufb01cult. To\ntest the interpretability of our learned soft prompts,\nwe compute the nearest neighbors to each prompt\ntoken from the frozen model\u2019s vocabulary. We use\ncosine distance between the vocabulary embedding\nvector and the prompt token representation as the\nsimilarity metric.\nWe observe that for a given learned prompt to-\nken, the top-5 nearest neighbors form tight seman-\ntic clusters. For example, we see lexically similar\nclusters such as { Technology /technology /Tech-\nnologies /technological /technologies }, as well\nas more diverse but still strongly related clusters\nsuch as { entirely /completely /totally /altogether\n/100% }. The nature of these clusters suggests that\nthe prompts are in fact learning \u201cword-like\u201d repre-\nsentations.", "start_char_idx": 0, "end_char_idx": 1183, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "527c9790-8f88-452c-8075-a1d50d4abb37": {"__data__": {"id_": "527c9790-8f88-452c-8075-a1d50d4abb37", "embedding": null, "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4832a0d177dd6918766af58eea501f73c529330643672681eb9b815871fc123f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f746793b-4175-4b08-811a-d0cd1f17c7a6", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "06165093e3fb905f4cfd50e6aa58b4c30ff005724a3a8cd293f0362f45792b22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f2ee4d8-c38c-4590-a8f6-29632711875e", "node_type": "1", "metadata": {}, "hash": "b24b34514e6881006cc3e0b88adc455ebd440c54f3ac406badf33cd73a6ad5d6", "class_name": "RelatedNodeInfo"}}, "text": "As prompt tuning works in the continuous em-\nbedding space rather than the discrete token space,\ninterpreting prompts becomes more dif\ufb01cult. To\ntest the interpretability of our learned soft prompts,\nwe compute the nearest neighbors to each prompt\ntoken from the frozen model\u2019s vocabulary. We use\ncosine distance between the vocabulary embedding\nvector and the prompt token representation as the\nsimilarity metric.\nWe observe that for a given learned prompt to-\nken, the top-5 nearest neighbors form tight seman-\ntic clusters. For example, we see lexically similar\nclusters such as { Technology /technology /Tech-\nnologies /technological /technologies }, as well\nas more diverse but still strongly related clusters\nsuch as { entirely /completely /totally /altogether\n/100% }. The nature of these clusters suggests that\nthe prompts are in fact learning \u201cword-like\u201d repre-\nsentations. We found that random vectors drawn\nfrom the embedding space do not show this sort of\nsemantic clustering.\nWhen initializing the prompts using the \u201cclass-\nlabel\u201d strategy, we often \ufb01nd that the class labels\npersist through training.", "start_char_idx": 302, "end_char_idx": 1415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f2ee4d8-c38c-4590-a8f6-29632711875e": {"__data__": {"id_": "6f2ee4d8-c38c-4590-a8f6-29632711875e", "embedding": null, "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4832a0d177dd6918766af58eea501f73c529330643672681eb9b815871fc123f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "527c9790-8f88-452c-8075-a1d50d4abb37", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "26c3e70155bcd7f31b4ededa33a2f58a5e4da621ad3ca32275ef98cf4d84c782", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8adabc5c-e492-45b8-8ffc-e9f0d273529a", "node_type": "1", "metadata": {}, "hash": "b547979fb91538eb59036d5afd5453e0690e223534c6c1f614005384e8545894", "class_name": "RelatedNodeInfo"}}, "text": "We use\ncosine distance between the vocabulary embedding\nvector and the prompt token representation as the\nsimilarity metric.\nWe observe that for a given learned prompt to-\nken, the top-5 nearest neighbors form tight seman-\ntic clusters. For example, we see lexically similar\nclusters such as { Technology /technology /Tech-\nnologies /technological /technologies }, as well\nas more diverse but still strongly related clusters\nsuch as { entirely /completely /totally /altogether\n/100% }. The nature of these clusters suggests that\nthe prompts are in fact learning \u201cword-like\u201d repre-\nsentations. We found that random vectors drawn\nfrom the embedding space do not show this sort of\nsemantic clustering.\nWhen initializing the prompts using the \u201cclass-\nlabel\u201d strategy, we often \ufb01nd that the class labels\npersist through training. Speci\ufb01cally, if a prompt\ntoken is initialized to a given label, that label is\noften among the learned token\u2019s nearest neighbors\nafter tuning.", "start_char_idx": 591, "end_char_idx": 1557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8adabc5c-e492-45b8-8ffc-e9f0d273529a": {"__data__": {"id_": "8adabc5c-e492-45b8-8ffc-e9f0d273529a", "embedding": null, "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4832a0d177dd6918766af58eea501f73c529330643672681eb9b815871fc123f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f2ee4d8-c38c-4590-a8f6-29632711875e", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "aa692e5f11c0726f441931594a6ada314629536443cfa165f8875d5b08d1b7d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51cf68e4-6de2-485d-ae67-2922cc51cafa", "node_type": "1", "metadata": {}, "hash": "99ad0f8fbc1be99cdbc4ed7496750b493e0e9faad5b737c2dd5b4642b4cfed6a", "class_name": "RelatedNodeInfo"}}, "text": "We observe that for a given learned prompt to-\nken, the top-5 nearest neighbors form tight seman-\ntic clusters. For example, we see lexically similar\nclusters such as { Technology /technology /Tech-\nnologies /technological /technologies }, as well\nas more diverse but still strongly related clusters\nsuch as { entirely /completely /totally /altogether\n/100% }. The nature of these clusters suggests that\nthe prompts are in fact learning \u201cword-like\u201d repre-\nsentations. We found that random vectors drawn\nfrom the embedding space do not show this sort of\nsemantic clustering.\nWhen initializing the prompts using the \u201cclass-\nlabel\u201d strategy, we often \ufb01nd that the class labels\npersist through training. Speci\ufb01cally, if a prompt\ntoken is initialized to a given label, that label is\noften among the learned token\u2019s nearest neighbors\nafter tuning. When initializing with the \u201cRandom\nUniform\u201d or \u201cSampled V ocab\u201d methods, the class\nlabels can also be found in the nearest neighbors\nof the prompts; however they tend to appear as\nneighbors to multiple prompt tokens.", "start_char_idx": 716, "end_char_idx": 1774, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51cf68e4-6de2-485d-ae67-2922cc51cafa": {"__data__": {"id_": "51cf68e4-6de2-485d-ae67-2922cc51cafa", "embedding": null, "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4832a0d177dd6918766af58eea501f73c529330643672681eb9b815871fc123f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8adabc5c-e492-45b8-8ffc-e9f0d273529a", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "7aef2f08b24838e83ecb352a7f77fc575448c412cbedb169e1eb639c04ce9033", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0516e21-8c2c-4b8a-b038-55de634e68f3", "node_type": "1", "metadata": {}, "hash": "299fc2fa0756460fe3e6964e0ae1e19ac0a91da34775a548cf3b81d7a3673c61", "class_name": "RelatedNodeInfo"}}, "text": "The nature of these clusters suggests that\nthe prompts are in fact learning \u201cword-like\u201d repre-\nsentations. We found that random vectors drawn\nfrom the embedding space do not show this sort of\nsemantic clustering.\nWhen initializing the prompts using the \u201cclass-\nlabel\u201d strategy, we often \ufb01nd that the class labels\npersist through training. Speci\ufb01cally, if a prompt\ntoken is initialized to a given label, that label is\noften among the learned token\u2019s nearest neighbors\nafter tuning. When initializing with the \u201cRandom\nUniform\u201d or \u201cSampled V ocab\u201d methods, the class\nlabels can also be found in the nearest neighbors\nof the prompts; however they tend to appear as\nneighbors to multiple prompt tokens. This suggests\nthat the model is learning to store the expected\noutput classes in the prompts as reference, and\ninitializing the prompt to outputs classes makes\nthis easier and more centralized.\nWhen examining longer prompts (e.g. size 100),\nwe often \ufb01nd several prompt tokens with the same\nnearest neighbors.", "start_char_idx": 1077, "end_char_idx": 2083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0516e21-8c2c-4b8a-b038-55de634e68f3": {"__data__": {"id_": "d0516e21-8c2c-4b8a-b038-55de634e68f3", "embedding": null, "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4832a0d177dd6918766af58eea501f73c529330643672681eb9b815871fc123f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51cf68e4-6de2-485d-ae67-2922cc51cafa", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "f0733db3ebdf05b55a4c8ff41af9c2d0d4770e1e018ed62e9b201d7cae425cc6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c9581b0-6633-4c0a-a3de-8f6bb89eeef6", "node_type": "1", "metadata": {}, "hash": "5f11521d98844f78f5002270ed1a45f0f511be9fbe441797041d5b6f983bf8b1", "class_name": "RelatedNodeInfo"}}, "text": "When initializing the prompts using the \u201cclass-\nlabel\u201d strategy, we often \ufb01nd that the class labels\npersist through training. Speci\ufb01cally, if a prompt\ntoken is initialized to a given label, that label is\noften among the learned token\u2019s nearest neighbors\nafter tuning. When initializing with the \u201cRandom\nUniform\u201d or \u201cSampled V ocab\u201d methods, the class\nlabels can also be found in the nearest neighbors\nof the prompts; however they tend to appear as\nneighbors to multiple prompt tokens. This suggests\nthat the model is learning to store the expected\noutput classes in the prompts as reference, and\ninitializing the prompt to outputs classes makes\nthis easier and more centralized.\nWhen examining longer prompts (e.g. size 100),\nwe often \ufb01nd several prompt tokens with the same\nnearest neighbors. This suggests there is either\nexcess capacity in the prompt, or that the lack of\nsequential structure in the prompt representation\nmakes it dif\ufb01cult for the model to localize informa-\ntion to a speci\ufb01c position.", "start_char_idx": 1290, "end_char_idx": 2295, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c9581b0-6633-4c0a-a3de-8f6bb89eeef6": {"__data__": {"id_": "4c9581b0-6633-4c0a-a3de-8f6bb89eeef6", "embedding": null, "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4832a0d177dd6918766af58eea501f73c529330643672681eb9b815871fc123f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0516e21-8c2c-4b8a-b038-55de634e68f3", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "3cf0088d9d7e958a66bac22b6a54a1f8ccf54137082a5d6a416e94bfb2650c51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79de0656-c33e-48fe-b16e-4b4a5dc56f40", "node_type": "1", "metadata": {}, "hash": "600e9338a41e17fa42fc940d5e417cc6c150beef8dcf990b30d5090c7bea88eb", "class_name": "RelatedNodeInfo"}}, "text": "When initializing with the \u201cRandom\nUniform\u201d or \u201cSampled V ocab\u201d methods, the class\nlabels can also be found in the nearest neighbors\nof the prompts; however they tend to appear as\nneighbors to multiple prompt tokens. This suggests\nthat the model is learning to store the expected\noutput classes in the prompts as reference, and\ninitializing the prompt to outputs classes makes\nthis easier and more centralized.\nWhen examining longer prompts (e.g. size 100),\nwe often \ufb01nd several prompt tokens with the same\nnearest neighbors. This suggests there is either\nexcess capacity in the prompt, or that the lack of\nsequential structure in the prompt representation\nmakes it dif\ufb01cult for the model to localize informa-\ntion to a speci\ufb01c position.\nWhile the learned prompts taken as sequencesshow little interpretability, we do observe a high\nfrequency of words like science ,technology and\nengineering as the nearest neighbors for prompts\ntrained on the BoolQ dataset and approximately\n20% of the questions are in the \u201cNature/Science\u201d\ncategory.", "start_char_idx": 1558, "end_char_idx": 2593, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79de0656-c33e-48fe-b16e-4b4a5dc56f40": {"__data__": {"id_": "79de0656-c33e-48fe-b16e-4b4a5dc56f40", "embedding": null, "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4832a0d177dd6918766af58eea501f73c529330643672681eb9b815871fc123f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c9581b0-6633-4c0a-a3de-8f6bb89eeef6", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e65c00b2166679add8e51e3eea52f96489d334c643bdf6c93e79522bf3c1cc7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "813b382a-3fa6-46bf-b827-f0c91fe6c876", "node_type": "1", "metadata": {}, "hash": "88cfc7a736fa9cf80b08148858489e4492cdd4cdf8f3866090601383dbba8c9c", "class_name": "RelatedNodeInfo"}}, "text": "This suggests\nthat the model is learning to store the expected\noutput classes in the prompts as reference, and\ninitializing the prompt to outputs classes makes\nthis easier and more centralized.\nWhen examining longer prompts (e.g. size 100),\nwe often \ufb01nd several prompt tokens with the same\nnearest neighbors. This suggests there is either\nexcess capacity in the prompt, or that the lack of\nsequential structure in the prompt representation\nmakes it dif\ufb01cult for the model to localize informa-\ntion to a speci\ufb01c position.\nWhile the learned prompts taken as sequencesshow little interpretability, we do observe a high\nfrequency of words like science ,technology and\nengineering as the nearest neighbors for prompts\ntrained on the BoolQ dataset and approximately\n20% of the questions are in the \u201cNature/Science\u201d\ncategory. While more investigation is needed, this\nsuggests that one role of the prompt may be to\nprime the model to interpret inputs in a speci\ufb01c\ndomain or context (e.g. \u201cscienti\ufb01c\u201d).\n8 Conclusion\nIn this paper, we showed that prompt tuning is\na competitive technique for adapting frozen pre-\ntrained language models to downstream tasks.", "start_char_idx": 1775, "end_char_idx": 2922, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "813b382a-3fa6-46bf-b827-f0c91fe6c876": {"__data__": {"id_": "813b382a-3fa6-46bf-b827-f0c91fe6c876", "embedding": null, "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4832a0d177dd6918766af58eea501f73c529330643672681eb9b815871fc123f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79de0656-c33e-48fe-b16e-4b4a5dc56f40", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "36e48bb08adc6533f57b315c543dea432970b6a374e53ef267888270b015081c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eef0880a-fdf4-4a0b-af16-09f9162865da", "node_type": "1", "metadata": {}, "hash": "127e43206c1069c5f92f032195417cb60edfca4600bd22cdccbc3a3b88e781a6", "class_name": "RelatedNodeInfo"}}, "text": "This suggests there is either\nexcess capacity in the prompt, or that the lack of\nsequential structure in the prompt representation\nmakes it dif\ufb01cult for the model to localize informa-\ntion to a speci\ufb01c position.\nWhile the learned prompts taken as sequencesshow little interpretability, we do observe a high\nfrequency of words like science ,technology and\nengineering as the nearest neighbors for prompts\ntrained on the BoolQ dataset and approximately\n20% of the questions are in the \u201cNature/Science\u201d\ncategory. While more investigation is needed, this\nsuggests that one role of the prompt may be to\nprime the model to interpret inputs in a speci\ufb01c\ndomain or context (e.g. \u201cscienti\ufb01c\u201d).\n8 Conclusion\nIn this paper, we showed that prompt tuning is\na competitive technique for adapting frozen pre-\ntrained language models to downstream tasks. On\nthe popular SuperGLUE benchmark, its task perfor-\nmance rivals that of traditional model tuning, with\nthe gap vanishing as model size increases. On zero-\nshot domain transfer, we found that prompt tuning\nleads to improved generalization.", "start_char_idx": 2084, "end_char_idx": 3163, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eef0880a-fdf4-4a0b-af16-09f9162865da": {"__data__": {"id_": "eef0880a-fdf4-4a0b-af16-09f9162865da", "embedding": null, "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4832a0d177dd6918766af58eea501f73c529330643672681eb9b815871fc123f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "813b382a-3fa6-46bf-b827-f0c91fe6c876", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "6a38acc9b361d899503a9bf6ea1686ad12b00ed5fbada06194c8a84f851c8375", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c12baca-0b1d-479f-99d4-f688ec2c4f38", "node_type": "1", "metadata": {}, "hash": "81a293e8c924d4c2b80e733d900a412c09d1876b41c023c968bb02e8fc382a18", "class_name": "RelatedNodeInfo"}}, "text": "While the learned prompts taken as sequencesshow little interpretability, we do observe a high\nfrequency of words like science ,technology and\nengineering as the nearest neighbors for prompts\ntrained on the BoolQ dataset and approximately\n20% of the questions are in the \u201cNature/Science\u201d\ncategory. While more investigation is needed, this\nsuggests that one role of the prompt may be to\nprime the model to interpret inputs in a speci\ufb01c\ndomain or context (e.g. \u201cscienti\ufb01c\u201d).\n8 Conclusion\nIn this paper, we showed that prompt tuning is\na competitive technique for adapting frozen pre-\ntrained language models to downstream tasks. On\nthe popular SuperGLUE benchmark, its task perfor-\nmance rivals that of traditional model tuning, with\nthe gap vanishing as model size increases. On zero-\nshot domain transfer, we found that prompt tuning\nleads to improved generalization. This plausibly in-\ndicates that freezing general-purpose language un-\nderstanding parameters and restricting downstream\nlearning to a lightweight parameter footprint can\nhelp to avoid over\ufb01tting to a speci\ufb01c domain.", "start_char_idx": 2296, "end_char_idx": 3379, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c12baca-0b1d-479f-99d4-f688ec2c4f38": {"__data__": {"id_": "3c12baca-0b1d-479f-99d4-f688ec2c4f38", "embedding": null, "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4832a0d177dd6918766af58eea501f73c529330643672681eb9b815871fc123f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eef0880a-fdf4-4a0b-af16-09f9162865da", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "6e4d2d9bb880aa0e318c951681d3b27ed1dc5f81a1d5a30cd26efbce7558a688", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "255a4ca4-5a39-4ba2-a18a-63e427abc4b4", "node_type": "1", "metadata": {}, "hash": "5cf48bb352d7e250a4da813a59ac4240f0df9845a3c442eb4fcb7564ae31449a", "class_name": "RelatedNodeInfo"}}, "text": "While more investigation is needed, this\nsuggests that one role of the prompt may be to\nprime the model to interpret inputs in a speci\ufb01c\ndomain or context (e.g. \u201cscienti\ufb01c\u201d).\n8 Conclusion\nIn this paper, we showed that prompt tuning is\na competitive technique for adapting frozen pre-\ntrained language models to downstream tasks. On\nthe popular SuperGLUE benchmark, its task perfor-\nmance rivals that of traditional model tuning, with\nthe gap vanishing as model size increases. On zero-\nshot domain transfer, we found that prompt tuning\nleads to improved generalization. This plausibly in-\ndicates that freezing general-purpose language un-\nderstanding parameters and restricting downstream\nlearning to a lightweight parameter footprint can\nhelp to avoid over\ufb01tting to a speci\ufb01c domain.\nBeyond task quality metrics, we discussed the\nappeal of moving to frozen pre-trained models in\nterms of storage and serving costs. This move\nenables both ef\ufb01cient multi-task serving, as well\nas ef\ufb01cient high-performing prompt ensembling.", "start_char_idx": 2594, "end_char_idx": 3617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "255a4ca4-5a39-4ba2-a18a-63e427abc4b4": {"__data__": {"id_": "255a4ca4-5a39-4ba2-a18a-63e427abc4b4", "embedding": null, "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4832a0d177dd6918766af58eea501f73c529330643672681eb9b815871fc123f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c12baca-0b1d-479f-99d4-f688ec2c4f38", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "29de573c9e881e0dd5d6e6af6f7798576949f06e7157b2abd0e292d1f59de935", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95bffacd-5b23-4d05-82d3-481a002b8a28", "node_type": "1", "metadata": {}, "hash": "0bf2c08931a4810cac1f3d4329fe25d234aaeacef6980925a62a80a1e583c981", "class_name": "RelatedNodeInfo"}}, "text": "\u201cscienti\ufb01c\u201d).\n8 Conclusion\nIn this paper, we showed that prompt tuning is\na competitive technique for adapting frozen pre-\ntrained language models to downstream tasks. On\nthe popular SuperGLUE benchmark, its task perfor-\nmance rivals that of traditional model tuning, with\nthe gap vanishing as model size increases. On zero-\nshot domain transfer, we found that prompt tuning\nleads to improved generalization. This plausibly in-\ndicates that freezing general-purpose language un-\nderstanding parameters and restricting downstream\nlearning to a lightweight parameter footprint can\nhelp to avoid over\ufb01tting to a speci\ufb01c domain.\nBeyond task quality metrics, we discussed the\nappeal of moving to frozen pre-trained models in\nterms of storage and serving costs. This move\nenables both ef\ufb01cient multi-task serving, as well\nas ef\ufb01cient high-performing prompt ensembling.\nLooking forward, we believe that factoring out\ntask-de\ufb01ning parameters as distinct from general\nlanguage-modeling parameters is an exciting step\nthat opens up many avenues for new research.", "start_char_idx": 2755, "end_char_idx": 3807, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95bffacd-5b23-4d05-82d3-481a002b8a28": {"__data__": {"id_": "95bffacd-5b23-4d05-82d3-481a002b8a28", "embedding": null, "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4832a0d177dd6918766af58eea501f73c529330643672681eb9b815871fc123f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "255a4ca4-5a39-4ba2-a18a-63e427abc4b4", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "7882e698147efe31e8ed0463161f3e7ae22d4a4e8ad20386c572ad01626a517b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d556a2c5-2206-4561-953d-3abf5da4651e", "node_type": "1", "metadata": {}, "hash": "bccc564dfa3e9b239ffdbb767e3251eaef50e1bc9587f7039c1165fcc3e592bc", "class_name": "RelatedNodeInfo"}}, "text": "On\nthe popular SuperGLUE benchmark, its task perfor-\nmance rivals that of traditional model tuning, with\nthe gap vanishing as model size increases. On zero-\nshot domain transfer, we found that prompt tuning\nleads to improved generalization. This plausibly in-\ndicates that freezing general-purpose language un-\nderstanding parameters and restricting downstream\nlearning to a lightweight parameter footprint can\nhelp to avoid over\ufb01tting to a speci\ufb01c domain.\nBeyond task quality metrics, we discussed the\nappeal of moving to frozen pre-trained models in\nterms of storage and serving costs. This move\nenables both ef\ufb01cient multi-task serving, as well\nas ef\ufb01cient high-performing prompt ensembling.\nLooking forward, we believe that factoring out\ntask-de\ufb01ning parameters as distinct from general\nlanguage-modeling parameters is an exciting step\nthat opens up many avenues for new research.\nAcknowledgements\nWe thank Lucas Dixon, Waleed Ammar, Slav\nPetrov and Sebastian Ruder for comments on an\nearlier draft, and the following people for helpful\ndiscussion: Colin Raffel, Adam Roberts, and Noam\nShazeer.", "start_char_idx": 2923, "end_char_idx": 4021, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d556a2c5-2206-4561-953d-3abf5da4651e": {"__data__": {"id_": "d556a2c5-2206-4561-953d-3abf5da4651e", "embedding": null, "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4832a0d177dd6918766af58eea501f73c529330643672681eb9b815871fc123f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95bffacd-5b23-4d05-82d3-481a002b8a28", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "9d6e4859be11f8eeba9ca7a2d9f5d5b7c00e7cb6121c702f8efda664843ef823", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7859ec4-1226-40ad-bb0b-54244b5ff11a", "node_type": "1", "metadata": {}, "hash": "f3d6432e261de320bebf17a87bb28fe1349e8fe7677587d55f6a5b70e16f4d86", "class_name": "RelatedNodeInfo"}}, "text": "Beyond task quality metrics, we discussed the\nappeal of moving to frozen pre-trained models in\nterms of storage and serving costs. This move\nenables both ef\ufb01cient multi-task serving, as well\nas ef\ufb01cient high-performing prompt ensembling.\nLooking forward, we believe that factoring out\ntask-de\ufb01ning parameters as distinct from general\nlanguage-modeling parameters is an exciting step\nthat opens up many avenues for new research.\nAcknowledgements\nWe thank Lucas Dixon, Waleed Ammar, Slav\nPetrov and Sebastian Ruder for comments on an\nearlier draft, and the following people for helpful\ndiscussion: Colin Raffel, Adam Roberts, and Noam\nShazeer. We thank Linting Xue for help with the\nLM adaptation training.\nReferences\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\nDanilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. 2006. The second PASCAL recognising\ntextual entailment challenge. In Proceedings of the\nsecond PASCAL challenges workshop on recognis-\ning textual entailment , volume 6, pages 6\u20134. Venice.", "start_char_idx": 3380, "end_char_idx": 4390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7859ec4-1226-40ad-bb0b-54244b5ff11a": {"__data__": {"id_": "a7859ec4-1226-40ad-bb0b-54244b5ff11a", "embedding": null, "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85ccb18b-1e90-4eb2-961c-fba10142d3b9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4832a0d177dd6918766af58eea501f73c529330643672681eb9b815871fc123f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d556a2c5-2206-4561-953d-3abf5da4651e", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "878ec6916392b092b3c526552b6b28030e3a1ddc50e71c322baa6701523e0528", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56314dd6-1061-4ddb-a636-db0206f57be9", "node_type": "1", "metadata": {}, "hash": "52299c33ae6213d53f0ec9dbe920e57a1a094e18368b38b1c1fb1ae1f5a63c3d", "class_name": "RelatedNodeInfo"}}, "text": "Acknowledgements\nWe thank Lucas Dixon, Waleed Ammar, Slav\nPetrov and Sebastian Ruder for comments on an\nearlier draft, and the following people for helpful\ndiscussion: Colin Raffel, Adam Roberts, and Noam\nShazeer. We thank Linting Xue for help with the\nLM adaptation training.\nReferences\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\nDanilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. 2006. The second PASCAL recognising\ntextual entailment challenge. In Proceedings of the\nsecond PASCAL challenges workshop on recognis-\ning textual entailment , volume 6, pages 6\u20134. Venice.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The \ufb01fth PASCAL recognizing\ntextual entailment challenge. In TAC.", "start_char_idx": 3808, "end_char_idx": 4528, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56314dd6-1061-4ddb-a636-db0206f57be9": {"__data__": {"id_": "56314dd6-1061-4ddb-a636-db0206f57be9", "embedding": null, "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "12aeaf684445dcd0966210d0ca8c77e24dc614c54f152c228187b9c09b4be7f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7859ec4-1226-40ad-bb0b-54244b5ff11a", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "41dc2b8364a2fef75d67123d2065009e732c957d00247738fda3885a93d84c89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6fba6239-e970-42ad-85e5-77522f5a20bc", "node_type": "1", "metadata": {}, "hash": "4a89c0c5e871d4f7989af6992cd1fb1191a87ad0e22aee9a0f3718b301399ca3", "class_name": "RelatedNodeInfo"}}, "text": "James Bradbury, Roy Frostig, Peter Hawkins,\nMatthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake\nVanderPlas, Skye Wanderman-Milne, and Qiao\nZhang. 2018. JAX: composable transformations of\nPython+NumPy programs.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems ,\nvolume 33, pages 1877\u20131901.", "start_char_idx": 0, "end_char_idx": 847, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6fba6239-e970-42ad-85e5-77522f5a20bc": {"__data__": {"id_": "6fba6239-e970-42ad-85e5-77522f5a20bc", "embedding": null, "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "12aeaf684445dcd0966210d0ca8c77e24dc614c54f152c228187b9c09b4be7f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56314dd6-1061-4ddb-a636-db0206f57be9", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "6db4abf47525d7b9ce41dcd9b9b9ed851ebc193f09f961db027bad9e1ab30dda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be1f73fa-af05-413f-b982-cba94adc557d", "node_type": "1", "metadata": {}, "hash": "307f8fe3b800ffa31a923d2e6b0bba056b5b357e0f0538a5dcdcf37528e51614", "class_name": "RelatedNodeInfo"}}, "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems ,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndif\ufb01culty of natural yes/no questions.", "start_char_idx": 245, "end_char_idx": 1053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be1f73fa-af05-413f-b982-cba94adc557d": {"__data__": {"id_": "be1f73fa-af05-413f-b982-cba94adc557d", "embedding": null, "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "12aeaf684445dcd0966210d0ca8c77e24dc614c54f152c228187b9c09b4be7f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6fba6239-e970-42ad-85e5-77522f5a20bc", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "c88f9e3bb05ab42be0eba10e631ef22125c1c7d567d7d5ea9d3b63535adc899e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d40b99d0-b88d-42bc-9114-127e80e37eec", "node_type": "1", "metadata": {}, "hash": "51cf73b7e808c11f038ba3be64557f5e3ce44c8f25353d2cd9c9bc67b9536e6c", "class_name": "RelatedNodeInfo"}}, "text": "2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems ,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndif\ufb01culty of natural yes/no questions. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2924\u20132936, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The PASCAL recognising textual entailment\nchallenge. In Machine Learning Challenges Work-\nshop , pages 177\u2013190. Springer.\nMarie-Catherine De Marneff, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The CommitmentBank: In-\nvestigating projection in naturally occurring dis-\ncourse.", "start_char_idx": 720, "end_char_idx": 1656, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d40b99d0-b88d-42bc-9114-127e80e37eec": {"__data__": {"id_": "d40b99d0-b88d-42bc-9114-127e80e37eec", "embedding": null, "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "12aeaf684445dcd0966210d0ca8c77e24dc614c54f152c228187b9c09b4be7f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be1f73fa-af05-413f-b982-cba94adc557d", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "1f1732267eacae10f4f9d5280e7c54f92486b34827194de7dfea48cc97c021d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68ff5801-50e7-428f-803a-b533a9e3e46f", "node_type": "1", "metadata": {}, "hash": "30af494aa3a2e030afc67a2e2560bbb38934d02b26d8a9b0ace2101272df4571", "class_name": "RelatedNodeInfo"}}, "text": "2019. BoolQ: Exploring the surprising\ndif\ufb01culty of natural yes/no questions. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2924\u20132936, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The PASCAL recognising textual entailment\nchallenge. In Machine Learning Challenges Work-\nshop , pages 177\u2013190. Springer.\nMarie-Catherine De Marneff, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The CommitmentBank: In-\nvestigating projection in naturally occurring dis-\ncourse. Proceedings of Sinn und Bedeutung 23 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.", "start_char_idx": 977, "end_char_idx": 1851, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68ff5801-50e7-428f-803a-b533a9e3e46f": {"__data__": {"id_": "68ff5801-50e7-428f-803a-b533a9e3e46f", "embedding": null, "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "12aeaf684445dcd0966210d0ca8c77e24dc614c54f152c228187b9c09b4be7f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d40b99d0-b88d-42bc-9114-127e80e37eec", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "bceb7915db098aa2c9ca18125d996364a3b09073dae77fde73f338b87c25ada5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee82a7a9-c60b-4806-b7d5-c098de5d78b2", "node_type": "1", "metadata": {}, "hash": "33dc8e93d344c1200219a1d366b8723eae55eb0449deb068a90d8cab76a9de0f", "class_name": "RelatedNodeInfo"}}, "text": "Association for Computational\nLinguistics.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The PASCAL recognising textual entailment\nchallenge. In Machine Learning Challenges Work-\nshop , pages 177\u2013190. Springer.\nMarie-Catherine De Marneff, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The CommitmentBank: In-\nvestigating projection in naturally occurring dis-\ncourse. Proceedings of Sinn und Bedeutung 23 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171\u20134186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.", "start_char_idx": 1283, "end_char_idx": 2226, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee82a7a9-c60b-4806-b7d5-c098de5d78b2": {"__data__": {"id_": "ee82a7a9-c60b-4806-b7d5-c098de5d78b2", "embedding": null, "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "12aeaf684445dcd0966210d0ca8c77e24dc614c54f152c228187b9c09b4be7f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68ff5801-50e7-428f-803a-b533a9e3e46f", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "3603c126b40fa9cf8d299f8a5ff12d8bc7f5d591fe814bfaf31dd655671dad98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ccbab52-6288-4576-95a0-a19a7abd4d1f", "node_type": "1", "metadata": {}, "hash": "3b4d7fb16f181f57c6606bf3e6207fff7acc6f6551de0b9c4f2d5a1295a36e40", "class_name": "RelatedNodeInfo"}}, "text": "Springer.\nMarie-Catherine De Marneff, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The CommitmentBank: In-\nvestigating projection in naturally occurring dis-\ncourse. Proceedings of Sinn und Bedeutung 23 .\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171\u20134186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nInProceedings of the Third International Workshop\non Paraphrasing (IWP2005) .\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs.", "start_char_idx": 1492, "end_char_idx": 2491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ccbab52-6288-4576-95a0-a19a7abd4d1f": {"__data__": {"id_": "3ccbab52-6288-4576-95a0-a19a7abd4d1f", "embedding": null, "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "12aeaf684445dcd0966210d0ca8c77e24dc614c54f152c228187b9c09b4be7f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee82a7a9-c60b-4806-b7d5-c098de5d78b2", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "8d269d52236a329743623838274635a30da91f1b7d462bc8b002a320f649189e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a0e28e3-c119-4cba-b3ae-9acbd1cd82bc", "node_type": "1", "metadata": {}, "hash": "291451816d1ffc366ef52e0d38004b56b8b7f005cb6531f2bd5e5de83e9b8bec", "class_name": "RelatedNodeInfo"}}, "text": "2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171\u20134186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nInProceedings of the Third International Workshop\non Paraphrasing (IWP2005) .\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1(Long and Short Papers) , pages 2368\u20132378, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.", "start_char_idx": 1762, "end_char_idx": 2762, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a0e28e3-c119-4cba-b3ae-9acbd1cd82bc": {"__data__": {"id_": "9a0e28e3-c119-4cba-b3ae-9acbd1cd82bc", "embedding": null, "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "12aeaf684445dcd0966210d0ca8c77e24dc614c54f152c228187b9c09b4be7f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ccbab52-6288-4576-95a0-a19a7abd4d1f", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "1cb1ae4a41de58e5167624b830b81e6c8a2b570e0f43b1063dc280d3ddbcdbe3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd766762-ca70-408f-827d-7285e8780576", "node_type": "1", "metadata": {}, "hash": "0b2a1933a68a794b6318c59961b5141f125356160080536362371095be1c332f", "class_name": "RelatedNodeInfo"}}, "text": "Associ-\nation for Computational Linguistics.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nInProceedings of the Third International Workshop\non Paraphrasing (IWP2005) .\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1(Long and Short Papers) , pages 2368\u20132378, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eu-\nnsol Choi, and Danqi Chen. 2019. MRQA 2019\nshared task: Evaluating generalization in reading\ncomprehension. In Proceedings of 2nd Machine\nReading for Reading Comprehension (MRQA) Work-\nshop at EMNLP .", "start_char_idx": 2075, "end_char_idx": 3016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd766762-ca70-408f-827d-7285e8780576": {"__data__": {"id_": "fd766762-ca70-408f-827d-7285e8780576", "embedding": null, "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "12aeaf684445dcd0966210d0ca8c77e24dc614c54f152c228187b9c09b4be7f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a0e28e3-c119-4cba-b3ae-9acbd1cd82bc", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "42374d99eadb559d855871b1ea8eefbd0ec093aefc5257c7d30c15a0d3249cc7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba52c01b-71d4-4afb-a5e8-a332aa3388d7", "node_type": "1", "metadata": {}, "hash": "487c1a24fe667ed3c0194c3c43107cedc8c429a5cdad16ffcfa4062e3e0457e9", "class_name": "RelatedNodeInfo"}}, "text": "Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1(Long and Short Papers) , pages 2368\u20132378, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eu-\nnsol Choi, and Danqi Chen. 2019. MRQA 2019\nshared task: Evaluating generalization in reading\ncomprehension. In Proceedings of 2nd Machine\nReading for Reading Comprehension (MRQA) Work-\nshop at EMNLP .\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recog-\nnizing textual entailment challenge. In Proceedings\nof the ACL-PASCAL workshop on textual entailment\nand paraphrasing , pages 1\u20139.", "start_char_idx": 2305, "end_char_idx": 3242, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba52c01b-71d4-4afb-a5e8-a332aa3388d7": {"__data__": {"id_": "ba52c01b-71d4-4afb-a5e8-a332aa3388d7", "embedding": null, "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "12aeaf684445dcd0966210d0ca8c77e24dc614c54f152c228187b9c09b4be7f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd766762-ca70-408f-827d-7285e8780576", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "5ce9665220c0a0598a5235cf1ad760453ded4daa18fcdf63a6173a893b30435f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c68bf0c-3a3e-4374-90a2-5c3320e9da1d", "node_type": "1", "metadata": {}, "hash": "89979045fe540a3339e4fb62bf032c3b43ab3c2d83969a4d72957d32e6a507b0", "class_name": "RelatedNodeInfo"}}, "text": "Association for Computational\nLinguistics.\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eu-\nnsol Choi, and Danqi Chen. 2019. MRQA 2019\nshared task: Evaluating generalization in reading\ncomprehension. In Proceedings of 2nd Machine\nReading for Reading Comprehension (MRQA) Work-\nshop at EMNLP .\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recog-\nnizing textual entailment challenge. In Proceedings\nof the ACL-PASCAL workshop on textual entailment\nand paraphrasing , pages 1\u20139. Association for Com-\nputational Linguistics.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021. WARP: Word-level Adversar-\nial ReProgramming. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers) , pages 4921\u20134933, Online.", "start_char_idx": 2720, "end_char_idx": 3618, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c68bf0c-3a3e-4374-90a2-5c3320e9da1d": {"__data__": {"id_": "1c68bf0c-3a3e-4374-90a2-5c3320e9da1d", "embedding": null, "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "12aeaf684445dcd0966210d0ca8c77e24dc614c54f152c228187b9c09b4be7f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba52c01b-71d4-4afb-a5e8-a332aa3388d7", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "74ee507ce33dc127cee90744c83ec6363563683e23932acb0b4b2e9670888a36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9bec683a-b82d-44d7-aa0a-26e69d186443", "node_type": "1", "metadata": {}, "hash": "bde08944d0db763614583c87d58429392c61f2647434476351a5025cf27c3da5", "class_name": "RelatedNodeInfo"}}, "text": "In Proceedings of 2nd Machine\nReading for Reading Comprehension (MRQA) Work-\nshop at EMNLP .\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recog-\nnizing textual entailment challenge. In Proceedings\nof the ACL-PASCAL workshop on textual entailment\nand paraphrasing , pages 1\u20139. Association for Com-\nputational Linguistics.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021. WARP: Word-level Adversar-\nial ReProgramming. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers) , pages 4921\u20134933, Online. Associa-\ntion for Computational Linguistics.\nL. K. Hansen and P. Salamon. 1990. Neural network\nensembles. IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 12(10):993\u20131001.", "start_char_idx": 2924, "end_char_idx": 3806, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bec683a-b82d-44d7-aa0a-26e69d186443": {"__data__": {"id_": "9bec683a-b82d-44d7-aa0a-26e69d186443", "embedding": null, "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "12aeaf684445dcd0966210d0ca8c77e24dc614c54f152c228187b9c09b4be7f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c68bf0c-3a3e-4374-90a2-5c3320e9da1d", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a695c344614da3a003731cc9a730cf38580b925af4a52160a26e1e1b8c652aec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e224cf01-5cb6-4c6f-8335-c7cf0b51ba6a", "node_type": "1", "metadata": {}, "hash": "12bca25ecf4a885135c67998993422bedab1b898fa9e6a879a10f984e20be4eb", "class_name": "RelatedNodeInfo"}}, "text": "The third PASCAL recog-\nnizing textual entailment challenge. In Proceedings\nof the ACL-PASCAL workshop on textual entailment\nand paraphrasing , pages 1\u20139. Association for Com-\nputational Linguistics.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021. WARP: Word-level Adversar-\nial ReProgramming. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers) , pages 4921\u20134933, Online. Associa-\ntion for Computational Linguistics.\nL. K. Hansen and P. Salamon. 1990. Neural network\nensembles. IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 12(10):993\u20131001.\nJonathan Heek, Anselm Levskaya, Avital Oliver, Mar-\nvin Ritter, Bertrand Rondepierre, Andreas Steiner,\nand Marc van Zee. 2020. Flax: A neural network\nlibrary and ecosystem for JAX.", "start_char_idx": 3088, "end_char_idx": 3987, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e224cf01-5cb6-4c6f-8335-c7cf0b51ba6a": {"__data__": {"id_": "e224cf01-5cb6-4c6f-8335-c7cf0b51ba6a", "embedding": null, "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "12aeaf684445dcd0966210d0ca8c77e24dc614c54f152c228187b9c09b4be7f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9bec683a-b82d-44d7-aa0a-26e69d186443", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "148e8af1d8057588b521f67a4879479d24871340877dd5fd42f9859067c533f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7cc6140d-66be-4eed-aafa-16fa334f286b", "node_type": "1", "metadata": {}, "hash": "7e48ec221b7d2976b927a80164b27667d6a9948192f9fab9d5d05ccaecd2e3fb", "class_name": "RelatedNodeInfo"}}, "text": "2021. WARP: Word-level Adversar-\nial ReProgramming. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers) , pages 4921\u20134933, Online. Associa-\ntion for Computational Linguistics.\nL. K. Hansen and P. Salamon. 1990. Neural network\nensembles. IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 12(10):993\u20131001.\nJonathan Heek, Anselm Levskaya, Avital Oliver, Mar-\nvin Ritter, Bertrand Rondepierre, Andreas Steiner,\nand Marc van Zee. 2020. Flax: A neural network\nlibrary and ecosystem for JAX.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-ef\ufb01cient transfer learning for NLP.", "start_char_idx": 3346, "end_char_idx": 4183, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7cc6140d-66be-4eed-aafa-16fa334f286b": {"__data__": {"id_": "7cc6140d-66be-4eed-aafa-16fa334f286b", "embedding": null, "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "12aeaf684445dcd0966210d0ca8c77e24dc614c54f152c228187b9c09b4be7f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e224cf01-5cb6-4c6f-8335-c7cf0b51ba6a", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "fd2feb72e3c0ae9178cfa968c51b15d62003350b72a52eac8adecbf1cbca7f4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13cd0e23-2706-4e65-abcc-caf9d0e3ffa6", "node_type": "1", "metadata": {}, "hash": "5f393a1017fc4d54a0b8b2d69a199a132dfd57c41423b976762ccb4a088d5db6", "class_name": "RelatedNodeInfo"}}, "text": "Associa-\ntion for Computational Linguistics.\nL. K. Hansen and P. Salamon. 1990. Neural network\nensembles. IEEE Transactions on Pattern Analysis\nand Machine Intelligence , 12(10):993\u20131001.\nJonathan Heek, Anselm Levskaya, Avital Oliver, Mar-\nvin Ritter, Bertrand Rondepierre, Andreas Steiner,\nand Marc van Zee. 2020. Flax: A neural network\nlibrary and ecosystem for JAX.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-ef\ufb01cient transfer learning for NLP.\nInProceedings of the 36th International Conference\non Machine Learning , volume 97 of Proceedings\nof Machine Learning Research , pages 2790\u20132799.\nPMLR.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation.", "start_char_idx": 3619, "end_char_idx": 4436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13cd0e23-2706-4e65-abcc-caf9d0e3ffa6": {"__data__": {"id_": "13cd0e23-2706-4e65-abcc-caf9d0e3ffa6", "embedding": null, "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "12aeaf684445dcd0966210d0ca8c77e24dc614c54f152c228187b9c09b4be7f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7cc6140d-66be-4eed-aafa-16fa334f286b", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a557143c7a3875bbd6c3227dac329c526aafbfd52b5c09f17b13ab91297b58e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e071bcdd-702a-4eb1-8c5e-c9d979f31d76", "node_type": "1", "metadata": {}, "hash": "4dd1fb6c68213a378ad54096c82cac12fb5e983b0b778ab8a29216821e4d9456", "class_name": "RelatedNodeInfo"}}, "text": "Jonathan Heek, Anselm Levskaya, Avital Oliver, Mar-\nvin Ritter, Bertrand Rondepierre, Andreas Steiner,\nand Marc van Zee. 2020. Flax: A neural network\nlibrary and ecosystem for JAX.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-ef\ufb01cient transfer learning for NLP.\nInProceedings of the 36th International Conference\non Machine Learning , volume 97 of Proceedings\nof Machine Learning Research , pages 2790\u20132799.\nPMLR.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 328\u2013339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nShankar Iyer, Nikhil Dandekar, and Kornel Csernai.\n2017.", "start_char_idx": 3807, "end_char_idx": 4692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e071bcdd-702a-4eb1-8c5e-c9d979f31d76": {"__data__": {"id_": "e071bcdd-702a-4eb1-8c5e-c9d979f31d76", "embedding": null, "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "12aeaf684445dcd0966210d0ca8c77e24dc614c54f152c228187b9c09b4be7f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13cd0e23-2706-4e65-abcc-caf9d0e3ffa6", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "acd336c4b79df0d3422512a1b3979bf7226ffed39c954f3f117be62d3ba4c3a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "670ea53d-d1e9-4bf4-9675-6cde9da55787", "node_type": "1", "metadata": {}, "hash": "51fe9cae8af13d7b899bc7340be311a1f16b4db06b602ee3b7501a0ec4c92247", "class_name": "RelatedNodeInfo"}}, "text": "2019. Parameter-ef\ufb01cient transfer learning for NLP.\nInProceedings of the 36th International Conference\non Machine Learning , volume 97 of Proceedings\nof Machine Learning Research , pages 2790\u20132799.\nPMLR.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 328\u2013339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nShankar Iyer, Nikhil Dandekar, and Kornel Csernai.\n2017. First Quora dataset release: Question pairs.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics , 8:423\u2013438.\nA. Kembhavi, M. Seo, D. Schwenk, J. Choi, A. Farhadi,\nand H. Hajishirzi. 2017.", "start_char_idx": 4132, "end_char_idx": 4999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "670ea53d-d1e9-4bf4-9675-6cde9da55787": {"__data__": {"id_": "670ea53d-d1e9-4bf4-9675-6cde9da55787", "embedding": null, "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "12aeaf684445dcd0966210d0ca8c77e24dc614c54f152c228187b9c09b4be7f2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e071bcdd-702a-4eb1-8c5e-c9d979f31d76", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "6811ce42fdbcc845ec8652e211be9e97e60cab62697bbfc3c5e8a46432e3bc52", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6781525e-8480-44c5-9174-6341b0e015ac", "node_type": "1", "metadata": {}, "hash": "7525664b25137171017e959d57d7ec4a41ccdfa3a0a245dea34d3eec1a732b74", "class_name": "RelatedNodeInfo"}}, "text": "2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 328\u2013339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nShankar Iyer, Nikhil Dandekar, and Kornel Csernai.\n2017. First Quora dataset release: Question pairs.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics , 8:423\u2013438.\nA. Kembhavi, M. Seo, D. Schwenk, J. Choi, A. Farhadi,\nand H. Hajishirzi. 2017. Are you smarter than a\nsixth grader? textbook question answering for multi-\nmodal machine comprehension. In 2017 IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR) , pages 5376\u20135384.", "start_char_idx": 4371, "end_char_idx": 5199, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6781525e-8480-44c5-9174-6341b0e015ac": {"__data__": {"id_": "6781525e-8480-44c5-9174-6341b0e015ac", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "670ea53d-d1e9-4bf4-9675-6cde9da55787", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "fedf62451ccf197071d4ccc04adf50993a40f8285f682a31ebe7f63d4e6e6e30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6791fd4-90fe-4ef5-af23-4c3fe135e934", "node_type": "1", "metadata": {}, "hash": "16c3b1f7fe5ab665528b689c8ebec899f6634c5f5732b3f40f2dbbc608bc961e", "class_name": "RelatedNodeInfo"}}, "text": "Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking be-\nyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof North American Chapter of the Association for\nComputational Linguistics (NAACL) .\nVid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu,\nYordan Yordanov, and Thomas Lukasiewicz. 2019.\nA surprisingly robust trick for the Winograd schema\nchallenge. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics ,\npages 4837\u20134842, Florence, Italy. Association for\nComputational Linguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations , pages 66\u201371, Brussels, Belgium.\nAssociation for Computational Linguistics.", "start_char_idx": 0, "end_char_idx": 957, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6791fd4-90fe-4ef5-af23-4c3fe135e934": {"__data__": {"id_": "e6791fd4-90fe-4ef5-af23-4c3fe135e934", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6781525e-8480-44c5-9174-6341b0e015ac", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "91ff6a8e904f2a38072f291ba86faa54fcb9c454fb77ff07b4936fada24fbedb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28a3eaf2-be00-4db7-a230-61f7b0c18454", "node_type": "1", "metadata": {}, "hash": "d189c0ff314b097aed66ad8e9aa63807b4e8806802592866f92c8ebcc751d36d", "class_name": "RelatedNodeInfo"}}, "text": "In Proceedings\nof North American Chapter of the Association for\nComputational Linguistics (NAACL) .\nVid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu,\nYordan Yordanov, and Thomas Lukasiewicz. 2019.\nA surprisingly robust trick for the Winograd schema\nchallenge. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics ,\npages 4837\u20134842, Florence, Italy. Association for\nComputational Linguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations , pages 66\u201371, Brussels, Belgium.\nAssociation for Computational Linguistics.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. RACE: Large-scale ReAd-\ning comprehension dataset from examinations.", "start_char_idx": 186, "end_char_idx": 1098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28a3eaf2-be00-4db7-a230-61f7b0c18454": {"__data__": {"id_": "28a3eaf2-be00-4db7-a230-61f7b0c18454", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6791fd4-90fe-4ef5-af23-4c3fe135e934", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "f6296f86ccaad98f68567aebb7644e6efe9daba2a44e22cc604c174a7d375f7e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f79e72eb-6945-4965-b735-42e762c84ee8", "node_type": "1", "metadata": {}, "hash": "a5befb8fdcda16a5c224b983b747c147a49ddfcfa2f5dd5ba77aaf3171bac691", "class_name": "RelatedNodeInfo"}}, "text": "2019.\nA surprisingly robust trick for the Winograd schema\nchallenge. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics ,\npages 4837\u20134842, Florence, Italy. Association for\nComputational Linguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations , pages 66\u201371, Brussels, Belgium.\nAssociation for Computational Linguistics.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. RACE: Large-scale ReAd-\ning comprehension dataset from examinations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n785\u2013794, Copenhagen, Denmark. Association for\nComputational Linguistics.\nBalaji Lakshminarayanan, Alexander Pritzel, and\nCharles Blundell. 2017. Simple and scalable predic-\ntive uncertainty estimation using deep ensembles.", "start_char_idx": 377, "end_char_idx": 1419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f79e72eb-6945-4965-b735-42e762c84ee8": {"__data__": {"id_": "f79e72eb-6945-4965-b735-42e762c84ee8", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28a3eaf2-be00-4db7-a230-61f7b0c18454", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4e33c0530accaa7bef19c155cb0480ebbad13a0b0428f2d210939a17ef2b7662", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "443a33d5-fe83-4c2c-a637-11e220d5e8b9", "node_type": "1", "metadata": {}, "hash": "9797a0b7f1f1eb4d1faa76651c5cd1c92c5cacbb39a1e0aa12ad5dc362aac415", "class_name": "RelatedNodeInfo"}}, "text": "2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations , pages 66\u201371, Brussels, Belgium.\nAssociation for Computational Linguistics.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. RACE: Large-scale ReAd-\ning comprehension dataset from examinations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n785\u2013794, Copenhagen, Denmark. Association for\nComputational Linguistics.\nBalaji Lakshminarayanan, Alexander Pritzel, and\nCharles Blundell. 2017. Simple and scalable predic-\ntive uncertainty estimation using deep ensembles. In\nAdvances in Neural Information Processing Systems ,\nvolume 30. Curran Associates, Inc.\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The Winograd schema challenge.", "start_char_idx": 649, "end_char_idx": 1602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "443a33d5-fe83-4c2c-a637-11e220d5e8b9": {"__data__": {"id_": "443a33d5-fe83-4c2c-a637-11e220d5e8b9", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f79e72eb-6945-4965-b735-42e762c84ee8", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "ae924c9ebd7ac46cc5b3d394ca7402fffa0d379dad3de8327c3ee72aa1e94e19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21a69d4d-32a2-4343-a878-df7303d60ff1", "node_type": "1", "metadata": {}, "hash": "06da45421e3962e356d5a7eb9c33c45176d2ca3c8258aa85f8cdd2632297319c", "class_name": "RelatedNodeInfo"}}, "text": "Association for Computational Linguistics.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. RACE: Large-scale ReAd-\ning comprehension dataset from examinations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n785\u2013794, Copenhagen, Denmark. Association for\nComputational Linguistics.\nBalaji Lakshminarayanan, Alexander Pritzel, and\nCharles Blundell. 2017. Simple and scalable predic-\ntive uncertainty estimation using deep ensembles. In\nAdvances in Neural Information Processing Systems ,\nvolume 30. Curran Associates, Inc.\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The Winograd schema challenge. In\nThirteenth International Conference on the Princi-\nples of Knowledge Representation and Reasoning .\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke\nZettlemoyer. 2017. Zero-shot relation extraction via\nreading comprehension.", "start_char_idx": 915, "end_char_idx": 1827, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21a69d4d-32a2-4343-a878-df7303d60ff1": {"__data__": {"id_": "21a69d4d-32a2-4343-a878-df7303d60ff1", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "443a33d5-fe83-4c2c-a637-11e220d5e8b9", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "164bfb687450ad81b3b8dc5f1fef1cbe78194f0435a55f69fd9651c39fa275c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2db2b4d6-7a81-4a9c-b6fc-22e1f92717ad", "node_type": "1", "metadata": {}, "hash": "949336a9842aca1006b89cab832796860f5d7716f6b30f71b02790a13b2b4033", "class_name": "RelatedNodeInfo"}}, "text": "In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing , pages\n785\u2013794, Copenhagen, Denmark. Association for\nComputational Linguistics.\nBalaji Lakshminarayanan, Alexander Pritzel, and\nCharles Blundell. 2017. Simple and scalable predic-\ntive uncertainty estimation using deep ensembles. In\nAdvances in Neural Information Processing Systems ,\nvolume 30. Curran Associates, Inc.\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The Winograd schema challenge. In\nThirteenth International Conference on the Princi-\nples of Knowledge Representation and Reasoning .\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke\nZettlemoyer. 2017. Zero-shot relation extraction via\nreading comprehension. In Proceedings of the 21st\nConference on Computational Natural Language\nLearning (CoNLL 2017) , pages 333\u2013342, Vancou-\nver, Canada. Association for Computational Linguis-\ntics.", "start_char_idx": 1099, "end_char_idx": 2004, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2db2b4d6-7a81-4a9c-b6fc-22e1f92717ad": {"__data__": {"id_": "2db2b4d6-7a81-4a9c-b6fc-22e1f92717ad", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21a69d4d-32a2-4343-a878-df7303d60ff1", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "f19976f81b1de4747fcd6defa1f45da56b855a06998877ef23c9a2f92053ebf4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c9fdaa2-1d76-4b85-ac8d-74d2f42f5802", "node_type": "1", "metadata": {}, "hash": "6635ff13854264e01d19795670a5cf2fd4fea149f5bef0685d3a125129942b9d", "class_name": "RelatedNodeInfo"}}, "text": "Balaji Lakshminarayanan, Alexander Pritzel, and\nCharles Blundell. 2017. Simple and scalable predic-\ntive uncertainty estimation using deep ensembles. In\nAdvances in Neural Information Processing Systems ,\nvolume 30. Curran Associates, Inc.\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The Winograd schema challenge. In\nThirteenth International Conference on the Princi-\nples of Knowledge Representation and Reasoning .\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke\nZettlemoyer. 2017. Zero-shot relation extraction via\nreading comprehension. In Proceedings of the 21st\nConference on Computational Natural Language\nLearning (CoNLL 2017) , pages 333\u2013342, Vancou-\nver, Canada. Association for Computational Linguis-\ntics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020.", "start_char_idx": 1270, "end_char_idx": 2143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c9fdaa2-1d76-4b85-ac8d-74d2f42f5802": {"__data__": {"id_": "3c9fdaa2-1d76-4b85-ac8d-74d2f42f5802", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2db2b4d6-7a81-4a9c-b6fc-22e1f92717ad", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "7931451091893bae5340478d87a69a13ff8716ea842855f8a5b99c726857c986", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9475402-a72e-478a-ad98-e336327f54e1", "node_type": "1", "metadata": {}, "hash": "8d075b7104db62cc8bfd0ef9f5d973be0eea3e7a9d84374a0d32a7197f3bdf91", "class_name": "RelatedNodeInfo"}}, "text": "Curran Associates, Inc.\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The Winograd schema challenge. In\nThirteenth International Conference on the Princi-\nples of Knowledge Representation and Reasoning .\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke\nZettlemoyer. 2017. Zero-shot relation extraction via\nreading comprehension. In Proceedings of the 21st\nConference on Computational Natural Language\nLearning (CoNLL 2017) , pages 333\u2013342, Vancou-\nver, Canada. Association for Computational Linguis-\ntics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics , pages 7871\u20137880, Online. Association\nfor Computational Linguistics.\nXiang Lisa Li and Percy Liang.", "start_char_idx": 1486, "end_char_idx": 2453, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9475402-a72e-478a-ad98-e336327f54e1": {"__data__": {"id_": "b9475402-a72e-478a-ad98-e336327f54e1", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c9fdaa2-1d76-4b85-ac8d-74d2f42f5802", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "09c26cf57b01b20f77f5dd05d357de4372e69312ca8c07a408dd075760de3ce8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7cceb124-274d-4a1e-8ea8-4d4286625a10", "node_type": "1", "metadata": {}, "hash": "64697d53586b003ed34eec49d86463050a41b96b65e2b35244822d8211bfc87b", "class_name": "RelatedNodeInfo"}}, "text": "2017. Zero-shot relation extraction via\nreading comprehension. In Proceedings of the 21st\nConference on Computational Natural Language\nLearning (CoNLL 2017) , pages 333\u2013342, Vancou-\nver, Canada. Association for Computational Linguis-\ntics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics , pages 7871\u20137880, Online. Association\nfor Computational Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Pre\ufb01x-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages4582\u20134597, Online. Association for Computational\nLinguistics.", "start_char_idx": 1765, "end_char_idx": 2780, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7cceb124-274d-4a1e-8ea8-4d4286625a10": {"__data__": {"id_": "7cceb124-274d-4a1e-8ea8-4d4286625a10", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9475402-a72e-478a-ad98-e336327f54e1", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "36831c9123caf89ac31d01ff632b220c3a4bf1f937cb6394ff8bddf415c07a9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83d857a3-86ea-42b0-b6d2-8173642ba453", "node_type": "1", "metadata": {}, "hash": "cced4230c9ae73890117273b329745644978a51546666684372c8c06c492fba5", "class_name": "RelatedNodeInfo"}}, "text": "2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics , pages 7871\u20137880, Online. Association\nfor Computational Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Pre\ufb01x-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages4582\u20134597, Online. Association for Computational\nLinguistics.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT\nunderstands, too. CoRR , abs/2103.10385.\nLajanugen Logeswaran, Ann Lee, Myle Ott, Honglak\nLee, Marc\u2019Aurelio Ranzato, and Arthur Szlam.\n2020.", "start_char_idx": 2138, "end_char_idx": 3018, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83d857a3-86ea-42b0-b6d2-8173642ba453": {"__data__": {"id_": "83d857a3-86ea-42b0-b6d2-8173642ba453", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7cceb124-274d-4a1e-8ea8-4d4286625a10", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "7dcc5c765c8ddd3717dbddfcf2cdd2a3b4a40feb0b5ed5226beaaba48c495232", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0db060d-6f3f-4100-8503-51203880b818", "node_type": "1", "metadata": {}, "hash": "440bb8355f1fb39b8bcf062cceb0ceabe0eb01a3537318db4b1cb7e4a695a69f", "class_name": "RelatedNodeInfo"}}, "text": "Association\nfor Computational Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Pre\ufb01x-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages4582\u20134597, Online. Association for Computational\nLinguistics.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT\nunderstands, too. CoRR , abs/2103.10385.\nLajanugen Logeswaran, Ann Lee, Myle Ott, Honglak\nLee, Marc\u2019Aurelio Ranzato, and Arthur Szlam.\n2020. Few-shot sequence learning with transform-\ners.CoRR , abs/2012.09543.\nVinod Nair and Geoffrey E. Hinton. 2010. Recti\ufb01ed\nlinear units improve restricted Boltzmann machines.", "start_char_idx": 2380, "end_char_idx": 3190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0db060d-6f3f-4100-8503-51203880b818": {"__data__": {"id_": "a0db060d-6f3f-4100-8503-51203880b818", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83d857a3-86ea-42b0-b6d2-8173642ba453", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "23eb5c2d6b5f2c142b96ae071757beb8e2a4717025c87713bdbd8aa0894ee4c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af8c6e75-5b3e-4ab3-9fb9-79308a312647", "node_type": "1", "metadata": {}, "hash": "2d98cf3392988b0cb3bb9189b0305b674d3018b56d6cc73a1df80bd03a92b806", "class_name": "RelatedNodeInfo"}}, "text": "Association for Computational\nLinguistics.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT\nunderstands, too. CoRR , abs/2103.10385.\nLajanugen Logeswaran, Ann Lee, Myle Ott, Honglak\nLee, Marc\u2019Aurelio Ranzato, and Arthur Szlam.\n2020. Few-shot sequence learning with transform-\ners.CoRR , abs/2012.09543.\nVinod Nair and Geoffrey E. Hinton. 2010. Recti\ufb01ed\nlinear units improve restricted Boltzmann machines.\nInProceedings of the 27th International Conference\non International Conference on Machine Learning ,\nICML\u201910, page 807\u2013814, Madison, WI, USA. Om-\nnipress.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations.", "start_char_idx": 2738, "end_char_idx": 3505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af8c6e75-5b3e-4ab3-9fb9-79308a312647": {"__data__": {"id_": "af8c6e75-5b3e-4ab3-9fb9-79308a312647", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0db060d-6f3f-4100-8503-51203880b818", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d6374d5ff578294df2f6a9589ebafea96f7b1a4b4ebf99d1614d0877f51788fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fde1bfd7-7ad9-4a3f-868e-ff550405e87c", "node_type": "1", "metadata": {}, "hash": "b9b57909a1e4a25498d16f7561af28f4bc49924dfc510eeef5cfb37eb0b49004", "class_name": "RelatedNodeInfo"}}, "text": "GPT\nunderstands, too. CoRR , abs/2103.10385.\nLajanugen Logeswaran, Ann Lee, Myle Ott, Honglak\nLee, Marc\u2019Aurelio Ranzato, and Arthur Szlam.\n2020. Few-shot sequence learning with transform-\ners.CoRR , abs/2012.09543.\nVinod Nair and Geoffrey E. Hinton. 2010. Recti\ufb01ed\nlinear units improve restricted Boltzmann machines.\nInProceedings of the 27th International Conference\non International Conference on Machine Learning ,\nICML\u201910, page 807\u2013814, Madison, WI, USA. Om-\nnipress.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers) , pages\n2227\u20132237, New Orleans, Louisiana. Association\nfor Computational Linguistics.", "start_char_idx": 2874, "end_char_idx": 3767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fde1bfd7-7ad9-4a3f-868e-ff550405e87c": {"__data__": {"id_": "fde1bfd7-7ad9-4a3f-868e-ff550405e87c", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af8c6e75-5b3e-4ab3-9fb9-79308a312647", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a9c204dc66814ead6ced2ddf0a981614a439a71eaaf47d27c87610f714fda7b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01fffc19-cc5f-4c5d-a929-1b28af70bca0", "node_type": "1", "metadata": {}, "hash": "c918e282b1da1f75cae5ae758a35774bd99223c93f070d901cadd818d5023093", "class_name": "RelatedNodeInfo"}}, "text": "Vinod Nair and Geoffrey E. Hinton. 2010. Recti\ufb01ed\nlinear units improve restricted Boltzmann machines.\nInProceedings of the 27th International Conference\non International Conference on Machine Learning ,\nICML\u201910, page 807\u2013814, Madison, WI, USA. Om-\nnipress.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers) , pages\n2227\u20132237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nJonas Pfeiffer, Ivan Vuli \u00b4c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.", "start_char_idx": 3089, "end_char_idx": 3915, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01fffc19-cc5f-4c5d-a929-1b28af70bca0": {"__data__": {"id_": "01fffc19-cc5f-4c5d-a929-1b28af70bca0", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fde1bfd7-7ad9-4a3f-868e-ff550405e87c", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "9bbdeae85d8731cb56a36b45a174f10894c8c157ca4c25c6f7f8c1cc1d35d1eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63a222a9-9a11-4255-a06c-572c10c91819", "node_type": "1", "metadata": {}, "hash": "274f153bc178b88818de74cc5cf386ff305d3a521e293b97e263529d4346a10e", "class_name": "RelatedNodeInfo"}}, "text": "Om-\nnipress.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers) , pages\n2227\u20132237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nJonas Pfeiffer, Ivan Vuli \u00b4c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nInProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) ,\npages 7654\u20137673, Online. Association for Computa-\ntional Linguistics.\nMohammad Taher Pilehvar and Jose Camacho-\nCollados. 2018. WiC: 10,000 example pairs for\nevaluating context-sensitive representations. CoRR ,\nabs/1808.09121.", "start_char_idx": 3333, "end_char_idx": 4241, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63a222a9-9a11-4255-a06c-572c10c91819": {"__data__": {"id_": "63a222a9-9a11-4255-a06c-572c10c91819", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01fffc19-cc5f-4c5d-a929-1b28af70bca0", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "9f7f707b26249fdf91af7b04634589ea1d0ada47649566212fde2f6b51d26415", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08b29995-a904-4aa0-afd4-beec83a1cafb", "node_type": "1", "metadata": {}, "hash": "14b441bf494ea27d4e15085b30fa0d597725a7096f06f3d6ebb38bddd6548a22", "class_name": "RelatedNodeInfo"}}, "text": "Association\nfor Computational Linguistics.\nJonas Pfeiffer, Ivan Vuli \u00b4c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nInProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) ,\npages 7654\u20137673, Online. Association for Computa-\ntional Linguistics.\nMohammad Taher Pilehvar and Jose Camacho-\nCollados. 2018. WiC: 10,000 example pairs for\nevaluating context-sensitive representations. CoRR ,\nabs/1808.09121.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nInProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 5203\u20135212, Online. Association for Compu-\ntational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever.", "start_char_idx": 3725, "end_char_idx": 4630, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08b29995-a904-4aa0-afd4-beec83a1cafb": {"__data__": {"id_": "08b29995-a904-4aa0-afd4-beec83a1cafb", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63a222a9-9a11-4255-a06c-572c10c91819", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "72e05e8a89944509dd250ad8361d874ef1c4bf861b640306757fb55cee04439b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a67b01e4-3050-4be7-a07d-306bc59c2b32", "node_type": "1", "metadata": {}, "hash": "222df63f64a95c09fea0e571e1335a49a2bf9f75670082ec516ec2714fa9e9ef", "class_name": "RelatedNodeInfo"}}, "text": "Association for Computa-\ntional Linguistics.\nMohammad Taher Pilehvar and Jose Camacho-\nCollados. 2018. WiC: 10,000 example pairs for\nevaluating context-sensitive representations. CoRR ,\nabs/1808.09121.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nInProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 5203\u20135212, Online. Association for Compu-\ntational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog .", "start_char_idx": 4040, "end_char_idx": 4854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a67b01e4-3050-4be7-a07d-306bc59c2b32": {"__data__": {"id_": "a67b01e4-3050-4be7-a07d-306bc59c2b32", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08b29995-a904-4aa0-afd4-beec83a1cafb", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "579b5fa6a46337f4a94cd76ee67ac9c140f119f13585bbb5d5ae40abadeb403c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9cc89ed-3da3-46f1-82b7-ccdf3bb4def6", "node_type": "1", "metadata": {}, "hash": "7b999dbf0301135b5357ff67baadee2bc4ec60f2b6289d41d8b2b8cc032e22f6", "class_name": "RelatedNodeInfo"}}, "text": "CoRR ,\nabs/1808.09121.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nInProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 5203\u20135212, Online. Association for Compu-\ntational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog .\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uni\ufb01ed text-to-\ntext transformer.", "start_char_idx": 4219, "end_char_idx": 5071, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9cc89ed-3da3-46f1-82b7-ccdf3bb4def6": {"__data__": {"id_": "e9cc89ed-3da3-46f1-82b7-ccdf3bb4def6", "embedding": null, "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b46805fa-d37e-4c10-b49b-378a48c3c60f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a77cce14eef97ce1f5583a47cab6876eb5fe892c533efb097ddaebba8e40957c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a67b01e4-3050-4be7-a07d-306bc59c2b32", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "c38f6f3c4db1801ce05bcd0f613c645e6ae80c383b6159245187c83f9d170a34", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4b9faf5-0f15-4da3-b0af-9070ab8c918f", "node_type": "1", "metadata": {}, "hash": "5776a93ef4d291489af7d3afea46bdba723300834f4965a3a16f503807088e21", "class_name": "RelatedNodeInfo"}}, "text": "Association for Compu-\ntational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog .\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uni\ufb01ed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch , 21(140):1\u201367.", "start_char_idx": 4518, "end_char_idx": 5126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4b9faf5-0f15-4da3-b0af-9070ab8c918f": {"__data__": {"id_": "b4b9faf5-0f15-4da3-b0af-9070ab8c918f", "embedding": null, "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "73d9fa8bf5100471ac4eabaebbb21f397c030a5c1df11fcf79ea5e796617ebe8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9cc89ed-3da3-46f1-82b7-ccdf3bb4def6", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a35e0dcc410a63077697aa22c0cb9626d041282e486f27cfbe5bb2bae49c5b8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dab3435e-7d10-4def-a8ba-4287f2b7802f", "node_type": "1", "metadata": {}, "hash": "4cd2fcb1beb77f1e522bb8af50062ca139931b9bc18bfe742e4cea18368bd87e", "class_name": "RelatedNodeInfo"}}, "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing , pages 2383\u20132392, Austin,\nTexas. Association for Computational Linguistics.\nSylvestre-Alvise Rebuf\ufb01, Hakan Bilen, and Andrea\nVedaldi. 2017. Learning multiple visual domains\nwith residual adapters. In Advances in Neural Infor-\nmation Processing Systems , volume 30. Curran As-\nsociates, Inc.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S Gordon. 2011. Choice of plausible alterna-\ntives: An evaluation of commonsense causal reason-\ning. In 2011 AAAI Spring Symposium Series .\nAmrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and\nKarthik Sankaranarayanan. 2018. DuoRC: Towards\ncomplex language understanding with paraphrased\nreading comprehension.", "start_char_idx": 0, "end_char_idx": 880, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dab3435e-7d10-4def-a8ba-4287f2b7802f": {"__data__": {"id_": "dab3435e-7d10-4def-a8ba-4287f2b7802f", "embedding": null, "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "73d9fa8bf5100471ac4eabaebbb21f397c030a5c1df11fcf79ea5e796617ebe8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4b9faf5-0f15-4da3-b0af-9070ab8c918f", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "7216cdca633d749eba10c9253c71831cc74e3506f04fa82cc431687e69e89406", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba1329eb-dd87-47d3-acb2-f06627b0a070", "node_type": "1", "metadata": {}, "hash": "8be2496788da5084678dc5056e51005c737885f5ad6aa51b967dabb7d8b992be", "class_name": "RelatedNodeInfo"}}, "text": "Association for Computational Linguistics.\nSylvestre-Alvise Rebuf\ufb01, Hakan Bilen, and Andrea\nVedaldi. 2017. Learning multiple visual domains\nwith residual adapters. In Advances in Neural Infor-\nmation Processing Systems , volume 30. Curran As-\nsociates, Inc.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S Gordon. 2011. Choice of plausible alterna-\ntives: An evaluation of commonsense causal reason-\ning. In 2011 AAAI Spring Symposium Series .\nAmrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and\nKarthik Sankaranarayanan. 2018. DuoRC: Towards\ncomplex language understanding with paraphrased\nreading comprehension. In Proceedings of the 56th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers) , pages\n1683\u20131693, Melbourne, Australia. Association for\nComputational Linguistics.\nTimo Schick and Hinrich Sch\u00fctze. 2021.", "start_char_idx": 260, "end_char_idx": 1120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba1329eb-dd87-47d3-acb2-f06627b0a070": {"__data__": {"id_": "ba1329eb-dd87-47d3-acb2-f06627b0a070", "embedding": null, "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "73d9fa8bf5100471ac4eabaebbb21f397c030a5c1df11fcf79ea5e796617ebe8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dab3435e-7d10-4def-a8ba-4287f2b7802f", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "3b5d145bafe603a75ea805a3f43b361689d3eb43baa60598ae18ee6d8fd35728", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "072e28c3-53dd-4b61-bab1-4c56b8f158b2", "node_type": "1", "metadata": {}, "hash": "4d2bf08a0027684fddb5d14b34ae27e4945f6bbdc1dbdd795a89d82291ee96e6", "class_name": "RelatedNodeInfo"}}, "text": "Curran As-\nsociates, Inc.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S Gordon. 2011. Choice of plausible alterna-\ntives: An evaluation of commonsense causal reason-\ning. In 2011 AAAI Spring Symposium Series .\nAmrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and\nKarthik Sankaranarayanan. 2018. DuoRC: Towards\ncomplex language understanding with paraphrased\nreading comprehension. In Proceedings of the 56th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers) , pages\n1683\u20131693, Melbourne, Australia. Association for\nComputational Linguistics.\nTimo Schick and Hinrich Sch\u00fctze. 2021. Exploiting\ncloze-questions for few-shot text classi\ufb01cation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255\u2013269, Online. Association for Com-\nputational Linguistics.\nNoam Shazeer.", "start_char_idx": 492, "end_char_idx": 1422, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "072e28c3-53dd-4b61-bab1-4c56b8f158b2": {"__data__": {"id_": "072e28c3-53dd-4b61-bab1-4c56b8f158b2", "embedding": null, "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "73d9fa8bf5100471ac4eabaebbb21f397c030a5c1df11fcf79ea5e796617ebe8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba1329eb-dd87-47d3-acb2-f06627b0a070", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "7c911f032c74cc1e9fbb324d601d995087365416c10d891cd2e6981678df122b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "750432d5-3963-4160-aca2-504a84d29755", "node_type": "1", "metadata": {}, "hash": "778ac4839d8639d3224e61b4d032b927e9b75f7e86134148b01bc2753c22976f", "class_name": "RelatedNodeInfo"}}, "text": "Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and\nKarthik Sankaranarayanan. 2018. DuoRC: Towards\ncomplex language understanding with paraphrased\nreading comprehension. In Proceedings of the 56th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers) , pages\n1683\u20131693, Melbourne, Australia. Association for\nComputational Linguistics.\nTimo Schick and Hinrich Sch\u00fctze. 2021. Exploiting\ncloze-questions for few-shot text classi\ufb01cation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255\u2013269, Online. Association for Com-\nputational Linguistics.\nNoam Shazeer. 2020. GLU variants improve trans-\nformer. CoRR , abs/2002.05202.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.", "start_char_idx": 710, "end_char_idx": 1589, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "750432d5-3963-4160-aca2-504a84d29755": {"__data__": {"id_": "750432d5-3963-4160-aca2-504a84d29755", "embedding": null, "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "73d9fa8bf5100471ac4eabaebbb21f397c030a5c1df11fcf79ea5e796617ebe8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "072e28c3-53dd-4b61-bab1-4c56b8f158b2", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "1325ca5853219d3af12d7707fb87ec9a35e6b64381bb299dd031debe0471627d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "858e6d4f-4cb9-452a-bc08-5e275ca9cb74", "node_type": "1", "metadata": {}, "hash": "e2d268d773ba9078ff00b6d6414720c89dee1387e44332d3c865389bff9a0605", "class_name": "RelatedNodeInfo"}}, "text": "In Proceedings of the 56th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers) , pages\n1683\u20131693, Melbourne, Australia. Association for\nComputational Linguistics.\nTimo Schick and Hinrich Sch\u00fctze. 2021. Exploiting\ncloze-questions for few-shot text classi\ufb01cation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255\u2013269, Online. Association for Com-\nputational Linguistics.\nNoam Shazeer. 2020. GLU variants improve trans-\nformer. CoRR , abs/2002.05202.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nInProceedings of the 35th International Conference\non Machine Learning , volume 80 of Proceedings\nof Machine Learning Research , pages 4596\u20134604.\nPMLR.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020.", "start_char_idx": 881, "end_char_idx": 1830, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "858e6d4f-4cb9-452a-bc08-5e275ca9cb74": {"__data__": {"id_": "858e6d4f-4cb9-452a-bc08-5e275ca9cb74", "embedding": null, "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "73d9fa8bf5100471ac4eabaebbb21f397c030a5c1df11fcf79ea5e796617ebe8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "750432d5-3963-4160-aca2-504a84d29755", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "f2d56244aa51be290fea69fb119523fe5ecfe8aa17229e69b136b3d90df65264", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38ff3acd-f6e8-4583-be08-942c089b333f", "node_type": "1", "metadata": {}, "hash": "ae0f6d0ff047083e4445112b0313cf4e9b9a6016257e0f7bc7bc8bd87aa36f4f", "class_name": "RelatedNodeInfo"}}, "text": "2021. Exploiting\ncloze-questions for few-shot text classi\ufb01cation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255\u2013269, Online. Association for Com-\nputational Linguistics.\nNoam Shazeer. 2020. GLU variants improve trans-\nformer. CoRR , abs/2002.05202.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nInProceedings of the 35th International Conference\non Machine Learning , volume 80 of Proceedings\nof Machine Learning Research , pages 4596\u20134604.\nPMLR.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4222\u20134235, Online. Association for Computational\nLinguistics.", "start_char_idx": 1115, "end_char_idx": 2091, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38ff3acd-f6e8-4583-be08-942c089b333f": {"__data__": {"id_": "38ff3acd-f6e8-4583-be08-942c089b333f", "embedding": null, "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "73d9fa8bf5100471ac4eabaebbb21f397c030a5c1df11fcf79ea5e796617ebe8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "858e6d4f-4cb9-452a-bc08-5e275ca9cb74", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "ea30809098b866b10ea09739c25430ec255186d0924daf940a13b22f4d58221a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13a375ac-50af-4d49-a8ca-ff69e6942061", "node_type": "1", "metadata": {}, "hash": "7a5572c0f6f23d67589bff8ed1569978e9edb1cc64d7f4517e8147990a7b1d74", "class_name": "RelatedNodeInfo"}}, "text": "Association for Com-\nputational Linguistics.\nNoam Shazeer. 2020. GLU variants improve trans-\nformer. CoRR , abs/2002.05202.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nInProceedings of the 35th International Conference\non Machine Learning , volume 80 of Proceedings\nof Machine Learning Research , pages 4596\u20134604.\nPMLR.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4222\u20134235, Online. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need.", "start_char_idx": 1364, "end_char_idx": 2249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13a375ac-50af-4d49-a8ca-ff69e6942061": {"__data__": {"id_": "13a375ac-50af-4d49-a8ca-ff69e6942061", "embedding": null, "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "73d9fa8bf5100471ac4eabaebbb21f397c030a5c1df11fcf79ea5e796617ebe8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38ff3acd-f6e8-4583-be08-942c089b333f", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "74b355a95aabb04e50de1b40bd46e717a7731146c528175c61f9fa4096358f1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "859b5f2e-8876-4f04-8d41-d74b3515be3b", "node_type": "1", "metadata": {}, "hash": "9fd72ae637b616fc331ffc39ab192b6be88cb03a78bdab04edd04c9dbeac36d1", "class_name": "RelatedNodeInfo"}}, "text": "InProceedings of the 35th International Conference\non Machine Learning , volume 80 of Proceedings\nof Machine Learning Research , pages 4596\u20134604.\nPMLR.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4222\u20134235, Online. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems , volume 30, pages 5998\u20136008.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019a.", "start_char_idx": 1590, "end_char_idx": 2462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "859b5f2e-8876-4f04-8d41-d74b3515be3b": {"__data__": {"id_": "859b5f2e-8876-4f04-8d41-d74b3515be3b", "embedding": null, "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "73d9fa8bf5100471ac4eabaebbb21f397c030a5c1df11fcf79ea5e796617ebe8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13a375ac-50af-4d49-a8ca-ff69e6942061", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "9136721b7e6fe2703c0a742a9a63dc5913ec66b0772147f1c0639b5b355ef7e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5759d082-e98b-40f2-9d57-5a4bf1c97e75", "node_type": "1", "metadata": {}, "hash": "eac855bd535f23e9aff46efa72e1acd5a35df7c68d0de1735b309eaa5219fb0d", "class_name": "RelatedNodeInfo"}}, "text": "2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4222\u20134235, Online. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems , volume 30, pages 5998\u20136008.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019a. SuperGLUE: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in Neural Infor-\nmation Processing Systems , volume 32.", "start_char_idx": 1825, "end_char_idx": 2616, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5759d082-e98b-40f2-9d57-5a4bf1c97e75": {"__data__": {"id_": "5759d082-e98b-40f2-9d57-5a4bf1c97e75", "embedding": null, "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "73d9fa8bf5100471ac4eabaebbb21f397c030a5c1df11fcf79ea5e796617ebe8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "859b5f2e-8876-4f04-8d41-d74b3515be3b", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "ac0b5480829cfce0cc5b25795016bb2bab859de8c30025ea7930c8b7575cab24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2deb1e91-700c-4e43-a86e-eb461dda5c30", "node_type": "1", "metadata": {}, "hash": "6b3c87fb57d608b8524450a4157476d656cfe6e7e8886b28fa2daa61e0493bf2", "class_name": "RelatedNodeInfo"}}, "text": "Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems , volume 30, pages 5998\u20136008.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019a. SuperGLUE: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in Neural Infor-\nmation Processing Systems , volume 32. Curran As-\nsociates, Inc.Alex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In the Pro-\nceedings of ICLR.\nMichael L. Waskom. 2021. seaborn: statistical data\nvisualization.", "start_char_idx": 2049, "end_char_idx": 2922, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2deb1e91-700c-4e43-a86e-eb461dda5c30": {"__data__": {"id_": "2deb1e91-700c-4e43-a86e-eb461dda5c30", "embedding": null, "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "73d9fa8bf5100471ac4eabaebbb21f397c030a5c1df11fcf79ea5e796617ebe8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5759d082-e98b-40f2-9d57-5a4bf1c97e75", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "f915cc6ead576f4f5b9a014db5047b3e29b6a72d624ac0f1c030c1d33d9fa0fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95277ebd-ef74-49c4-8d5d-d0e3b85dd531", "node_type": "1", "metadata": {}, "hash": "03ac1922a1b71bf8477afbc8c86768cec92bfe3fe51365ddc4c8e7c6f67434e8", "class_name": "RelatedNodeInfo"}}, "text": "In Advances in Neural Information Pro-\ncessing Systems , volume 30, pages 5998\u20136008.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019a. SuperGLUE: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in Neural Infor-\nmation Processing Systems , volume 32. Curran As-\nsociates, Inc.Alex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In the Pro-\nceedings of ICLR.\nMichael L. Waskom. 2021. seaborn: statistical data\nvisualization. Journal of Open Source Software ,\n6(60):3021.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nReCoRD: Bridging the gap between human and ma-\nchine commonsense reading comprehension.", "start_char_idx": 2250, "end_char_idx": 3152, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95277ebd-ef74-49c4-8d5d-d0e3b85dd531": {"__data__": {"id_": "95277ebd-ef74-49c4-8d5d-d0e3b85dd531", "embedding": null, "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd01a0da-e11d-4095-b518-a3aebffc89eb", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "73d9fa8bf5100471ac4eabaebbb21f397c030a5c1df11fcf79ea5e796617ebe8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2deb1e91-700c-4e43-a86e-eb461dda5c30", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4d8957996a9ec060e8a2c63e0871cb0970c9ecaf92029c496142b4839823005c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee7f0fd2-eb8b-4a40-a3c4-62067cf8181a", "node_type": "1", "metadata": {}, "hash": "cc8dc85a9610afd86cd131f6a91ef76c6efec6dc62616a7667e1d98d04afe043", "class_name": "RelatedNodeInfo"}}, "text": "2019a. SuperGLUE: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in Neural Infor-\nmation Processing Systems , volume 32. Curran As-\nsociates, Inc.Alex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In the Pro-\nceedings of ICLR.\nMichael L. Waskom. 2021. seaborn: statistical data\nvisualization. Journal of Open Source Software ,\n6(60):3021.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nReCoRD: Bridging the gap between human and ma-\nchine commonsense reading comprehension. CoRR ,\nabs/1810.12885.", "start_char_idx": 2456, "end_char_idx": 3175, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee7f0fd2-eb8b-4a40-a3c4-62067cf8181a": {"__data__": {"id_": "ee7f0fd2-eb8b-4a40-a3c4-62067cf8181a", "embedding": null, "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85737b80-6c4a-445e-b173-8c9277e49204", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d9db6be5a9fd5547c873968f57fe09d7b8c22f23c70f384c38d503c102be7349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95277ebd-ef74-49c4-8d5d-d0e3b85dd531", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "fc5111b021761c6cc7ead7903034143e5d2f03a17dc7a606d435867b688a3c64", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "20d922a1-47d7-4c52-ac2f-4796b063d8f9", "node_type": "1", "metadata": {}, "hash": "bc0f8f7c4926ada9446857e7d3bcb4a492e38ee11b325955c6d63c8ac476f953", "class_name": "RelatedNodeInfo"}}, "text": "A Reproducibility\nA.1 Experimental Settings\nWe evaluate each GLUE and SuperGLUE dataset\nusing the metric speci\ufb01ed in the benchmark. We\nreuse the evaluation code from the publicly avail-\nable T5 open-source release to compute metrics.12\nFor the SQuAD and MRQA datasets, we evaluate\nusing F1, one of the metrics used by the SQuAD\nbenchmark, where partial answer spans are consid-\nered. Again, we use the T5 open-source release for\nmetric calculation.13All of our models use T5 1.1\nas the base frozen model, additional details and pre-\ntrained checkpoints can be found on GitHub.1415\nAll prompts for T5 Small and Base models were\ntrained on 4TPU v2 chips, while prompts for larger\nmodels were trained on 16TPU v3 chips.\nParameter counts for each prompt can be found\nin Table 4. Average runtimes until convergence can\nbe found in Table 5.", "start_char_idx": 0, "end_char_idx": 834, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20d922a1-47d7-4c52-ac2f-4796b063d8f9": {"__data__": {"id_": "20d922a1-47d7-4c52-ac2f-4796b063d8f9", "embedding": null, "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85737b80-6c4a-445e-b173-8c9277e49204", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d9db6be5a9fd5547c873968f57fe09d7b8c22f23c70f384c38d503c102be7349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee7f0fd2-eb8b-4a40-a3c4-62067cf8181a", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "07bfaa38c49780b0490d2da20d6be94aa812f1834dd2ff8f1fe18f354845f28d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7078776-4551-4944-ba1e-8b1ec24cd9e9", "node_type": "1", "metadata": {}, "hash": "1d4d01ec2b8f5177a04081bf722d988dc99ba6a22450fdae21595c4eb7b455b3", "class_name": "RelatedNodeInfo"}}, "text": "We\nreuse the evaluation code from the publicly avail-\nable T5 open-source release to compute metrics.12\nFor the SQuAD and MRQA datasets, we evaluate\nusing F1, one of the metrics used by the SQuAD\nbenchmark, where partial answer spans are consid-\nered. Again, we use the T5 open-source release for\nmetric calculation.13All of our models use T5 1.1\nas the base frozen model, additional details and pre-\ntrained checkpoints can be found on GitHub.1415\nAll prompts for T5 Small and Base models were\ntrained on 4TPU v2 chips, while prompts for larger\nmodels were trained on 16TPU v3 chips.\nParameter counts for each prompt can be found\nin Table 4. Average runtimes until convergence can\nbe found in Table 5.\nA.2 Hyperparameter Search\nThis work used 77hyperparameter search trials\n(40for prompt tuning and 37for single-task model\ntuning), and 3training runs (with validation evalu-\nation) for each baseline con\ufb01guration and ablation\nsetting, for a total of 195runs for our main result\nand ablations.", "start_char_idx": 132, "end_char_idx": 1125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7078776-4551-4944-ba1e-8b1ec24cd9e9": {"__data__": {"id_": "a7078776-4551-4944-ba1e-8b1ec24cd9e9", "embedding": null, "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85737b80-6c4a-445e-b173-8c9277e49204", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d9db6be5a9fd5547c873968f57fe09d7b8c22f23c70f384c38d503c102be7349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20d922a1-47d7-4c52-ac2f-4796b063d8f9", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "427f3e035e186b596b96d4680ce0c6942ac3ea850daa7da726af291af9517ff4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17f82a1d-2aee-4478-ac8f-013105a4b701", "node_type": "1", "metadata": {}, "hash": "9d3ae102b80b2ca8f59e1607c774ecf197a1c675ee7702f8a6f2cb674ad4b826", "class_name": "RelatedNodeInfo"}}, "text": "Again, we use the T5 open-source release for\nmetric calculation.13All of our models use T5 1.1\nas the base frozen model, additional details and pre-\ntrained checkpoints can be found on GitHub.1415\nAll prompts for T5 Small and Base models were\ntrained on 4TPU v2 chips, while prompts for larger\nmodels were trained on 16TPU v3 chips.\nParameter counts for each prompt can be found\nin Table 4. Average runtimes until convergence can\nbe found in Table 5.\nA.2 Hyperparameter Search\nThis work used 77hyperparameter search trials\n(40for prompt tuning and 37for single-task model\ntuning), and 3training runs (with validation evalu-\nation) for each baseline con\ufb01guration and ablation\nsetting, for a total of 195runs for our main result\nand ablations. There were an additional 18runs for\nthe domain shift experiments and 24extra runs to\ncreate the ensemble. Hyperparameter bounds can\nbe found in Table 6. Hyperparameter tuning was\ndone via manual tuning and settings were selected\nbased on the SuperGLUE score.", "start_char_idx": 384, "end_char_idx": 1384, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17f82a1d-2aee-4478-ac8f-013105a4b701": {"__data__": {"id_": "17f82a1d-2aee-4478-ac8f-013105a4b701", "embedding": null, "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85737b80-6c4a-445e-b173-8c9277e49204", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d9db6be5a9fd5547c873968f57fe09d7b8c22f23c70f384c38d503c102be7349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7078776-4551-4944-ba1e-8b1ec24cd9e9", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "7ca2b3f0f6a75466e71fcf21b3de730a15fd50cd8ec442c028e49105e3622a48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f177db70-122b-4602-9ab4-cd7f3ece95d5", "node_type": "1", "metadata": {}, "hash": "a23f372d61abd5b1e86aa27e7b0ff0ee1872418a2e33bd155c50fbb3efca1a47", "class_name": "RelatedNodeInfo"}}, "text": "Parameter counts for each prompt can be found\nin Table 4. Average runtimes until convergence can\nbe found in Table 5.\nA.2 Hyperparameter Search\nThis work used 77hyperparameter search trials\n(40for prompt tuning and 37for single-task model\ntuning), and 3training runs (with validation evalu-\nation) for each baseline con\ufb01guration and ablation\nsetting, for a total of 195runs for our main result\nand ablations. There were an additional 18runs for\nthe domain shift experiments and 24extra runs to\ncreate the ensemble. Hyperparameter bounds can\nbe found in Table 6. Hyperparameter tuning was\ndone via manual tuning and settings were selected\nbased on the SuperGLUE score. All experiments in\nthis work, outside of the hyperparameter being ab-\nlated, use our default con\ufb01guration of 100K steps\nof LM Adapation, a prompt length of 100, and\n\u201cclass-label\u201d initialization.\nAll graphs of our experimental results plot the\nmean and standard deviation over 3 runs as com-\nputed by Seaborn (Waskom, 2021).", "start_char_idx": 717, "end_char_idx": 1708, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f177db70-122b-4602-9ab4-cd7f3ece95d5": {"__data__": {"id_": "f177db70-122b-4602-9ab4-cd7f3ece95d5", "embedding": null, "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85737b80-6c4a-445e-b173-8c9277e49204", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d9db6be5a9fd5547c873968f57fe09d7b8c22f23c70f384c38d503c102be7349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17f82a1d-2aee-4478-ac8f-013105a4b701", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "9889811f66551a2f5abd1d6795206a032a217d0adcf607963dec10fcdb26d5b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6189ab3c-eb2e-46a8-9ef4-a2c873357e56", "node_type": "1", "metadata": {}, "hash": "88db8b4f84091d988050b141b889a818074de8b3d1d82842382cd27ed26f2844", "class_name": "RelatedNodeInfo"}}, "text": "There were an additional 18runs for\nthe domain shift experiments and 24extra runs to\ncreate the ensemble. Hyperparameter bounds can\nbe found in Table 6. Hyperparameter tuning was\ndone via manual tuning and settings were selected\nbased on the SuperGLUE score. All experiments in\nthis work, outside of the hyperparameter being ab-\nlated, use our default con\ufb01guration of 100K steps\nof LM Adapation, a prompt length of 100, and\n\u201cclass-label\u201d initialization.\nAll graphs of our experimental results plot the\nmean and standard deviation over 3 runs as com-\nputed by Seaborn (Waskom, 2021). Some settings\nhave such low variance that the standard deviation\n12https://github.com/google-research/\ntext-to-text-transfer-transformer/blob/\nmaster/t5/evaluation/metrics.py\n13https://github.com/google-research/\ntext-to-text-transfer-transformer/blob/\nmaster/t5/evaluation/metrics.py#L151\n14https://github.com/google-research/\ntext-to-text-transfer-transformer/blob/\nmaster/released_checkpoints.md#t511\n15https://github.com/google-research/\ntext-to-text-transfer-transformer/\nblob/main/released_checkpoints.md#\nlm-adapted-t511lm100kis hidden behind the line itself, such as \u201cModel Tun-\ning (Multi-task)\u201d in Figure 1 and the Base, Large,\nand XL prompts trained on the \u201cSpan Corruption\u201d\npretraining objective in Figure 3(b).", "start_char_idx": 1126, "end_char_idx": 2432, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6189ab3c-eb2e-46a8-9ef4-a2c873357e56": {"__data__": {"id_": "6189ab3c-eb2e-46a8-9ef4-a2c873357e56", "embedding": null, "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85737b80-6c4a-445e-b173-8c9277e49204", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d9db6be5a9fd5547c873968f57fe09d7b8c22f23c70f384c38d503c102be7349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f177db70-122b-4602-9ab4-cd7f3ece95d5", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "b33c93df1b576dbe00dfb417a04dbf68799367cda14b060b03db445e397e398b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8247b101-90cc-4661-8f72-78a888d708c7", "node_type": "1", "metadata": {}, "hash": "c05488b68d929765e48dbf8ba8d10869a4a9e30ea5f952524d4247f7117bf04e", "class_name": "RelatedNodeInfo"}}, "text": "Some settings\nhave such low variance that the standard deviation\n12https://github.com/google-research/\ntext-to-text-transfer-transformer/blob/\nmaster/t5/evaluation/metrics.py\n13https://github.com/google-research/\ntext-to-text-transfer-transformer/blob/\nmaster/t5/evaluation/metrics.py#L151\n14https://github.com/google-research/\ntext-to-text-transfer-transformer/blob/\nmaster/released_checkpoints.md#t511\n15https://github.com/google-research/\ntext-to-text-transfer-transformer/\nblob/main/released_checkpoints.md#\nlm-adapted-t511lm100kis hidden behind the line itself, such as \u201cModel Tun-\ning (Multi-task)\u201d in Figure 1 and the Base, Large,\nand XL prompts trained on the \u201cSpan Corruption\u201d\npretraining objective in Figure 3(b). Figure 4 also\nshows mean and standard deviation for the num-\nber of parameters each method uses as the prompt\nlength varies from 1\u2013100. The \u201cPre\ufb01x Tuning\n(Train)\u201d curves appears to have no standard de-\nviation because the parameter count is so strongly\ndominated by the cost of the reparameterization\nparameters that the standard deviation bands are\noccluded.", "start_char_idx": 1709, "end_char_idx": 2792, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8247b101-90cc-4661-8f72-78a888d708c7": {"__data__": {"id_": "8247b101-90cc-4661-8f72-78a888d708c7", "embedding": null, "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85737b80-6c4a-445e-b173-8c9277e49204", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d9db6be5a9fd5547c873968f57fe09d7b8c22f23c70f384c38d503c102be7349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6189ab3c-eb2e-46a8-9ef4-a2c873357e56", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "59ed19c40a9611124eea6280367d7f9979a48fb37f17148557f5f20a13e6f0e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "562f8e26-f9be-4c6f-a83a-bfe6195546ec", "node_type": "1", "metadata": {}, "hash": "eb5931788671f6b3d857f8acf482f0315ab11fea941f4c6f0eca983c83993db6", "class_name": "RelatedNodeInfo"}}, "text": "Figure 4 also\nshows mean and standard deviation for the num-\nber of parameters each method uses as the prompt\nlength varies from 1\u2013100. The \u201cPre\ufb01x Tuning\n(Train)\u201d curves appears to have no standard de-\nviation because the parameter count is so strongly\ndominated by the cost of the reparameterization\nparameters that the standard deviation bands are\noccluded. For our experiments on domain transfer,\nwe report mean and standard deviation over 3 runs.\nA.3 Datasets\nAll datasets used are in English. For the GLUE16,17\nand SuperGLUE18datasets, we used the training,\nvalidation, and test splits that ship with TensorFlow\nDatasets. We used version 1.0.0 for GLUE and\n1.0.2 for SuperGLUE datasets. For SQuAD19\nwe used v1.1:3.0.0 from Tensor\ufb02ow Datasets\nand follow the provided training, validation, and\ntest splits. For the out-of-domain datasets we used\nthe development splits distributed as part of the\nMRQA shared task.20Dataset sizes can be found\nin Table 7.", "start_char_idx": 2433, "end_char_idx": 3389, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "562f8e26-f9be-4c6f-a83a-bfe6195546ec": {"__data__": {"id_": "562f8e26-f9be-4c6f-a83a-bfe6195546ec", "embedding": null, "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85737b80-6c4a-445e-b173-8c9277e49204", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d9db6be5a9fd5547c873968f57fe09d7b8c22f23c70f384c38d503c102be7349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8247b101-90cc-4661-8f72-78a888d708c7", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "82ced87bf2e394755bb6773dbae2a6eb6d9833ca4c4ce5ce40079d2a148a5c67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7949cd4f-c5e9-437d-84f4-462bc1211f3f", "node_type": "1", "metadata": {}, "hash": "1fef78073243166cfb125f4b987dc0c1c3de5f3b81e0110136af055df0fcb71f", "class_name": "RelatedNodeInfo"}}, "text": "For our experiments on domain transfer,\nwe report mean and standard deviation over 3 runs.\nA.3 Datasets\nAll datasets used are in English. For the GLUE16,17\nand SuperGLUE18datasets, we used the training,\nvalidation, and test splits that ship with TensorFlow\nDatasets. We used version 1.0.0 for GLUE and\n1.0.2 for SuperGLUE datasets. For SQuAD19\nwe used v1.1:3.0.0 from Tensor\ufb02ow Datasets\nand follow the provided training, validation, and\ntest splits. For the out-of-domain datasets we used\nthe development splits distributed as part of the\nMRQA shared task.20Dataset sizes can be found\nin Table 7. The label distributions for each dataset\ncan be found in Table 8 (BoolQ), Table 9 (CB),\nTable 10 (COPA), Table 11 (MultiRC), Table 14\n(RTE), Table 12 (WiC), Table 13 (WSC), Table 15\n(MRPC) and Table 16 (QQP).", "start_char_idx": 2793, "end_char_idx": 3598, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7949cd4f-c5e9-437d-84f4-462bc1211f3f": {"__data__": {"id_": "7949cd4f-c5e9-437d-84f4-462bc1211f3f", "embedding": null, "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85737b80-6c4a-445e-b173-8c9277e49204", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d9db6be5a9fd5547c873968f57fe09d7b8c22f23c70f384c38d503c102be7349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "562f8e26-f9be-4c6f-a83a-bfe6195546ec", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e675e630612a35d43c66ac099204dcbef2ac5361cbb02079b6a8cb5ffb3647c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "050b11a9-35b3-4a5b-abfe-6a90536f68c5", "node_type": "1", "metadata": {}, "hash": "0385e8c3b169121e7d961e865aa7c63d21b1ba6598a644d85e40ae21e430f01f", "class_name": "RelatedNodeInfo"}}, "text": "We used version 1.0.0 for GLUE and\n1.0.2 for SuperGLUE datasets. For SQuAD19\nwe used v1.1:3.0.0 from Tensor\ufb02ow Datasets\nand follow the provided training, validation, and\ntest splits. For the out-of-domain datasets we used\nthe development splits distributed as part of the\nMRQA shared task.20Dataset sizes can be found\nin Table 7. The label distributions for each dataset\ncan be found in Table 8 (BoolQ), Table 9 (CB),\nTable 10 (COPA), Table 11 (MultiRC), Table 14\n(RTE), Table 12 (WiC), Table 13 (WSC), Table 15\n(MRPC) and Table 16 (QQP).\nThe question answering datasets are extractive\ndatasets with a variety of answers, so there isn\u2019t a\nlabel distribution to report. Similarly, the ReCoRD\ndataset is a multiple choice dataset where the model\nmust predict the masked out entity from a list of\npossible entities. Due to this formulation there isn\u2019t\na meaningful label distribution.", "start_char_idx": 3060, "end_char_idx": 3941, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "050b11a9-35b3-4a5b-abfe-6a90536f68c5": {"__data__": {"id_": "050b11a9-35b3-4a5b-abfe-6a90536f68c5", "embedding": null, "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85737b80-6c4a-445e-b173-8c9277e49204", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d9db6be5a9fd5547c873968f57fe09d7b8c22f23c70f384c38d503c102be7349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7949cd4f-c5e9-437d-84f4-462bc1211f3f", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "85309c14c5e4bfcb9460b25ed101a71a4edcae18551d4e84337d0d211e7e2860", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f38bc9b7-f176-4299-980d-e2ec65b1083f", "node_type": "1", "metadata": {}, "hash": "f30e17f32be034700c224a06b96a5dd8363c10bdeaebbe487c6e3f159e339638", "class_name": "RelatedNodeInfo"}}, "text": "For the out-of-domain datasets we used\nthe development splits distributed as part of the\nMRQA shared task.20Dataset sizes can be found\nin Table 7. The label distributions for each dataset\ncan be found in Table 8 (BoolQ), Table 9 (CB),\nTable 10 (COPA), Table 11 (MultiRC), Table 14\n(RTE), Table 12 (WiC), Table 13 (WSC), Table 15\n(MRPC) and Table 16 (QQP).\nThe question answering datasets are extractive\ndatasets with a variety of answers, so there isn\u2019t a\nlabel distribution to report. Similarly, the ReCoRD\ndataset is a multiple choice dataset where the model\nmust predict the masked out entity from a list of\npossible entities. Due to this formulation there isn\u2019t\na meaningful label distribution.\nWe followed the open-source T5 preprocess-\ning procedure21for each dataset, except that we\nomit the dataset pre\ufb01x denoting which SuperGLUE\ndataset an example belongs to.", "start_char_idx": 3243, "end_char_idx": 4111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f38bc9b7-f176-4299-980d-e2ec65b1083f": {"__data__": {"id_": "f38bc9b7-f176-4299-980d-e2ec65b1083f", "embedding": null, "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85737b80-6c4a-445e-b173-8c9277e49204", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "d9db6be5a9fd5547c873968f57fe09d7b8c22f23c70f384c38d503c102be7349", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "050b11a9-35b3-4a5b-abfe-6a90536f68c5", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "55bbf8572b546c998a2354d4d2932323320127d4b91c244febce0b35aa9a3f04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4605c950-9913-4c4f-a6be-a97671cf5cdb", "node_type": "1", "metadata": {}, "hash": "549b6bb611e40cff39ccc3dcd71c744d0c0c31a91c97f61e03d4c50eb65c2d7c", "class_name": "RelatedNodeInfo"}}, "text": "The label distributions for each dataset\ncan be found in Table 8 (BoolQ), Table 9 (CB),\nTable 10 (COPA), Table 11 (MultiRC), Table 14\n(RTE), Table 12 (WiC), Table 13 (WSC), Table 15\n(MRPC) and Table 16 (QQP).\nThe question answering datasets are extractive\ndatasets with a variety of answers, so there isn\u2019t a\nlabel distribution to report. Similarly, the ReCoRD\ndataset is a multiple choice dataset where the model\nmust predict the masked out entity from a list of\npossible entities. Due to this formulation there isn\u2019t\na meaningful label distribution.\nWe followed the open-source T5 preprocess-\ning procedure21for each dataset, except that we\nomit the dataset pre\ufb01x denoting which SuperGLUE\ndataset an example belongs to. For the SQuAD and\n16https://www.tensorflow.org/datasets/\ncatalog/glue#gluemrpc\n17https://www.tensorflow.org/datasets/\ncatalog/glue#glueqqp\n18https://www.tensorflow.org/datasets/\ncatalog/super_glue\n19https://www.tensorflow.org/datasets/\ncatalog/squad#squadv11_default_config\n20https://github.com/mrqa/\nMRQA-Shared-Task-2019#out-of-domain\n21https://github.com/google-research/\ntext-to-text-transfer-transformer/blob/\nmaster/t5/data/preprocessors.py", "start_char_idx": 3390, "end_char_idx": 4558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4605c950-9913-4c4f-a6be-a97671cf5cdb": {"__data__": {"id_": "4605c950-9913-4c4f-a6be-a97671cf5cdb", "embedding": null, "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "413315cd-b11e-4878-a2b9-e653548db0f6", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e0e069e9ca3b22ce573393121a17287569ea2db6bf5d45a98bf4fcc268494792", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f38bc9b7-f176-4299-980d-e2ec65b1083f", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "66410bb9ad51b5d679e3f9d7c02a26259b6638b13a87c54c745552fcb130d58b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f751593-1b85-4d9c-a8bb-0e4176fbeac7", "node_type": "1", "metadata": {}, "hash": "a761c46fd55de8ebc5e61a311547194dad446f0866cf217515f223426940bda8", "class_name": "RelatedNodeInfo"}}, "text": "T5 Size Prompt Length Trainable Parameters Total Parameters Percent Trainable\nSmall 1 512 76 ,961,664 0 .00067 %\n5 2,560 76 ,963,712 0 .00333 %\n20 10,420 76 ,971,572 0 .01330 %\n50 25,600 76 ,986,752 0 .03325 %\n100 51,200 77 ,012,352 0 .06648 %\n150 76,800 77 ,037,952 0 .09969 %\nBase 1 768 247 ,578,624 0 .00031 %\n5 3,840 247 ,581,696 0 .00155 %\n20 15,360 247 ,593,216 0 .00620 %\n50 38,400 247 ,616,256 0 .01551 %\n100 76,800 247 ,654,656 0 .03101 %\n150 115,200 247 ,693,056 0 .04651 %\nLarge 1 1,024 783 ,151,104 0 .00013 %\n5 5,120 783 ,155,200 0 .00065 %\n20 20,480 783 ,170,", "start_char_idx": 0, "end_char_idx": 573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f751593-1b85-4d9c-a8bb-0e4176fbeac7": {"__data__": {"id_": "8f751593-1b85-4d9c-a8bb-0e4176fbeac7", "embedding": null, "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "413315cd-b11e-4878-a2b9-e653548db0f6", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e0e069e9ca3b22ce573393121a17287569ea2db6bf5d45a98bf4fcc268494792", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4605c950-9913-4c4f-a6be-a97671cf5cdb", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "b407420d92864ff0f61222da26d6eba3cd539b97572c95340a49f876100e0d57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2c1f884-b681-4218-b1a5-d4badc00bc61", "node_type": "1", "metadata": {}, "hash": "c7ea572985f2b501e15df58c40ab74d418831ca5db595c6e59293f1aaa264a4f", "class_name": "RelatedNodeInfo"}}, "text": "600 76 ,986,752 0 .03325 %\n100 51,200 77 ,012,352 0 .06648 %\n150 76,800 77 ,037,952 0 .09969 %\nBase 1 768 247 ,578,624 0 .00031 %\n5 3,840 247 ,581,696 0 .00155 %\n20 15,360 247 ,593,216 0 .00620 %\n50 38,400 247 ,616,256 0 .01551 %\n100 76,800 247 ,654,656 0 .03101 %\n150 115,200 247 ,693,056 0 .04651 %\nLarge 1 1,024 783 ,151,104 0 .00013 %\n5 5,120 783 ,155,200 0 .00065 %\n20 20,480 783 ,170,560 0 .00262 %\n50 51,200 783 ,201,280 0 .00654 %\n100 102,400 783 ,252,480 0 .01907 %\n150 153,600 783 ,303,680 0 .01961 %\nXL 1 2,048 2 ,849,", "start_char_idx": 183, "end_char_idx": 712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2c1f884-b681-4218-b1a5-d4badc00bc61": {"__data__": {"id_": "b2c1f884-b681-4218-b1a5-d4badc00bc61", "embedding": null, "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "413315cd-b11e-4878-a2b9-e653548db0f6", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e0e069e9ca3b22ce573393121a17287569ea2db6bf5d45a98bf4fcc268494792", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f751593-1b85-4d9c-a8bb-0e4176fbeac7", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "ad2d89189703632c27d98192b450bf991a09cf3c85484084f9397e76a6c86468", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acd201aa-2168-4178-9c9d-6368e54bcecb", "node_type": "1", "metadata": {}, "hash": "ab6e598456ab5a7be6eaa11bfd361634a5737024f206fcceeda0c4b200d7a1fe", "class_name": "RelatedNodeInfo"}}, "text": "840 247 ,581,696 0 .00155 %\n20 15,360 247 ,593,216 0 .00620 %\n50 38,400 247 ,616,256 0 .01551 %\n100 76,800 247 ,654,656 0 .03101 %\n150 115,200 247 ,693,056 0 .04651 %\nLarge 1 1,024 783 ,151,104 0 .00013 %\n5 5,120 783 ,155,200 0 .00065 %\n20 20,480 783 ,170,560 0 .00262 %\n50 51,200 783 ,201,280 0 .00654 %\n100 102,400 783 ,252,480 0 .01907 %\n150 153,600 783 ,303,680 0 .01961 %\nXL 1 2,048 2 ,849,759,232 0 .00007 %\n5 10,240 2 ,849,767,424 0 .00036 %\n20 40,960 2 ,849,798,144 0 .00143 %\n50 102,400 2 ,849,859,584 0 .", "start_char_idx": 317, "end_char_idx": 831, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acd201aa-2168-4178-9c9d-6368e54bcecb": {"__data__": {"id_": "acd201aa-2168-4178-9c9d-6368e54bcecb", "embedding": null, "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "413315cd-b11e-4878-a2b9-e653548db0f6", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e0e069e9ca3b22ce573393121a17287569ea2db6bf5d45a98bf4fcc268494792", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2c1f884-b681-4218-b1a5-d4badc00bc61", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "b4732816c2aba181dba7fea899067dcc357089a9b29805ef1776d4dc1a05c5bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69d9c997-ccc9-4670-a6b2-6099224c2b49", "node_type": "1", "metadata": {}, "hash": "b248d4b56278329691c67e0f9d083e3680ad621ea3ab45f93e83824f9080b0f0", "class_name": "RelatedNodeInfo"}}, "text": "03101 %\n150 115,200 247 ,693,056 0 .04651 %\nLarge 1 1,024 783 ,151,104 0 .00013 %\n5 5,120 783 ,155,200 0 .00065 %\n20 20,480 783 ,170,560 0 .00262 %\n50 51,200 783 ,201,280 0 .00654 %\n100 102,400 783 ,252,480 0 .01907 %\n150 153,600 783 ,303,680 0 .01961 %\nXL 1 2,048 2 ,849,759,232 0 .00007 %\n5 10,240 2 ,849,767,424 0 .00036 %\n20 40,960 2 ,849,798,144 0 .00143 %\n50 102,400 2 ,849,859,584 0 .00359 %\n100 204,800 2 ,849,961,984 0 .00718 %\n150 307,200 2 ,850,064,384 0 .01078 %\nXXL 1 4,096 11 ,135,336,448 0 .00004 %\n5 20,", "start_char_idx": 440, "end_char_idx": 959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69d9c997-ccc9-4670-a6b2-6099224c2b49": {"__data__": {"id_": "69d9c997-ccc9-4670-a6b2-6099224c2b49", "embedding": null, "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "413315cd-b11e-4878-a2b9-e653548db0f6", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e0e069e9ca3b22ce573393121a17287569ea2db6bf5d45a98bf4fcc268494792", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acd201aa-2168-4178-9c9d-6368e54bcecb", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "9dd33c414c513e9487149fb9bfdf14fb62bc1c69467d55b60e96f6132e70dc8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7087c5e-dd79-404a-ac42-ec59a44b3e30", "node_type": "1", "metadata": {}, "hash": "44a12490f534f7f2b75af5054db8df3a54ca367de7763c51961235d09ba6b14e", "class_name": "RelatedNodeInfo"}}, "text": "560 0 .00262 %\n50 51,200 783 ,201,280 0 .00654 %\n100 102,400 783 ,252,480 0 .01907 %\n150 153,600 783 ,303,680 0 .01961 %\nXL 1 2,048 2 ,849,759,232 0 .00007 %\n5 10,240 2 ,849,767,424 0 .00036 %\n20 40,960 2 ,849,798,144 0 .00143 %\n50 102,400 2 ,849,859,584 0 .00359 %\n100 204,800 2 ,849,961,984 0 .00718 %\n150 307,200 2 ,850,064,384 0 .01078 %\nXXL 1 4,096 11 ,135,336,448 0 .00004 %\n5 20,480 11 ,135,352,832 0 .00018 %\n20 81,920 11 ,135,414,272 0 .00074 %\n50 204,800 11 ,137,380,352 0 .00184 %\n100 409,600 11 ,135,741,", "start_char_idx": 573, "end_char_idx": 1089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7087c5e-dd79-404a-ac42-ec59a44b3e30": {"__data__": {"id_": "a7087c5e-dd79-404a-ac42-ec59a44b3e30", "embedding": null, "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "413315cd-b11e-4878-a2b9-e653548db0f6", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e0e069e9ca3b22ce573393121a17287569ea2db6bf5d45a98bf4fcc268494792", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69d9c997-ccc9-4670-a6b2-6099224c2b49", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "a6b426494cbc40575054723b9fcabe211f1a49882c809629c1b2060b7227fc0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a546e0f-a684-43d6-9dfd-c7489a85d753", "node_type": "1", "metadata": {}, "hash": "8b8fe7df226bc104dc043558a12242de064228603fb9c05f0091b648a3143c72", "class_name": "RelatedNodeInfo"}}, "text": "849,759,232 0 .00007 %\n5 10,240 2 ,849,767,424 0 .00036 %\n20 40,960 2 ,849,798,144 0 .00143 %\n50 102,400 2 ,849,859,584 0 .00359 %\n100 204,800 2 ,849,961,984 0 .00718 %\n150 307,200 2 ,850,064,384 0 .01078 %\nXXL 1 4,096 11 ,135,336,448 0 .00004 %\n5 20,480 11 ,135,352,832 0 .00018 %\n20 81,920 11 ,135,414,272 0 .00074 %\n50 204,800 11 ,137,380,352 0 .00184 %\n100 409,600 11 ,135,741,952 0 .00368 %\n150 614,400 11 ,135,946,752 0 .00552 %\nTable 4: Number of parameters used for various prompt lengths and T5 model sizes.", "start_char_idx": 708, "end_char_idx": 1224, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a546e0f-a684-43d6-9dfd-c7489a85d753": {"__data__": {"id_": "4a546e0f-a684-43d6-9dfd-c7489a85d753", "embedding": null, "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "413315cd-b11e-4878-a2b9-e653548db0f6", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e0e069e9ca3b22ce573393121a17287569ea2db6bf5d45a98bf4fcc268494792", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7087c5e-dd79-404a-ac42-ec59a44b3e30", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "90b1fcbbfbdd6b6554664aa9904fb2616767843c408a79474e93e4fc8dfc2cfa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5077c8de-7305-4184-a9fc-ee4f5cbfa31c", "node_type": "1", "metadata": {}, "hash": "a893c86d008f77a18c7f65937af777219dabc27d2f7ed9e8497ad5d847ee28df", "class_name": "RelatedNodeInfo"}}, "text": "144 0 .00143 %\n50 102,400 2 ,849,859,584 0 .00359 %\n100 204,800 2 ,849,961,984 0 .00718 %\n150 307,200 2 ,850,064,384 0 .01078 %\nXXL 1 4,096 11 ,135,336,448 0 .00004 %\n5 20,480 11 ,135,352,832 0 .00018 %\n20 81,920 11 ,135,414,272 0 .00074 %\n50 204,800 11 ,137,380,352 0 .00184 %\n100 409,600 11 ,135,741,952 0 .00368 %\n150 614,400 11 ,135,946,752 0 .00552 %\nTable 4: Number of parameters used for various prompt lengths and T5 model sizes. Trainable parameters is\nthe number of parameters in the prompt itself, while total parameters includes the prompt plus the original T5\nparameters. The T5 parameters are frozen and shared across all tasks, and include the SentencePiece lookup table\nparameters.", "start_char_idx": 787, "end_char_idx": 1484, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5077c8de-7305-4184-a9fc-ee4f5cbfa31c": {"__data__": {"id_": "5077c8de-7305-4184-a9fc-ee4f5cbfa31c", "embedding": null, "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "413315cd-b11e-4878-a2b9-e653548db0f6", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e0e069e9ca3b22ce573393121a17287569ea2db6bf5d45a98bf4fcc268494792", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a546e0f-a684-43d6-9dfd-c7489a85d753", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "bcdf38604bf1be384db078b24197804da4e63d92c6ac3cf23b78595a1e54044b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7108d42-1b03-488f-9b45-42e9b2339bde", "node_type": "1", "metadata": {}, "hash": "00493269d2fb8469bb57c63f55b141847c72ae4e2527a00c85cc0ee496a8d007", "class_name": "RelatedNodeInfo"}}, "text": "850,064,384 0 .01078 %\nXXL 1 4,096 11 ,135,336,448 0 .00004 %\n5 20,480 11 ,135,352,832 0 .00018 %\n20 81,920 11 ,135,414,272 0 .00074 %\n50 204,800 11 ,137,380,352 0 .00184 %\n100 409,600 11 ,135,741,952 0 .00368 %\n150 614,400 11 ,135,946,752 0 .00552 %\nTable 4: Number of parameters used for various prompt lengths and T5 model sizes. Trainable parameters is\nthe number of parameters in the prompt itself, while total parameters includes the prompt plus the original T5\nparameters. The T5 parameters are frozen and shared across all tasks, and include the SentencePiece lookup table\nparameters. The \ufb01nal column is the percentage of total parameters that are trainable.\nMRQA datasets we used the T5 SQuAD prepro-\ncessing code22. By following the T5 preprocessing\nand text-to-text format, we recast the WSC dataset\nas a text generation task.", "start_char_idx": 892, "end_char_idx": 1729, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7108d42-1b03-488f-9b45-42e9b2339bde": {"__data__": {"id_": "c7108d42-1b03-488f-9b45-42e9b2339bde", "embedding": null, "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "413315cd-b11e-4878-a2b9-e653548db0f6", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e0e069e9ca3b22ce573393121a17287569ea2db6bf5d45a98bf4fcc268494792", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5077c8de-7305-4184-a9fc-ee4f5cbfa31c", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "dfc2d7ddf104c063f7a28b01bffb849a0cdf0d830b35b4bc75229183a61c7259", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05dbb448-bf7d-40ff-854b-4cc2b0cc172a", "node_type": "1", "metadata": {}, "hash": "739491ebf87ad5b4c2bf91dec3154f281b5ff48aa0baadfbe27d10b994e71adc", "class_name": "RelatedNodeInfo"}}, "text": "414,272 0 .00074 %\n50 204,800 11 ,137,380,352 0 .00184 %\n100 409,600 11 ,135,741,952 0 .00368 %\n150 614,400 11 ,135,946,752 0 .00552 %\nTable 4: Number of parameters used for various prompt lengths and T5 model sizes. Trainable parameters is\nthe number of parameters in the prompt itself, while total parameters includes the prompt plus the original T5\nparameters. The T5 parameters are frozen and shared across all tasks, and include the SentencePiece lookup table\nparameters. The \ufb01nal column is the percentage of total parameters that are trainable.\nMRQA datasets we used the T5 SQuAD prepro-\ncessing code22. By following the T5 preprocessing\nand text-to-text format, we recast the WSC dataset\nas a text generation task. Instead of predicting\nwhether a supplied referent is correct for a high-\nlighted span, our model predicts the correct referent\ndirectly.", "start_char_idx": 1008, "end_char_idx": 1866, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05dbb448-bf7d-40ff-854b-4cc2b0cc172a": {"__data__": {"id_": "05dbb448-bf7d-40ff-854b-4cc2b0cc172a", "embedding": null, "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "413315cd-b11e-4878-a2b9-e653548db0f6", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e0e069e9ca3b22ce573393121a17287569ea2db6bf5d45a98bf4fcc268494792", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7108d42-1b03-488f-9b45-42e9b2339bde", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "736ffa6607660150b0291e95a5a67cd408ae83dd87746cc51ebdbc20aa14b8cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5874d62b-383b-4ccc-9117-d2a5e44c32de", "node_type": "1", "metadata": {}, "hash": "e454169d169fdae41a4c5544ce076c1387ee8fb0346fb9752ee36b4574f00c51", "class_name": "RelatedNodeInfo"}}, "text": "600 11 ,135,741,952 0 .00368 %\n150 614,400 11 ,135,946,752 0 .00552 %\nTable 4: Number of parameters used for various prompt lengths and T5 model sizes. Trainable parameters is\nthe number of parameters in the prompt itself, while total parameters includes the prompt plus the original T5\nparameters. The T5 parameters are frozen and shared across all tasks, and include the SentencePiece lookup table\nparameters. The \ufb01nal column is the percentage of total parameters that are trainable.\nMRQA datasets we used the T5 SQuAD prepro-\ncessing code22. By following the T5 preprocessing\nand text-to-text format, we recast the WSC dataset\nas a text generation task. Instead of predicting\nwhether a supplied referent is correct for a high-\nlighted span, our model predicts the correct referent\ndirectly. As such, we can only learn from training\nexamples where the referent is correct, so WSC\ntraining data where the supplied referent is incor-\nrect are omitted.\nNo new data was collected for this work.", "start_char_idx": 1073, "end_char_idx": 2065, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5874d62b-383b-4ccc-9117-d2a5e44c32de": {"__data__": {"id_": "5874d62b-383b-4ccc-9117-d2a5e44c32de", "embedding": null, "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "413315cd-b11e-4878-a2b9-e653548db0f6", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e0e069e9ca3b22ce573393121a17287569ea2db6bf5d45a98bf4fcc268494792", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05dbb448-bf7d-40ff-854b-4cc2b0cc172a", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4e2b5273b054d28cb74f33ec7553220bc0fc8d0e80b60105b4e154eae7301e6b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04131aff-4e42-4283-94a1-09140859b23e", "node_type": "1", "metadata": {}, "hash": "e469017eeeef3b333d3f4a88098975de97b5dc2e6e9a2da46d35448788627a07", "class_name": "RelatedNodeInfo"}}, "text": "Trainable parameters is\nthe number of parameters in the prompt itself, while total parameters includes the prompt plus the original T5\nparameters. The T5 parameters are frozen and shared across all tasks, and include the SentencePiece lookup table\nparameters. The \ufb01nal column is the percentage of total parameters that are trainable.\nMRQA datasets we used the T5 SQuAD prepro-\ncessing code22. By following the T5 preprocessing\nand text-to-text format, we recast the WSC dataset\nas a text generation task. Instead of predicting\nwhether a supplied referent is correct for a high-\nlighted span, our model predicts the correct referent\ndirectly. As such, we can only learn from training\nexamples where the referent is correct, so WSC\ntraining data where the supplied referent is incor-\nrect are omitted.\nNo new data was collected for this work.\n22https://github.com/google-research/\ntext-to-text-transfer-transformer/blob/\nmaster/t5/data/preprocessors.py#L264Prompt Length T5 Size Time\n1 Large 3:17\u00b102:10\nXL 3:37\u00b102:11\nXXL 21:23\u00b101:54\n20 XL 49:08\u00b118:53\nXXL 53:03\u00b116:25\n50 Small 09:05\u00b105:07\nBase 55:01\u00b127:48\nLarge 1:14:16\u00b113:12\nXL 2:30:10\u00b125:40\nXXL 3:13:13\u00b123:08\n100 Small 16:25\u00b101:15\nBase 29:57\u00b100:18\nLarge 1:23:36\u00b110:21\nXL 3:35:00\u00b154:42\nXXL 3:51:15\u00b145:53\nTable 5: Mean and standard deviation of the runtime\nuntil convergence for the BoolQ dataset and various\nprompt lengths and model sizes.", "start_char_idx": 1225, "end_char_idx": 2612, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04131aff-4e42-4283-94a1-09140859b23e": {"__data__": {"id_": "04131aff-4e42-4283-94a1-09140859b23e", "embedding": null, "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "413315cd-b11e-4878-a2b9-e653548db0f6", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "e0e069e9ca3b22ce573393121a17287569ea2db6bf5d45a98bf4fcc268494792", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5874d62b-383b-4ccc-9117-d2a5e44c32de", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "fa02c2c31dafc13bcdc45d45109737a055ddb1712571a08f4cb72999bccc9632", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d4ce580-c085-4728-bd9f-7b321d548077", "node_type": "1", "metadata": {}, "hash": "3282063f27775f6946b8c46722efa158c589ec757cd7a31c4b8017782b5512a3", "class_name": "RelatedNodeInfo"}}, "text": "Convergence is de-\n\ufb01ned as reaching a performance within 1% of the mean\nvalue for that model con\ufb01guration. A few con\ufb01gura-\ntions have been omitted because their runtimes were\narti\ufb01cially extended due to preemption.", "start_char_idx": 2613, "end_char_idx": 2827, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d4ce580-c085-4728-bd9f-7b321d548077": {"__data__": {"id_": "1d4ce580-c085-4728-bd9f-7b321d548077", "embedding": null, "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9256579-366e-4b1d-821b-7e410b884184", "node_type": "4", "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "71badfa87b1f3c3c6d030d4c81c9923d18ec9dd7de00c195d6d2c580deaf4ef9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04131aff-4e42-4283-94a1-09140859b23e", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "4346224ff96329df4216e2fc2467a616a68621ea5f61f72ca44098abf4c85c35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6359578d-c194-43bb-a730-686f0f0680a5", "node_type": "1", "metadata": {}, "hash": "40145562bc9c9abef518e022b1aeef503cd1b70703808e09603d62e27b3bae96", "class_name": "RelatedNodeInfo"}}, "text": "Hyperparameter Search Space\nLearning Rate 0.001\u20130.5\nParameter Scaling {True, False}\nBatch Size {32,64,126,256,512}\nNumber of Steps {10,000,20,000,30,000}\nWarmup Steps {off,2,000,3,000}\nDecay Factor {off,0.1,0.5}\nSteps per Decay {off,4,000,6,000,8,000}\nTable 6: Search space for each hyperparameter consid-\nered. Parameter Scaling refers to the Adafactor setting\nwhere an update is scaled by the norm of the parameter\nit will be applied to. Warmup Steps is the number of\nsteps before a linearly increasing learning rate reaches\nthe Learning Rate value, starting from zero. Decay Fac-\ntor is the reduction in Learning Rate size that occurs\nevery \u201cSteps per Decay\u201d steps.", "start_char_idx": 0, "end_char_idx": 668, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6359578d-c194-43bb-a730-686f0f0680a5": {"__data__": {"id_": "6359578d-c194-43bb-a730-686f0f0680a5", "embedding": null, "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9256579-366e-4b1d-821b-7e410b884184", "node_type": "4", "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "71badfa87b1f3c3c6d030d4c81c9923d18ec9dd7de00c195d6d2c580deaf4ef9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d4ce580-c085-4728-bd9f-7b321d548077", "node_type": "1", "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "c12cbac0df625d18238b1c2d5f9e4f954477c938ef26294eaf8fcb1c6e045f05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3063fa75-f221-41bc-b87f-86db7e9a3ca0", "node_type": "1", "metadata": {}, "hash": "e7a0b9fffd53537a06db92b86a6cd10ee8abfc3ac37a072d63d1fce19f4f2e89", "class_name": "RelatedNodeInfo"}}, "text": "Hyperparameter Search Space\nLearning Rate 0.001\u20130.5\nParameter Scaling {True, False}\nBatch Size {32,64,126,256,512}\nNumber of Steps {10,000,20,000,30,000}\nWarmup Steps {off,2,000,3,000}\nDecay Factor {off,0.1,0.5}\nSteps per Decay {off,4,000,6,000,8,000}\nTable 6: Search space for each hyperparameter consid-\nered. Parameter Scaling refers to the Adafactor setting\nwhere an update is scaled by the norm of the parameter\nit will be applied to. Warmup Steps is the number of\nsteps before a linearly increasing learning rate reaches\nthe Learning Rate value, starting from zero. Decay Fac-\ntor is the reduction in Learning Rate size that occurs\nevery \u201cSteps per Decay\u201d steps.\nDataset Training Validation Testing\nBoolQ 9,427 3 ,270 3 ,245\nCB 250 56 250\nCOPA 400 100 500\nMultiRC 27,243 4 ,848 9 ,693\nReCoRD 100,730 10 ,000 10 ,000\nRTE 2,490 277 3 ,000\nWiC 5,428 638 1 ,400\nWSC 259\u2217104 146\nMRPC 3,668 408 1 ,725\nQQP 363,849 40 ,430 390 ,965\nSQuAD 87,599 10 ,570 -\nTextbookQA - 1,504 -\nRACE - 1,503 -\nBioASQ - 1,501 -\nRE - 674 -\nDuoRC - 2,948 -\nDROP - 1,503 -\nTable 7: Sizes for training, validation, and testing splits\nof each dataset used.\u2217Following T5, our casting of\nWSC as a text generation problems means we can only\ntrain on examples where the supplied referent is correct.", "start_char_idx": 0, "end_char_idx": 1269, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3063fa75-f221-41bc-b87f-86db7e9a3ca0": {"__data__": {"id_": "3063fa75-f221-41bc-b87f-86db7e9a3ca0", "embedding": null, "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9256579-366e-4b1d-821b-7e410b884184", "node_type": "4", "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "71badfa87b1f3c3c6d030d4c81c9923d18ec9dd7de00c195d6d2c580deaf4ef9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6359578d-c194-43bb-a730-686f0f0680a5", "node_type": "1", "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "b048a7ed323c5eb1cbe5a139499724d8df0794bc001c0b6c53080624b2f7c250", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "398cdb4a-588c-405d-a78d-3007f1e610cd", "node_type": "1", "metadata": {}, "hash": "7d74713e838a809293ccdac5b587343f5f6164097645bd682b6a4c7bd2763a4c", "class_name": "RelatedNodeInfo"}}, "text": "This means our training dataset is smaller than the nor-\nmal WSC training dataset, which has 554examples.\nSplit False True\nTraining 37.7 62 .3\nValidation 37.8 62 .2\nTable 8: Label distribution for the BoolQ dataset.\nSplit contradiction entailment neutral\nTraining 47.6 46 .0 6 .4\nValidation 50.0 41 .1 8 .9\nTable 9: Label distribution for the CB dataset.\nSplit choice1 choice2\nTraining 48.8 51 .2\nValidation 55.0 45 .0\nTable 10: Label distribution for the COPA dataset.Split False True\nTraining 55.9 44 .1\nValidation 57.2 42 .8\nTable 11: Label distribution for the MultiRC dataset.\nSplit False True\nTraining 50.0 50 .0\nValidation 50.0 50 .0\nTable 12: Label distribution for the WiC dataset.\nSplit False True\nTraining 0.0 100 .0\nValidation 63.5 36 .5\nTable 13: Label distribution for the WSC dataset.", "start_char_idx": 1270, "end_char_idx": 2069, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "398cdb4a-588c-405d-a78d-3007f1e610cd": {"__data__": {"id_": "398cdb4a-588c-405d-a78d-3007f1e610cd", "embedding": null, "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9256579-366e-4b1d-821b-7e410b884184", "node_type": "4", "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "71badfa87b1f3c3c6d030d4c81c9923d18ec9dd7de00c195d6d2c580deaf4ef9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3063fa75-f221-41bc-b87f-86db7e9a3ca0", "node_type": "1", "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "15cd05de8ad136d0987ed38a0581b25737aa5473bba3afb8b6e4ef1a4c53ea8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "621f8d8f-325c-4cc5-8faf-0ca60d8ac2c9", "node_type": "1", "metadata": {}, "hash": "e08659ad4677b4103ca34fa7d652751d2e739ec568945500c1aaad00789511c9", "class_name": "RelatedNodeInfo"}}, "text": "Split contradiction entailment neutral\nTraining 47.6 46 .0 6 .4\nValidation 50.0 41 .1 8 .9\nTable 9: Label distribution for the CB dataset.\nSplit choice1 choice2\nTraining 48.8 51 .2\nValidation 55.0 45 .0\nTable 10: Label distribution for the COPA dataset.Split False True\nTraining 55.9 44 .1\nValidation 57.2 42 .8\nTable 11: Label distribution for the MultiRC dataset.\nSplit False True\nTraining 50.0 50 .0\nValidation 50.0 50 .0\nTable 12: Label distribution for the WiC dataset.\nSplit False True\nTraining 0.0 100 .0\nValidation 63.5 36 .5\nTable 13: Label distribution for the WSC dataset. Fol-\nlowing T5, we cast the WSC dataset to a free-form text\ngeneration task where the model generates the referent\nto the highlighted span instead predicting if the sup-\nplied entity is the correct referent of the highlighted\nspan.", "start_char_idx": 1486, "end_char_idx": 2301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "621f8d8f-325c-4cc5-8faf-0ca60d8ac2c9": {"__data__": {"id_": "621f8d8f-325c-4cc5-8faf-0ca60d8ac2c9", "embedding": null, "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9256579-366e-4b1d-821b-7e410b884184", "node_type": "4", "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "71badfa87b1f3c3c6d030d4c81c9923d18ec9dd7de00c195d6d2c580deaf4ef9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "398cdb4a-588c-405d-a78d-3007f1e610cd", "node_type": "1", "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "134d928b811883de871b63bc2add8bdedacce780fec305d979cae94efc9bb347", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c87fcee-c971-45a2-b1e6-c0e689eb9d50", "node_type": "1", "metadata": {}, "hash": "18bda1544219f0fd6a2a9de15d89e7916953e5348c57bba4c5147b171259628d", "class_name": "RelatedNodeInfo"}}, "text": "Split choice1 choice2\nTraining 48.8 51 .2\nValidation 55.0 45 .0\nTable 10: Label distribution for the COPA dataset.Split False True\nTraining 55.9 44 .1\nValidation 57.2 42 .8\nTable 11: Label distribution for the MultiRC dataset.\nSplit False True\nTraining 50.0 50 .0\nValidation 50.0 50 .0\nTable 12: Label distribution for the WiC dataset.\nSplit False True\nTraining 0.0 100 .0\nValidation 63.5 36 .5\nTable 13: Label distribution for the WSC dataset. Fol-\nlowing T5, we cast the WSC dataset to a free-form text\ngeneration task where the model generates the referent\nto the highlighted span instead predicting if the sup-\nplied entity is the correct referent of the highlighted\nspan. Thus, we only use training data where the sup-\nplied referent is correct making our training label dis-\ntribution focused entirely on True .", "start_char_idx": 1625, "end_char_idx": 2442, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c87fcee-c971-45a2-b1e6-c0e689eb9d50": {"__data__": {"id_": "0c87fcee-c971-45a2-b1e6-c0e689eb9d50", "embedding": null, "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9256579-366e-4b1d-821b-7e410b884184", "node_type": "4", "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "71badfa87b1f3c3c6d030d4c81c9923d18ec9dd7de00c195d6d2c580deaf4ef9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "621f8d8f-325c-4cc5-8faf-0ca60d8ac2c9", "node_type": "1", "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "2f67570d9ad21d1466eba309e7c3b1d24dd5e35abc2099480ec782c679f78a9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3301bf78-27a9-4cfd-a84f-14739569d00b", "node_type": "1", "metadata": {}, "hash": "5b493306783f61354f1c38109fa546ed65b5e059d56e276423ea837392f755fb", "class_name": "RelatedNodeInfo"}}, "text": "Split False True\nTraining 50.0 50 .0\nValidation 50.0 50 .0\nTable 12: Label distribution for the WiC dataset.\nSplit False True\nTraining 0.0 100 .0\nValidation 63.5 36 .5\nTable 13: Label distribution for the WSC dataset. Fol-\nlowing T5, we cast the WSC dataset to a free-form text\ngeneration task where the model generates the referent\nto the highlighted span instead predicting if the sup-\nplied entity is the correct referent of the highlighted\nspan. Thus, we only use training data where the sup-\nplied referent is correct making our training label dis-\ntribution focused entirely on True .\nSplit entailment not_entailment\nTraining 51.2 49 .8\nValidation 52.7 47 .3\nTable 14: Label distribution for the RTE dataset.\nSplit equivalent not_equivalent\nTraining 67.4 32 .6\nValidation 68.4 31 .6\nTable 15: Label distribution for the MRPC dataset.", "start_char_idx": 1852, "end_char_idx": 2691, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3301bf78-27a9-4cfd-a84f-14739569d00b": {"__data__": {"id_": "3301bf78-27a9-4cfd-a84f-14739569d00b", "embedding": null, "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9256579-366e-4b1d-821b-7e410b884184", "node_type": "4", "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "71badfa87b1f3c3c6d030d4c81c9923d18ec9dd7de00c195d6d2c580deaf4ef9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c87fcee-c971-45a2-b1e6-c0e689eb9d50", "node_type": "1", "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "86b481775b4dfc9cbcd0dfd50c57f62673ff68aadcd48c6e0bb2574dac5b4df1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aea7399d-1604-4f71-a6c2-e5ed90e864f2", "node_type": "1", "metadata": {}, "hash": "a3bdc52056c5907ca73aaaa8d6190fe0c97dc33438adc01d12dfe97f5aff7eda", "class_name": "RelatedNodeInfo"}}, "text": "Split False True\nTraining 0.0 100 .0\nValidation 63.5 36 .5\nTable 13: Label distribution for the WSC dataset. Fol-\nlowing T5, we cast the WSC dataset to a free-form text\ngeneration task where the model generates the referent\nto the highlighted span instead predicting if the sup-\nplied entity is the correct referent of the highlighted\nspan. Thus, we only use training data where the sup-\nplied referent is correct making our training label dis-\ntribution focused entirely on True .\nSplit entailment not_entailment\nTraining 51.2 49 .8\nValidation 52.7 47 .3\nTable 14: Label distribution for the RTE dataset.\nSplit equivalent not_equivalent\nTraining 67.4 32 .6\nValidation 68.4 31 .6\nTable 15: Label distribution for the MRPC dataset.\nSplit duplicate not_duplicate\nTraining 36.9 63 .1\nValidation 36.8 63 .2\nTable 16: Label distribution for the QQP dataset.", "start_char_idx": 1961, "end_char_idx": 2813, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aea7399d-1604-4f71-a6c2-e5ed90e864f2": {"__data__": {"id_": "aea7399d-1604-4f71-a6c2-e5ed90e864f2", "embedding": null, "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13e8ae9-34c4-472f-9fd5-2bd7c44a33f8", "node_type": "4", "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6dd31c1c995cf86349c7687f43981212c5ee766b5d67518a18b5cdf72485a1c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3301bf78-27a9-4cfd-a84f-14739569d00b", "node_type": "1", "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}, "hash": "af1190f75bd7b586af9307ebac303d4307e38474947be8734edaeee6be438016", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3668a3e5-f6cb-4f29-abb7-0cefdb73436b", "node_type": "1", "metadata": {}, "hash": "749c09dd96ed040904de185722adf36d5daf73f69bfbf89c048cb08216f4cd20", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nLONG LORA: E FFICIENT FINE-TUNING OF LONG -\nCONTEXT LARGE LANGUAGE MODELS\nYukang Chen1Shengju Qian1Haotian Tang2Xin Lai1\nZhijian Liu2Song Han2,3Jiaya Jia1\n1CUHK2MIT3NVIDIA\nABSTRACT\nWe present LongLoRA, an efficient fine-tuning approach that extends the context\nsizes of pre-trained large language models (LLMs), with limited computation cost.\nTypically, training LLMs with long context sizes is computationally expensive,\nrequiring extensive training hours and GPU resources. For example, training on\nthe context length of 8192 needs 16 \u00d7computational costs in self-attention layers\nas that of 2048. In this paper, we speed up the context extension of LLMs in\ntwo aspects. On the one hand, although dense global attention is needed during\ninference, fine-tuning the model can be effectively and efficiently done by sparse\nlocal attention.", "start_char_idx": 0, "end_char_idx": 883, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3668a3e5-f6cb-4f29-abb7-0cefdb73436b": {"__data__": {"id_": "3668a3e5-f6cb-4f29-abb7-0cefdb73436b", "embedding": null, "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13e8ae9-34c4-472f-9fd5-2bd7c44a33f8", "node_type": "4", "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6dd31c1c995cf86349c7687f43981212c5ee766b5d67518a18b5cdf72485a1c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aea7399d-1604-4f71-a6c2-e5ed90e864f2", "node_type": "1", "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "e38e58f1a7ec5573589f573f846aec758721b5ce77c7f3e8fda04a87acae216d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6405d710-c524-4550-aa6d-e590008be9f9", "node_type": "1", "metadata": {}, "hash": "500ccd399c3bf79b35fb73caf889e6901b575dbeb1c8329e9b1c335330b73bfb", "class_name": "RelatedNodeInfo"}}, "text": "Typically, training LLMs with long context sizes is computationally expensive,\nrequiring extensive training hours and GPU resources. For example, training on\nthe context length of 8192 needs 16 \u00d7computational costs in self-attention layers\nas that of 2048. In this paper, we speed up the context extension of LLMs in\ntwo aspects. On the one hand, although dense global attention is needed during\ninference, fine-tuning the model can be effectively and efficiently done by sparse\nlocal attention. The proposed shifted sparse attention (S2-Attn) effectively enables\ncontext extension, leading to non-trivial computation saving with similar perfor-\nmance to fine-tuning with vanilla attention. Particularly, it can be implemented\nwith only two lines of code in training, while being optional in inference. On\nthe other hand, we revisit the parameter-efficient fine-tuning regime for context\nexpansion. Notably, we find that LoRA for context extension works well under\nthe premise of trainable embedding and normalization. LongLoRA combines this\nimproved LoRA with S2-Attn.", "start_char_idx": 388, "end_char_idx": 1457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6405d710-c524-4550-aa6d-e590008be9f9": {"__data__": {"id_": "6405d710-c524-4550-aa6d-e590008be9f9", "embedding": null, "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13e8ae9-34c4-472f-9fd5-2bd7c44a33f8", "node_type": "4", "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6dd31c1c995cf86349c7687f43981212c5ee766b5d67518a18b5cdf72485a1c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3668a3e5-f6cb-4f29-abb7-0cefdb73436b", "node_type": "1", "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "8df2e1c6a5b0d0089dde2c0f733e8eae5d269190307765779167573806ef39f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "095c0460-a0b6-435c-9893-82878fd88e64", "node_type": "1", "metadata": {}, "hash": "b0d8bc3870c0f3e8bdac0489afa8f240f1a122670b9d146d93d6e413d3f97884", "class_name": "RelatedNodeInfo"}}, "text": "In this paper, we speed up the context extension of LLMs in\ntwo aspects. On the one hand, although dense global attention is needed during\ninference, fine-tuning the model can be effectively and efficiently done by sparse\nlocal attention. The proposed shifted sparse attention (S2-Attn) effectively enables\ncontext extension, leading to non-trivial computation saving with similar perfor-\nmance to fine-tuning with vanilla attention. Particularly, it can be implemented\nwith only two lines of code in training, while being optional in inference. On\nthe other hand, we revisit the parameter-efficient fine-tuning regime for context\nexpansion. Notably, we find that LoRA for context extension works well under\nthe premise of trainable embedding and normalization. LongLoRA combines this\nimproved LoRA with S2-Attn. LongLoRA demonstrates strong empirical results\non various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends\nLlama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8 \u00d7A100\nmachine.", "start_char_idx": 645, "end_char_idx": 1660, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "095c0460-a0b6-435c-9893-82878fd88e64": {"__data__": {"id_": "095c0460-a0b6-435c-9893-82878fd88e64", "embedding": null, "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13e8ae9-34c4-472f-9fd5-2bd7c44a33f8", "node_type": "4", "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6dd31c1c995cf86349c7687f43981212c5ee766b5d67518a18b5cdf72485a1c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6405d710-c524-4550-aa6d-e590008be9f9", "node_type": "1", "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "7cfaeb9153f5d5ec552d32271a32326f219152904214a4852482b4b7cbb5ee16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bcbeda0-c2e2-44cb-ab6b-c835e19884e8", "node_type": "1", "metadata": {}, "hash": "ad3da788e995f3eb4d9f66d554fdd7e3f72a70834273a79d8a4df586e3306525", "class_name": "RelatedNodeInfo"}}, "text": "Particularly, it can be implemented\nwith only two lines of code in training, while being optional in inference. On\nthe other hand, we revisit the parameter-efficient fine-tuning regime for context\nexpansion. Notably, we find that LoRA for context extension works well under\nthe premise of trainable embedding and normalization. LongLoRA combines this\nimproved LoRA with S2-Attn. LongLoRA demonstrates strong empirical results\non various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends\nLlama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8 \u00d7A100\nmachine. LongLoRA extends models\u2019 context while retaining their original archi-\ntectures, and is compatible with most existing techniques, like Flash-Attention2.\nIn addition, we further conduct supervised fine-tuning with LongLoRA and our\nlong instruction-following LongAlpaca dataset. All our code, models, dataset, and\ndemo are available at github.com/dvlab-research/LongLoRA.", "start_char_idx": 1079, "end_char_idx": 2030, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1bcbeda0-c2e2-44cb-ab6b-c835e19884e8": {"__data__": {"id_": "1bcbeda0-c2e2-44cb-ab6b-c835e19884e8", "embedding": null, "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13e8ae9-34c4-472f-9fd5-2bd7c44a33f8", "node_type": "4", "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6dd31c1c995cf86349c7687f43981212c5ee766b5d67518a18b5cdf72485a1c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "095c0460-a0b6-435c-9893-82878fd88e64", "node_type": "1", "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "fa546294b2648c0f0fb0e5bb937bd707c1e99b8184eac3e49ae665c9676a00d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64f21f32-de35-476e-add8-69af4ecce98c", "node_type": "1", "metadata": {}, "hash": "c609dc5dd3eddc1ef7a35a505a02c529f24f3903bd8562c2bdcdf47809cadf20", "class_name": "RelatedNodeInfo"}}, "text": "LongLoRA combines this\nimproved LoRA with S2-Attn. LongLoRA demonstrates strong empirical results\non various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends\nLlama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8 \u00d7A100\nmachine. LongLoRA extends models\u2019 context while retaining their original archi-\ntectures, and is compatible with most existing techniques, like Flash-Attention2.\nIn addition, we further conduct supervised fine-tuning with LongLoRA and our\nlong instruction-following LongAlpaca dataset. All our code, models, dataset, and\ndemo are available at github.com/dvlab-research/LongLoRA.\n2.662.522.482.782.942.983.732.42.93.43.9\n8192163643276865536Perplexity46.357.468.825.634.646.369.820406080\n8192163643276865536GPU memory\nFull FTLoRALongLoRA7.416.339.85.211.324.652.403060908192163643276865536Training hours\nContexthoursGB\nContextContextOOMOOM92.5\nFigure 1: LongLoRA closes the accuracy gap that between conventional LoRA and full fine-tuning,\nwhile still maintaining up to 1.8 \u00d7lower memory cost than full fine-tuning.", "start_char_idx": 1407, "end_char_idx": 2465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64f21f32-de35-476e-add8-69af4ecce98c": {"__data__": {"id_": "64f21f32-de35-476e-add8-69af4ecce98c", "embedding": null, "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13e8ae9-34c4-472f-9fd5-2bd7c44a33f8", "node_type": "4", "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6dd31c1c995cf86349c7687f43981212c5ee766b5d67518a18b5cdf72485a1c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1bcbeda0-c2e2-44cb-ab6b-c835e19884e8", "node_type": "1", "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "e13b456e283f50efb0d0f778c7298cb6b8cde2e06c6979c068221046c30bd261", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f789e43-ad43-4f41-a2f9-b0bed5dc4ad9", "node_type": "1", "metadata": {}, "hash": "e1103567873e36ccc0a81e6ee8af436ab72b5439293811b897386334837ca7a3", "class_name": "RelatedNodeInfo"}}, "text": "All our code, models, dataset, and\ndemo are available at github.com/dvlab-research/LongLoRA.\n2.662.522.482.782.942.983.732.42.93.43.9\n8192163643276865536Perplexity46.357.468.825.634.646.369.820406080\n8192163643276865536GPU memory\nFull FTLoRALongLoRA7.416.339.85.211.324.652.403060908192163643276865536Training hours\nContexthoursGB\nContextContextOOMOOM92.5\nFigure 1: LongLoRA closes the accuracy gap that between conventional LoRA and full fine-tuning,\nwhile still maintaining up to 1.8 \u00d7lower memory cost than full fine-tuning. Furthermore, LongLoRA\nimproves the training speed of LoRA by up to 1.8 \u00d7withS2-Attn.", "start_char_idx": 1938, "end_char_idx": 2550, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f789e43-ad43-4f41-a2f9-b0bed5dc4ad9": {"__data__": {"id_": "8f789e43-ad43-4f41-a2f9-b0bed5dc4ad9", "embedding": null, "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13e8ae9-34c4-472f-9fd5-2bd7c44a33f8", "node_type": "4", "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6dd31c1c995cf86349c7687f43981212c5ee766b5d67518a18b5cdf72485a1c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64f21f32-de35-476e-add8-69af4ecce98c", "node_type": "1", "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "80405be03e5880b596ae04b70fe786093b9c265213d30e2c7dfe0cbfd8dcde79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7213d1b-3a36-484a-9fbc-ea4155b6664b", "node_type": "1", "metadata": {}, "hash": "d309ae4a0422ab8a8150a29466ce1ced30adf84b68e418e0ca2a5915ce143e39", "class_name": "RelatedNodeInfo"}}, "text": "2.662.522.482.782.942.983.732.42.93.43.9\n8192163643276865536Perplexity46.357.468.825.634.646.369.820406080\n8192163643276865536GPU memory\nFull FTLoRALongLoRA7.416.339.85.211.324.652.403060908192163643276865536Training hours\nContexthoursGB\nContextContextOOMOOM92.5\nFigure 1: LongLoRA closes the accuracy gap that between conventional LoRA and full fine-tuning,\nwhile still maintaining up to 1.8 \u00d7lower memory cost than full fine-tuning. Furthermore, LongLoRA\nimproves the training speed of LoRA by up to 1.8 \u00d7withS2-Attn. Llama2-7B are fine-tuned to\nvarious context lengths with Flash-Attention2 (Dao, 2023) and DeepSpeed (Rasley et al., 2020) stage\n2 and evaluated on the proof-pile (Azerbayev et al., 2022) test set in perplexity.", "start_char_idx": 2031, "end_char_idx": 2761, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7213d1b-3a36-484a-9fbc-ea4155b6664b": {"__data__": {"id_": "e7213d1b-3a36-484a-9fbc-ea4155b6664b", "embedding": null, "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f13e8ae9-34c4-472f-9fd5-2bd7c44a33f8", "node_type": "4", "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6dd31c1c995cf86349c7687f43981212c5ee766b5d67518a18b5cdf72485a1c6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f789e43-ad43-4f41-a2f9-b0bed5dc4ad9", "node_type": "1", "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6f8d48b30061b9c32d2ea90e00a16d08754e7fd1443c5f4b76c97e114316573d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f25a8684-701e-4bf8-94cf-c03306e2618e", "node_type": "1", "metadata": {}, "hash": "23e7d77b54fab2ce15dd84b65152bb427d570a62a8b3dbc2a105b871236bf108", "class_name": "RelatedNodeInfo"}}, "text": "Furthermore, LongLoRA\nimproves the training speed of LoRA by up to 1.8 \u00d7withS2-Attn. Llama2-7B are fine-tuned to\nvarious context lengths with Flash-Attention2 (Dao, 2023) and DeepSpeed (Rasley et al., 2020) stage\n2 and evaluated on the proof-pile (Azerbayev et al., 2022) test set in perplexity.\n1 I NTRODUCTION\nLarge language models (LLMs) are typically trained with a pre-defined context size, such as 2048\ntokens for LLaMA (Touvron et al., 2023a) and 4096 tokens for Llama2 (Touvron et al., 2023b).\n1arXiv:2309.12307v3  [cs.CL]  8 Mar 2024", "start_char_idx": 2466, "end_char_idx": 3008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f25a8684-701e-4bf8-94cf-c03306e2618e": {"__data__": {"id_": "f25a8684-701e-4bf8-94cf-c03306e2618e", "embedding": null, "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c07a6846437159e61da96c2fa769a41ac2ae58b6b0d32f724041f03a2da043a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7213d1b-3a36-484a-9fbc-ea4155b6664b", "node_type": "1", "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "8d96e912159bd97663328ab3368755af4cc551f209b4ef5fa934cb5de0fac83b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4ed1c5c-539d-4b3d-a13d-9d4fc5ba828e", "node_type": "1", "metadata": {}, "hash": "2966f53b119c5c633882cde74845872ef7317102c3e011f37cb4a5842c11c6e2", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n!Trainable\n\u2744FrozenNormpostMul(-headSelf-A1en(onFeed ForwardNorminput++Lora\n!\n!Embedding\n!\n!\n\u2744(b) Low-rank adaptx N\n\u2744Pa1ern 1 -w/o shi@(a) Shi3ed sparse a6en7onEach pa1ern in half heads\na\n!Norm (0.004%)a\n!Lora (0.12%) a\n\u2744 Linear Projec3on (96%) a\n!Embedding (1.94%)a\n\u2744Head (1.94%) (c) Propor5ons of Parameters (LLaMA7B for example) Pa1ern 2 -w/ shi@Combina(on\nFigure 2: Overview of LongLoRA . We introduce Shifted Sparse Attention (S2-Attn) during fine-\ntuning. The trained model retains original standard self-attention at inference time. In addition to\ntraining LoRA weights in linear layers, LongLoRA further makes embedding and normalization\nlayers trainable. This extension is pivotal for context extension, and only introduces a minimal\nnumber of additional trainable parameters.", "start_char_idx": 0, "end_char_idx": 829, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4ed1c5c-539d-4b3d-a13d-9d4fc5ba828e": {"__data__": {"id_": "f4ed1c5c-539d-4b3d-a13d-9d4fc5ba828e", "embedding": null, "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c07a6846437159e61da96c2fa769a41ac2ae58b6b0d32f724041f03a2da043a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f25a8684-701e-4bf8-94cf-c03306e2618e", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "7ce4d693b90fe933d9060611f8354035231da66c34394f59acaee258afbd7f6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61113d88-0609-496e-91d0-575c2547835f", "node_type": "1", "metadata": {}, "hash": "fea030de878c4fd7bcd87c782d7ccb0a4cf788604982566b503be23c4841b48c", "class_name": "RelatedNodeInfo"}}, "text": "We introduce Shifted Sparse Attention (S2-Attn) during fine-\ntuning. The trained model retains original standard self-attention at inference time. In addition to\ntraining LoRA weights in linear layers, LongLoRA further makes embedding and normalization\nlayers trainable. This extension is pivotal for context extension, and only introduces a minimal\nnumber of additional trainable parameters.\nHowever, the pre-defined size limits LLMs in many applications, like summarizing long documents\nor answering long questions. To resolve this limitation, some recent works (Chen et al., 2023;\nTworkowski et al., 2023; Mohtashami & Jaggi, 2023) train or fine-tune LLMs to longer context.\nHowever, training an LLM from scratch with long sequences poses computational challenges, and\nfine-tuning an existing pre-trained LLM is also considerably expensive. For instance, Position\nInterpolation (Chen et al., 2023) spent 32 A100 GPUs to extend LLaMA models from 2k to 8k\ncontext, and 128 A100 GPUs for longer context fine-tuning.", "start_char_idx": 437, "end_char_idx": 1452, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61113d88-0609-496e-91d0-575c2547835f": {"__data__": {"id_": "61113d88-0609-496e-91d0-575c2547835f", "embedding": null, "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c07a6846437159e61da96c2fa769a41ac2ae58b6b0d32f724041f03a2da043a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4ed1c5c-539d-4b3d-a13d-9d4fc5ba828e", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "40b05fbfec89793b92fd810575ed6c48a54ac641e16cdacfb39c688d79e5f698", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83bf573e-f907-4ba2-aada-1a32bfd179ef", "node_type": "1", "metadata": {}, "hash": "ed5b3acec58b1bde4c5dd7fb8a9a66878b8c5edd4b99011c3ff5c4df76ef9a66", "class_name": "RelatedNodeInfo"}}, "text": "This extension is pivotal for context extension, and only introduces a minimal\nnumber of additional trainable parameters.\nHowever, the pre-defined size limits LLMs in many applications, like summarizing long documents\nor answering long questions. To resolve this limitation, some recent works (Chen et al., 2023;\nTworkowski et al., 2023; Mohtashami & Jaggi, 2023) train or fine-tune LLMs to longer context.\nHowever, training an LLM from scratch with long sequences poses computational challenges, and\nfine-tuning an existing pre-trained LLM is also considerably expensive. For instance, Position\nInterpolation (Chen et al., 2023) spent 32 A100 GPUs to extend LLaMA models from 2k to 8k\ncontext, and 128 A100 GPUs for longer context fine-tuning. FOT (Tworkowski et al., 2023) used 32\nTPUs for standard transformer training and 128 TPUs for LongLLaMA. These computation resources\nare typically unaffordable for common researchers, which naturally leads us to question: can we\nextend the context window of LLMs efficiently?", "start_char_idx": 708, "end_char_idx": 1728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83bf573e-f907-4ba2-aada-1a32bfd179ef": {"__data__": {"id_": "83bf573e-f907-4ba2-aada-1a32bfd179ef", "embedding": null, "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c07a6846437159e61da96c2fa769a41ac2ae58b6b0d32f724041f03a2da043a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61113d88-0609-496e-91d0-575c2547835f", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "df3dcdd9dc50148f782044c75f45fafd70cd6164f5e1f225ad7b9b209c5cd1b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98b2f0f3-f3fb-43db-a496-1ec409796ee8", "node_type": "1", "metadata": {}, "hash": "0cb1114fdc55b62f8bc5425621e344e5e8c966264b62bb02320bd08e04b6f7e4", "class_name": "RelatedNodeInfo"}}, "text": "However, training an LLM from scratch with long sequences poses computational challenges, and\nfine-tuning an existing pre-trained LLM is also considerably expensive. For instance, Position\nInterpolation (Chen et al., 2023) spent 32 A100 GPUs to extend LLaMA models from 2k to 8k\ncontext, and 128 A100 GPUs for longer context fine-tuning. FOT (Tworkowski et al., 2023) used 32\nTPUs for standard transformer training and 128 TPUs for LongLLaMA. These computation resources\nare typically unaffordable for common researchers, which naturally leads us to question: can we\nextend the context window of LLMs efficiently?\nOne straightforward approach is to fine-tune a pre-trained LLM via low-rank adaptation (LoRA) (Hu\net al., 2022). LoRA modifies the linear projection layers in self-attention blocks by utilizing low-rank\nmatrices, which are generally efficient and reduce the number of trainable parameters. However, our\nempirical findings indicate that training long context models in this manner is neither sufficiently\neffective nor efficient.", "start_char_idx": 1115, "end_char_idx": 2157, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98b2f0f3-f3fb-43db-a496-1ec409796ee8": {"__data__": {"id_": "98b2f0f3-f3fb-43db-a496-1ec409796ee8", "embedding": null, "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c07a6846437159e61da96c2fa769a41ac2ae58b6b0d32f724041f03a2da043a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83bf573e-f907-4ba2-aada-1a32bfd179ef", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6ee4b2d7fd3160c2c4ab39d55281a51bd53f4ed2f6b23be621a729802052c568", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18c69308-84f7-4dc8-92ce-8aba47ec6874", "node_type": "1", "metadata": {}, "hash": "398ac364f646f7a243f88edae4703cb6fbacfca5af044e9a038652e868b7a135", "class_name": "RelatedNodeInfo"}}, "text": "FOT (Tworkowski et al., 2023) used 32\nTPUs for standard transformer training and 128 TPUs for LongLLaMA. These computation resources\nare typically unaffordable for common researchers, which naturally leads us to question: can we\nextend the context window of LLMs efficiently?\nOne straightforward approach is to fine-tune a pre-trained LLM via low-rank adaptation (LoRA) (Hu\net al., 2022). LoRA modifies the linear projection layers in self-attention blocks by utilizing low-rank\nmatrices, which are generally efficient and reduce the number of trainable parameters. However, our\nempirical findings indicate that training long context models in this manner is neither sufficiently\neffective nor efficient. In terms of effectiveness , plain low-rank adaptation results in a high perplexity\nin long context extension, as in Table 2. Increasing the rank to a higher value, e.g., rank = 256,\ndoes not alleviate this issue. In terms of efficiency , regardless of whether LoRA is employed or not,\ncomputational cost increases dramatically as the context size expands, primarily due to the standard\nself-attention mechanism (Vaswani et al., 2017).", "start_char_idx": 1453, "end_char_idx": 2592, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18c69308-84f7-4dc8-92ce-8aba47ec6874": {"__data__": {"id_": "18c69308-84f7-4dc8-92ce-8aba47ec6874", "embedding": null, "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c07a6846437159e61da96c2fa769a41ac2ae58b6b0d32f724041f03a2da043a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98b2f0f3-f3fb-43db-a496-1ec409796ee8", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "cc14a6d2a841420ce760626e618b22987eb39b243dfa7c6e1c2e72e8a243595f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4540d62f-d7cc-4feb-a2c6-fede1456f207", "node_type": "1", "metadata": {}, "hash": "907fe6789821cf6e16f28d51270e49d35322f412e42fe5fefeeff745056d3280", "class_name": "RelatedNodeInfo"}}, "text": "One straightforward approach is to fine-tune a pre-trained LLM via low-rank adaptation (LoRA) (Hu\net al., 2022). LoRA modifies the linear projection layers in self-attention blocks by utilizing low-rank\nmatrices, which are generally efficient and reduce the number of trainable parameters. However, our\nempirical findings indicate that training long context models in this manner is neither sufficiently\neffective nor efficient. In terms of effectiveness , plain low-rank adaptation results in a high perplexity\nin long context extension, as in Table 2. Increasing the rank to a higher value, e.g., rank = 256,\ndoes not alleviate this issue. In terms of efficiency , regardless of whether LoRA is employed or not,\ncomputational cost increases dramatically as the context size expands, primarily due to the standard\nself-attention mechanism (Vaswani et al., 2017). As shown in Figure 1, even with LoRA, the training\nhours for the standard Llama2 model increase substantially when the context window expands.", "start_char_idx": 1729, "end_char_idx": 2735, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4540d62f-d7cc-4feb-a2c6-fede1456f207": {"__data__": {"id_": "4540d62f-d7cc-4feb-a2c6-fede1456f207", "embedding": null, "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c07a6846437159e61da96c2fa769a41ac2ae58b6b0d32f724041f03a2da043a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18c69308-84f7-4dc8-92ce-8aba47ec6874", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "d8acccfc7518e7a53f3b923a27b4aff4308382239861ccbafad8437ca2fad97e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bd16d89-190c-49f4-b8d6-a55b730f5f85", "node_type": "1", "metadata": {}, "hash": "718a4a83c69632c2cc0e2410e52505c5dafaaaca6f1712cf667be167d01ee1f7", "class_name": "RelatedNodeInfo"}}, "text": "LoRA modifies the linear projection layers in self-attention blocks by utilizing low-rank\nmatrices, which are generally efficient and reduce the number of trainable parameters. However, our\nempirical findings indicate that training long context models in this manner is neither sufficiently\neffective nor efficient. In terms of effectiveness , plain low-rank adaptation results in a high perplexity\nin long context extension, as in Table 2. Increasing the rank to a higher value, e.g., rank = 256,\ndoes not alleviate this issue. In terms of efficiency , regardless of whether LoRA is employed or not,\ncomputational cost increases dramatically as the context size expands, primarily due to the standard\nself-attention mechanism (Vaswani et al., 2017). As shown in Figure 1, even with LoRA, the training\nhours for the standard Llama2 model increase substantially when the context window expands.\nIn this work, we introduce LongLoRA, an efficient fine-tuning approach that extends the context\nwindows of pre-trained LLMs, e.g., Llama2 (Touvron et al., 2023b).", "start_char_idx": 1842, "end_char_idx": 2898, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5bd16d89-190c-49f4-b8d6-a55b730f5f85": {"__data__": {"id_": "5bd16d89-190c-49f4-b8d6-a55b730f5f85", "embedding": null, "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c07a6846437159e61da96c2fa769a41ac2ae58b6b0d32f724041f03a2da043a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4540d62f-d7cc-4feb-a2c6-fede1456f207", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "d3d285353f521e76cd836a48dd7045f678d125501c1dcc742d25116c781964df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecdfb7f9-71db-4368-99de-b40bd2642a44", "node_type": "1", "metadata": {}, "hash": "26cf135c277030d3714ce3b3e7df35db0f5fa885731e957a167a0d5fda2c0764", "class_name": "RelatedNodeInfo"}}, "text": "In terms of effectiveness , plain low-rank adaptation results in a high perplexity\nin long context extension, as in Table 2. Increasing the rank to a higher value, e.g., rank = 256,\ndoes not alleviate this issue. In terms of efficiency , regardless of whether LoRA is employed or not,\ncomputational cost increases dramatically as the context size expands, primarily due to the standard\nself-attention mechanism (Vaswani et al., 2017). As shown in Figure 1, even with LoRA, the training\nhours for the standard Llama2 model increase substantially when the context window expands.\nIn this work, we introduce LongLoRA, an efficient fine-tuning approach that extends the context\nwindows of pre-trained LLMs, e.g., Llama2 (Touvron et al., 2023b). LoRA (Hu et al., 2022) uses\nlow-rank weight updates to approximate full fine-tuning. Similarly, we find that short attention is\nalso able to approximate long context during training. We present shifted sparse attention (S2-Attn)\nas an efficient substitute for standard self-attention.", "start_char_idx": 2158, "end_char_idx": 3183, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecdfb7f9-71db-4368-99de-b40bd2642a44": {"__data__": {"id_": "ecdfb7f9-71db-4368-99de-b40bd2642a44", "embedding": null, "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c07a6846437159e61da96c2fa769a41ac2ae58b6b0d32f724041f03a2da043a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5bd16d89-190c-49f4-b8d6-a55b730f5f85", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c07993f07e797ae8bef8e06ac97cf5439c1ad78c767b2dcd2501210d05bb29e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f8c5dd6-26fb-481c-831e-711684b41a30", "node_type": "1", "metadata": {}, "hash": "edc8740bc45af45ff85c3b7a9d6a3f1e610c21a2fb5616a8c1dbaa2ebd986ff3", "class_name": "RelatedNodeInfo"}}, "text": "In terms of efficiency , regardless of whether LoRA is employed or not,\ncomputational cost increases dramatically as the context size expands, primarily due to the standard\nself-attention mechanism (Vaswani et al., 2017). As shown in Figure 1, even with LoRA, the training\nhours for the standard Llama2 model increase substantially when the context window expands.\nIn this work, we introduce LongLoRA, an efficient fine-tuning approach that extends the context\nwindows of pre-trained LLMs, e.g., Llama2 (Touvron et al., 2023b). LoRA (Hu et al., 2022) uses\nlow-rank weight updates to approximate full fine-tuning. Similarly, we find that short attention is\nalso able to approximate long context during training. We present shifted sparse attention (S2-Attn)\nas an efficient substitute for standard self-attention. As shown in Figure 2, we split context length\ninto several groups and conduct attention in each group individually. In half attention heads, we shift\nthe tokens by half group size, which ensures the information flow between neighboring groups.", "start_char_idx": 2371, "end_char_idx": 3427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f8c5dd6-26fb-481c-831e-711684b41a30": {"__data__": {"id_": "9f8c5dd6-26fb-481c-831e-711684b41a30", "embedding": null, "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c07a6846437159e61da96c2fa769a41ac2ae58b6b0d32f724041f03a2da043a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ecdfb7f9-71db-4368-99de-b40bd2642a44", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "e82aa4dd8cc6b1d56413b82488b9d391c2693cfc95e61b1b59cebf091dace50f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ca79a94-2bc0-4855-bbb0-c31be0cfab29", "node_type": "1", "metadata": {}, "hash": "1840a1076e4f2468676d100e673ed590b15d127d6f746d7cce7b938597f3e948", "class_name": "RelatedNodeInfo"}}, "text": "As shown in Figure 1, even with LoRA, the training\nhours for the standard Llama2 model increase substantially when the context window expands.\nIn this work, we introduce LongLoRA, an efficient fine-tuning approach that extends the context\nwindows of pre-trained LLMs, e.g., Llama2 (Touvron et al., 2023b). LoRA (Hu et al., 2022) uses\nlow-rank weight updates to approximate full fine-tuning. Similarly, we find that short attention is\nalso able to approximate long context during training. We present shifted sparse attention (S2-Attn)\nas an efficient substitute for standard self-attention. As shown in Figure 2, we split context length\ninto several groups and conduct attention in each group individually. In half attention heads, we shift\nthe tokens by half group size, which ensures the information flow between neighboring groups. For\nexample, we use S2-Attn with group size 2048 to approximate the total 8192 context length training.\nThis shares a high-level spirit with Swin Transformer (Liu et al., 2021).", "start_char_idx": 2593, "end_char_idx": 3605, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ca79a94-2bc0-4855-bbb0-c31be0cfab29": {"__data__": {"id_": "7ca79a94-2bc0-4855-bbb0-c31be0cfab29", "embedding": null, "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c07a6846437159e61da96c2fa769a41ac2ae58b6b0d32f724041f03a2da043a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f8c5dd6-26fb-481c-831e-711684b41a30", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "389ee720a4bc4dbccabb26a69d1f5eb56379a589b7b40c0e2cea669e68c7f928", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92d3c07e-4996-4422-b0da-f53f5e19ac8e", "node_type": "1", "metadata": {}, "hash": "549c2de466602e0f6c7b4483f3bc6767992e01d3324c5a0e5d031ea94f4b05ff", "class_name": "RelatedNodeInfo"}}, "text": "LoRA (Hu et al., 2022) uses\nlow-rank weight updates to approximate full fine-tuning. Similarly, we find that short attention is\nalso able to approximate long context during training. We present shifted sparse attention (S2-Attn)\nas an efficient substitute for standard self-attention. As shown in Figure 2, we split context length\ninto several groups and conduct attention in each group individually. In half attention heads, we shift\nthe tokens by half group size, which ensures the information flow between neighboring groups. For\nexample, we use S2-Attn with group size 2048 to approximate the total 8192 context length training.\nThis shares a high-level spirit with Swin Transformer (Liu et al., 2021).\nModels fine-tuned via S2-Attn retain the original attention architecture during inference. This\nfacilitates most existing optimization and infrastructure. Techniques for common LLMs can also be\napplied to ours. For example, Flash-Attention2 (Dao et al., 2022; Dao, 2023) is compatible with our\nmethod in both training and inference time.", "start_char_idx": 2899, "end_char_idx": 3943, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92d3c07e-4996-4422-b0da-f53f5e19ac8e": {"__data__": {"id_": "92d3c07e-4996-4422-b0da-f53f5e19ac8e", "embedding": null, "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c07a6846437159e61da96c2fa769a41ac2ae58b6b0d32f724041f03a2da043a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ca79a94-2bc0-4855-bbb0-c31be0cfab29", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "acc83984f96b74edf5acc009d77d80de32223d30998c7f8fca8445d3dc72f945", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b23392f-ef78-4af5-b649-cad427e33eec", "node_type": "1", "metadata": {}, "hash": "f4c0545b653e75b4f4fb19ba23eb150bebc1502f073debcdf53011b45c847e21", "class_name": "RelatedNodeInfo"}}, "text": "As shown in Figure 2, we split context length\ninto several groups and conduct attention in each group individually. In half attention heads, we shift\nthe tokens by half group size, which ensures the information flow between neighboring groups. For\nexample, we use S2-Attn with group size 2048 to approximate the total 8192 context length training.\nThis shares a high-level spirit with Swin Transformer (Liu et al., 2021).\nModels fine-tuned via S2-Attn retain the original attention architecture during inference. This\nfacilitates most existing optimization and infrastructure. Techniques for common LLMs can also be\napplied to ours. For example, Flash-Attention2 (Dao et al., 2022; Dao, 2023) is compatible with our\nmethod in both training and inference time. The reason behind this is that short attention resembles\nthe attention scheme in the pre-training stage of LLMs. Other efficient attentions, e.g., dilated or\nsparse attention, have a large gap to the standard style and do not work well like ours, as in Table 6.", "start_char_idx": 3184, "end_char_idx": 4205, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b23392f-ef78-4af5-b649-cad427e33eec": {"__data__": {"id_": "3b23392f-ef78-4af5-b649-cad427e33eec", "embedding": null, "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d29ca23e-6ef3-4619-8c0c-64dc12de09ea", "node_type": "4", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c07a6846437159e61da96c2fa769a41ac2ae58b6b0d32f724041f03a2da043a8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92d3c07e-4996-4422-b0da-f53f5e19ac8e", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "d3ad8de3a971dd0a9d50ec12511294d2562aa78bad172cc0023f070442b5a416", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "941e6e63-0642-4911-93c3-1a436c8d6f75", "node_type": "1", "metadata": {}, "hash": "3c4db906aabd3902c8f87478d739714bb02cc0b3b6600bfeb36aa6ae96352d90", "class_name": "RelatedNodeInfo"}}, "text": "For\nexample, we use S2-Attn with group size 2048 to approximate the total 8192 context length training.\nThis shares a high-level spirit with Swin Transformer (Liu et al., 2021).\nModels fine-tuned via S2-Attn retain the original attention architecture during inference. This\nfacilitates most existing optimization and infrastructure. Techniques for common LLMs can also be\napplied to ours. For example, Flash-Attention2 (Dao et al., 2022; Dao, 2023) is compatible with our\nmethod in both training and inference time. The reason behind this is that short attention resembles\nthe attention scheme in the pre-training stage of LLMs. Other efficient attentions, e.g., dilated or\nsparse attention, have a large gap to the standard style and do not work well like ours, as in Table 6.\nWe empirically show that learnable embedding and normalization layers are the key to unlocking\nlong context LoRA fine-tuning, in Table 2. Embedding and normalization layers take up a small\n2", "start_char_idx": 3428, "end_char_idx": 4396, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "941e6e63-0642-4911-93c3-1a436c8d6f75": {"__data__": {"id_": "941e6e63-0642-4911-93c3-1a436c8d6f75", "embedding": null, "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "831b1971-2e28-49f8-814b-0a40a7f215fa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4963fc5809ddfe8a173611fdd30ab87b4fdc33ecef856d6e0bfef54db617ff6b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b23392f-ef78-4af5-b649-cad427e33eec", "node_type": "1", "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "9e11c5ad61f1d8c840927b8e9c4910b4827b943d83d148ae3be3e57ec06deb1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e40c9e7d-d78a-46f6-8b38-b6f0cac92220", "node_type": "1", "metadata": {}, "hash": "eb220ae38c2db473955233b5e712de4d0c91ba0ea805985b2077ddab69a8e2ce", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nStep 2Shi$ tokensStep 3ReshapeGroupsizeHalf group\nSelf-a8en9on in each groupTokensMul9-headsSelf-a8en9on among all tokensSelf-attentionShifted short attentionStep 1Split headsHalf heads\nStep 2Shi$Step 3GroupHalf\nS2-AttnStep 1SplitHalf headsAttention w/o shift!\"Attention w shift12345678TokensHeads123456788123456712345678123456781234567881234567Group\nPa8ern1!\"Pa8ern2Pa8ern1Pa8ern2InputsSplit a8en9on headsinto 2 partsShi$ the 2ndpartby half group\nFigure 3: Illustration of S2-Attn . It involves three steps. First, it splits features along the head\ndimension into two chunks. Second, tokens in one of the chunks are shifted by half of the group size.\nThird, we split tokens into groups and reshape them into batch dimensions. Attention only computes\nin each group in ours while the information flows between groups via shifting.", "start_char_idx": 0, "end_char_idx": 874, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e40c9e7d-d78a-46f6-8b38-b6f0cac92220": {"__data__": {"id_": "e40c9e7d-d78a-46f6-8b38-b6f0cac92220", "embedding": null, "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "831b1971-2e28-49f8-814b-0a40a7f215fa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4963fc5809ddfe8a173611fdd30ab87b4fdc33ecef856d6e0bfef54db617ff6b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "941e6e63-0642-4911-93c3-1a436c8d6f75", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "30d8105994bd4f9570cb93580154b53504924048b01c9537afd881f02aec2b65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "244a53aa-238d-4837-8a03-9702012af209", "node_type": "1", "metadata": {}, "hash": "4f8c5c09bc169ce8c0dcfc3b6cef46cc0847f772e8890fc52db61606e4189b00", "class_name": "RelatedNodeInfo"}}, "text": "\"Attention w shift12345678TokensHeads123456788123456712345678123456781234567881234567Group\nPa8ern1!\"Pa8ern2Pa8ern1Pa8ern2InputsSplit a8en9on headsinto 2 partsShi$ the 2ndpartby half group\nFigure 3: Illustration of S2-Attn . It involves three steps. First, it splits features along the head\ndimension into two chunks. Second, tokens in one of the chunks are shifted by half of the group size.\nThird, we split tokens into groups and reshape them into batch dimensions. Attention only computes\nin each group in ours while the information flows between groups via shifting. Potential information\nleakage might be introduced by shifting, while this is easy to prevent via a small modification on the\nattention mask. We ablate this in the variant 2 in Section B.3 in the appendix.\nproportion of parameters in the entire LLM. For example, embedding has ( <2%) parameters, and\nnormalization has ( \u22640.004%) parameters in Llama2 7B. This ratio decreases for even larger LLMs.", "start_char_idx": 305, "end_char_idx": 1270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "244a53aa-238d-4837-8a03-9702012af209": {"__data__": {"id_": "244a53aa-238d-4837-8a03-9702012af209", "embedding": null, "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "831b1971-2e28-49f8-814b-0a40a7f215fa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4963fc5809ddfe8a173611fdd30ab87b4fdc33ecef856d6e0bfef54db617ff6b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e40c9e7d-d78a-46f6-8b38-b6f0cac92220", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "e0c418b44bfd183798d82876adb3663bb5d491f6ada097f37b381d21ff897d81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8c99bef-a6dc-458c-9f15-a7129365f7c9", "node_type": "1", "metadata": {}, "hash": "d3ff477570d1e455bccd33406d2c3390b44d416a6a7e8ba39b07000f99c5a236", "class_name": "RelatedNodeInfo"}}, "text": "It involves three steps. First, it splits features along the head\ndimension into two chunks. Second, tokens in one of the chunks are shifted by half of the group size.\nThird, we split tokens into groups and reshape them into batch dimensions. Attention only computes\nin each group in ours while the information flows between groups via shifting. Potential information\nleakage might be introduced by shifting, while this is easy to prevent via a small modification on the\nattention mask. We ablate this in the variant 2 in Section B.3 in the appendix.\nproportion of parameters in the entire LLM. For example, embedding has ( <2%) parameters, and\nnormalization has ( \u22640.004%) parameters in Llama2 7B. This ratio decreases for even larger LLMs.\nIn experiments, we show that LongLoRA is effective and efficient. We present experimental results\nof extending the context window for Llama2 7B, 13B, and 70B. Following the experimental settings\nof Position Interpolation (Chen et al., 2023), we fine-tune models with proper position embeddings.", "start_char_idx": 529, "end_char_idx": 1565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8c99bef-a6dc-458c-9f15-a7129365f7c9": {"__data__": {"id_": "d8c99bef-a6dc-458c-9f15-a7129365f7c9", "embedding": null, "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "831b1971-2e28-49f8-814b-0a40a7f215fa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4963fc5809ddfe8a173611fdd30ab87b4fdc33ecef856d6e0bfef54db617ff6b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "244a53aa-238d-4837-8a03-9702012af209", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "5bc7eecb8cc2eee7707bdf2ac8d2cadb3b5f1a0ad872df8c00dc25aa8d3c062e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "998f049d-b5d3-4bd5-80d4-5f2814e83138", "node_type": "1", "metadata": {}, "hash": "afec2d2d4b038a4b14a7ccf1c4d76e15ae3c943cea3a272616d378bf3c229d41", "class_name": "RelatedNodeInfo"}}, "text": "Attention only computes\nin each group in ours while the information flows between groups via shifting. Potential information\nleakage might be introduced by shifting, while this is easy to prevent via a small modification on the\nattention mask. We ablate this in the variant 2 in Section B.3 in the appendix.\nproportion of parameters in the entire LLM. For example, embedding has ( <2%) parameters, and\nnormalization has ( \u22640.004%) parameters in Llama2 7B. This ratio decreases for even larger LLMs.\nIn experiments, we show that LongLoRA is effective and efficient. We present experimental results\nof extending the context window for Llama2 7B, 13B, and 70B. Following the experimental settings\nof Position Interpolation (Chen et al., 2023), we fine-tune models with proper position embeddings.\nThe trained models achieve comparable performance to the full-attention and fully fine-tuned results,\nwhile the computational cost is much less as shown in Figure 1.", "start_char_idx": 772, "end_char_idx": 1731, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "998f049d-b5d3-4bd5-80d4-5f2814e83138": {"__data__": {"id_": "998f049d-b5d3-4bd5-80d4-5f2814e83138", "embedding": null, "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "831b1971-2e28-49f8-814b-0a40a7f215fa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4963fc5809ddfe8a173611fdd30ab87b4fdc33ecef856d6e0bfef54db617ff6b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8c99bef-a6dc-458c-9f15-a7129365f7c9", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "05abc2436fd32c24fe3dd4248aa162a5a9f4e5ccda34a903564fa3a7b5dd3d9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80e443ac-edbf-4a93-8662-05614bf6a340", "node_type": "1", "metadata": {}, "hash": "77b12e1809c60ca3871fdeaf6c51de6f125eee44ade02aa96309263f18b2b5f4", "class_name": "RelatedNodeInfo"}}, "text": "We ablate this in the variant 2 in Section B.3 in the appendix.\nproportion of parameters in the entire LLM. For example, embedding has ( <2%) parameters, and\nnormalization has ( \u22640.004%) parameters in Llama2 7B. This ratio decreases for even larger LLMs.\nIn experiments, we show that LongLoRA is effective and efficient. We present experimental results\nof extending the context window for Llama2 7B, 13B, and 70B. Following the experimental settings\nof Position Interpolation (Chen et al., 2023), we fine-tune models with proper position embeddings.\nThe trained models achieve comparable performance to the full-attention and fully fine-tuned results,\nwhile the computational cost is much less as shown in Figure 1. LongLoRA can fine-tune Llama2 7B\nup to 100k context, or a 70B model up to 32k, on a single 8\u00d7A100 machine.\nIn addition, we present a solution for supervised fine-tuning (SFT) with our self-collected long\ninstruction-following dataset, LongAlpaca.", "start_char_idx": 1016, "end_char_idx": 1978, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80e443ac-edbf-4a93-8662-05614bf6a340": {"__data__": {"id_": "80e443ac-edbf-4a93-8662-05614bf6a340", "embedding": null, "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "831b1971-2e28-49f8-814b-0a40a7f215fa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4963fc5809ddfe8a173611fdd30ab87b4fdc33ecef856d6e0bfef54db617ff6b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "998f049d-b5d3-4bd5-80d4-5f2814e83138", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "35c90b3f76a66b6904cec808da592d302c03e686c9bbda07fa20e9fa8ac31521", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e2c1c46-67c2-4fc0-98cd-9c9fce6549a4", "node_type": "1", "metadata": {}, "hash": "e9debd4ef1cc0bce791e6c358a213ba537d4e88f2f695769efc0d0d0ac329466", "class_name": "RelatedNodeInfo"}}, "text": "This ratio decreases for even larger LLMs.\nIn experiments, we show that LongLoRA is effective and efficient. We present experimental results\nof extending the context window for Llama2 7B, 13B, and 70B. Following the experimental settings\nof Position Interpolation (Chen et al., 2023), we fine-tune models with proper position embeddings.\nThe trained models achieve comparable performance to the full-attention and fully fine-tuned results,\nwhile the computational cost is much less as shown in Figure 1. LongLoRA can fine-tune Llama2 7B\nup to 100k context, or a 70B model up to 32k, on a single 8\u00d7A100 machine.\nIn addition, we present a solution for supervised fine-tuning (SFT) with our self-collected long\ninstruction-following dataset, LongAlpaca. Our LongLoRA models are further fine-tuned with long\nquestions and the corresponding answers. We design various types of questions for technical papers,\nscience fiction, and other books. SFT is important for improving the chat ability of LLMs. We\nintroduce our SFT settings in Section B.6 in the appendix.", "start_char_idx": 1228, "end_char_idx": 2284, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e2c1c46-67c2-4fc0-98cd-9c9fce6549a4": {"__data__": {"id_": "7e2c1c46-67c2-4fc0-98cd-9c9fce6549a4", "embedding": null, "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "831b1971-2e28-49f8-814b-0a40a7f215fa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4963fc5809ddfe8a173611fdd30ab87b4fdc33ecef856d6e0bfef54db617ff6b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80e443ac-edbf-4a93-8662-05614bf6a340", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "cb599b712bc16b2a65a4d3b063195f250d3e9af5d55a64e26e27e72a42127949", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89a3eb54-3214-4003-adc7-5995d9c40349", "node_type": "1", "metadata": {}, "hash": "70fe0959402bf7249acc06d12c494b70a6789cf43f87ef18c4b8eebbb2a9ffb3", "class_name": "RelatedNodeInfo"}}, "text": "The trained models achieve comparable performance to the full-attention and fully fine-tuned results,\nwhile the computational cost is much less as shown in Figure 1. LongLoRA can fine-tune Llama2 7B\nup to 100k context, or a 70B model up to 32k, on a single 8\u00d7A100 machine.\nIn addition, we present a solution for supervised fine-tuning (SFT) with our self-collected long\ninstruction-following dataset, LongAlpaca. Our LongLoRA models are further fine-tuned with long\nquestions and the corresponding answers. We design various types of questions for technical papers,\nscience fiction, and other books. SFT is important for improving the chat ability of LLMs. We\nintroduce our SFT settings in Section B.6 in the appendix.\n2 R ELATED WORK\nLong-context Transformers. A large body of research has been developed to increase the context\nlength of transformers.", "start_char_idx": 1566, "end_char_idx": 2419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89a3eb54-3214-4003-adc7-5995d9c40349": {"__data__": {"id_": "89a3eb54-3214-4003-adc7-5995d9c40349", "embedding": null, "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "831b1971-2e28-49f8-814b-0a40a7f215fa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4963fc5809ddfe8a173611fdd30ab87b4fdc33ecef856d6e0bfef54db617ff6b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e2c1c46-67c2-4fc0-98cd-9c9fce6549a4", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "71e8fc27d214bb8af9414722abf6949817e4586f47c613f3a97f493012d4ced2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "180498c1-ec3e-49ed-b945-0914ef8c282a", "node_type": "1", "metadata": {}, "hash": "834247e2e598fccd1c3073df488a2e4d537ecfd1fb44b47c824f4b66e2a96180", "class_name": "RelatedNodeInfo"}}, "text": "LongLoRA can fine-tune Llama2 7B\nup to 100k context, or a 70B model up to 32k, on a single 8\u00d7A100 machine.\nIn addition, we present a solution for supervised fine-tuning (SFT) with our self-collected long\ninstruction-following dataset, LongAlpaca. Our LongLoRA models are further fine-tuned with long\nquestions and the corresponding answers. We design various types of questions for technical papers,\nscience fiction, and other books. SFT is important for improving the chat ability of LLMs. We\nintroduce our SFT settings in Section B.6 in the appendix.\n2 R ELATED WORK\nLong-context Transformers. A large body of research has been developed to increase the context\nlength of transformers. Some of these approaches are retrieval-based (Karpukhin et al., 2020; Izacard\net al., 2022; Guu et al., 2020), which augment language models via fetching related documents and\nincluding the retrieved results into contexts. Our work is complementary to these works, as our\nattention mechanism is unmodified during inference.", "start_char_idx": 1732, "end_char_idx": 2743, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "180498c1-ec3e-49ed-b945-0914ef8c282a": {"__data__": {"id_": "180498c1-ec3e-49ed-b945-0914ef8c282a", "embedding": null, "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "831b1971-2e28-49f8-814b-0a40a7f215fa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4963fc5809ddfe8a173611fdd30ab87b4fdc33ecef856d6e0bfef54db617ff6b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89a3eb54-3214-4003-adc7-5995d9c40349", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "806849044295f709a79f95790de012adb246234c57b11852213a0172dc54ae27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00bec5fd-2fda-407f-af14-2cd7fe9e18ef", "node_type": "1", "metadata": {}, "hash": "8308d66b82b8902c041bc1070718c5f408f6e2f99783659690f1f18b3a9b937a", "class_name": "RelatedNodeInfo"}}, "text": "Our LongLoRA models are further fine-tuned with long\nquestions and the corresponding answers. We design various types of questions for technical papers,\nscience fiction, and other books. SFT is important for improving the chat ability of LLMs. We\nintroduce our SFT settings in Section B.6 in the appendix.\n2 R ELATED WORK\nLong-context Transformers. A large body of research has been developed to increase the context\nlength of transformers. Some of these approaches are retrieval-based (Karpukhin et al., 2020; Izacard\net al., 2022; Guu et al., 2020), which augment language models via fetching related documents and\nincluding the retrieved results into contexts. Our work is complementary to these works, as our\nattention mechanism is unmodified during inference. Many works modify multi-head attention to be\napproximated ones (Wang et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020;\nBulatov et al., 2022; Ding et al., 2023; Qiu et al., 2020).", "start_char_idx": 1979, "end_char_idx": 2950, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00bec5fd-2fda-407f-af14-2cd7fe9e18ef": {"__data__": {"id_": "00bec5fd-2fda-407f-af14-2cd7fe9e18ef", "embedding": null, "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "831b1971-2e28-49f8-814b-0a40a7f215fa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4963fc5809ddfe8a173611fdd30ab87b4fdc33ecef856d6e0bfef54db617ff6b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "180498c1-ec3e-49ed-b945-0914ef8c282a", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "2a4b23984e09e92a6f0a3a3da84596a17206e15332328ee44c1c411f714ebf13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80b5b7dc-f200-4b30-8526-224b6537b1e5", "node_type": "1", "metadata": {}, "hash": "7acd7eade6f0f541b603330f66c051dd9a9aedb6bd6fa6c5d4eed96104a34ce7", "class_name": "RelatedNodeInfo"}}, "text": "2 R ELATED WORK\nLong-context Transformers. A large body of research has been developed to increase the context\nlength of transformers. Some of these approaches are retrieval-based (Karpukhin et al., 2020; Izacard\net al., 2022; Guu et al., 2020), which augment language models via fetching related documents and\nincluding the retrieved results into contexts. Our work is complementary to these works, as our\nattention mechanism is unmodified during inference. Many works modify multi-head attention to be\napproximated ones (Wang et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020;\nBulatov et al., 2022; Ding et al., 2023; Qiu et al., 2020). They alleviate the quadratic complexity of\nthe self-attention computation. For example, Longformer (Beltagy et al., 2020) and BigBird (Zaheer\net al., 2020) use sparse attention to handle long sequences.", "start_char_idx": 2285, "end_char_idx": 3153, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80b5b7dc-f200-4b30-8526-224b6537b1e5": {"__data__": {"id_": "80b5b7dc-f200-4b30-8526-224b6537b1e5", "embedding": null, "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "831b1971-2e28-49f8-814b-0a40a7f215fa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4963fc5809ddfe8a173611fdd30ab87b4fdc33ecef856d6e0bfef54db617ff6b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00bec5fd-2fda-407f-af14-2cd7fe9e18ef", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "b0d60346b4b36917bc8c9b29b92dfd6afc64c6f2ca0ec4a403d25c461ab1748b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb70aaa1-0326-41cd-9992-8b98cc90d3d7", "node_type": "1", "metadata": {}, "hash": "92cac1e0c9a8f1b26838d5e595a5dde0111922ffbb24436abe7f4f1e08ccf9a3", "class_name": "RelatedNodeInfo"}}, "text": "Our work is complementary to these works, as our\nattention mechanism is unmodified during inference. Many works modify multi-head attention to be\napproximated ones (Wang et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Kitaev et al., 2020;\nBulatov et al., 2022; Ding et al., 2023; Qiu et al., 2020). They alleviate the quadratic complexity of\nthe self-attention computation. For example, Longformer (Beltagy et al., 2020) and BigBird (Zaheer\net al., 2020) use sparse attention to handle long sequences. Other works (Wu et al., 2022; Bulatov\net al., 2022) utilize memory mechanisms as a compression on past inputs, to look up relevant tokens.\nOne limitation of these works is that these compressions have a large gap to full attention, making\nit infeasible to fine-tune pre-trained LLMs. Although our work also involves an approximation of\nattention mechanism, it has a similar shape and a small gap to standard attention.", "start_char_idx": 2643, "end_char_idx": 3572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb70aaa1-0326-41cd-9992-8b98cc90d3d7": {"__data__": {"id_": "fb70aaa1-0326-41cd-9992-8b98cc90d3d7", "embedding": null, "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "831b1971-2e28-49f8-814b-0a40a7f215fa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4963fc5809ddfe8a173611fdd30ab87b4fdc33ecef856d6e0bfef54db617ff6b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80b5b7dc-f200-4b30-8526-224b6537b1e5", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "634a5cab0cc138fd8e62c0b2226a9bdc1eab142521e1baa713fbd7c9defc81ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87c2b0f5-8ae0-4338-b26f-a1ac43d16bcb", "node_type": "1", "metadata": {}, "hash": "bab5acdd5b58d64bdb402a652f91935d2e25d4a6b5ea85616b7fbd72fc161a22", "class_name": "RelatedNodeInfo"}}, "text": "They alleviate the quadratic complexity of\nthe self-attention computation. For example, Longformer (Beltagy et al., 2020) and BigBird (Zaheer\net al., 2020) use sparse attention to handle long sequences. Other works (Wu et al., 2022; Bulatov\net al., 2022) utilize memory mechanisms as a compression on past inputs, to look up relevant tokens.\nOne limitation of these works is that these compressions have a large gap to full attention, making\nit infeasible to fine-tune pre-trained LLMs. Although our work also involves an approximation of\nattention mechanism, it has a similar shape and a small gap to standard attention. This enables\nfine-tuning pre-trained LLMs on S2-Attn and maintain full attention during inference.\nLong-context LLMs. LLMs are typically pre-trained with a pre-defined context length, such as\n2048 for LLaMA (Touvron et al., 2023a) and 4096 for Llama2 (Touvron et al., 2023b). Training LLMs\nwith long context from scratch is prohibitively expensive for most researchers.", "start_char_idx": 2951, "end_char_idx": 3942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87c2b0f5-8ae0-4338-b26f-a1ac43d16bcb": {"__data__": {"id_": "87c2b0f5-8ae0-4338-b26f-a1ac43d16bcb", "embedding": null, "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "831b1971-2e28-49f8-814b-0a40a7f215fa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4963fc5809ddfe8a173611fdd30ab87b4fdc33ecef856d6e0bfef54db617ff6b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb70aaa1-0326-41cd-9992-8b98cc90d3d7", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c7dff94c052d28340d28634940c177f3cdc4fe991d83abc2fc3e280287d90638", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76561b4e-24e4-49d9-a25f-1655f512412e", "node_type": "1", "metadata": {}, "hash": "43ef3c1ffedb5547a074109a0d29c1eafcf207cddedb07a2964df031f66f7a32", "class_name": "RelatedNodeInfo"}}, "text": "One limitation of these works is that these compressions have a large gap to full attention, making\nit infeasible to fine-tune pre-trained LLMs. Although our work also involves an approximation of\nattention mechanism, it has a similar shape and a small gap to standard attention. This enables\nfine-tuning pre-trained LLMs on S2-Attn and maintain full attention during inference.\nLong-context LLMs. LLMs are typically pre-trained with a pre-defined context length, such as\n2048 for LLaMA (Touvron et al., 2023a) and 4096 for Llama2 (Touvron et al., 2023b). Training LLMs\nwith long context from scratch is prohibitively expensive for most researchers. Recently, several\nworks have tried to extend the context length of LLMs via fine-tuning. Position Interpolation (Chen\net al., 2023) modifies rotary position encoding (Su et al., 2021) and extends the context length of\nLLaMA to 32768. Focused Transformer (Tworkowski et al., 2023) utilizes contrastive learning\nto train LongLLaMA.", "start_char_idx": 3293, "end_char_idx": 4272, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76561b4e-24e4-49d9-a25f-1655f512412e": {"__data__": {"id_": "76561b4e-24e4-49d9-a25f-1655f512412e", "embedding": null, "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "831b1971-2e28-49f8-814b-0a40a7f215fa", "node_type": "4", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4963fc5809ddfe8a173611fdd30ab87b4fdc33ecef856d6e0bfef54db617ff6b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87c2b0f5-8ae0-4338-b26f-a1ac43d16bcb", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "44d5855f7294ac007ae03c804e8d037f44ec6a947166b30a2a4e85f45dfb15a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e28e121b-2c44-48c9-8eba-90d578892858", "node_type": "1", "metadata": {}, "hash": "322331becd9488c0a0f67c1f56d984ec8c7b11b20effdedeb4749e2a1a4ca068", "class_name": "RelatedNodeInfo"}}, "text": "This enables\nfine-tuning pre-trained LLMs on S2-Attn and maintain full attention during inference.\nLong-context LLMs. LLMs are typically pre-trained with a pre-defined context length, such as\n2048 for LLaMA (Touvron et al., 2023a) and 4096 for Llama2 (Touvron et al., 2023b). Training LLMs\nwith long context from scratch is prohibitively expensive for most researchers. Recently, several\nworks have tried to extend the context length of LLMs via fine-tuning. Position Interpolation (Chen\net al., 2023) modifies rotary position encoding (Su et al., 2021) and extends the context length of\nLLaMA to 32768. Focused Transformer (Tworkowski et al., 2023) utilizes contrastive learning\nto train LongLLaMA. Both of them rely on full fine-tuning, which is computationally expensive\n(128 A100 GPUs / 128 TPUv3 for training). Landmark attention (Mohtashami & Jaggi, 2023) is an\n3", "start_char_idx": 3573, "end_char_idx": 4442, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e28e121b-2c44-48c9-8eba-90d578892858": {"__data__": {"id_": "e28e121b-2c44-48c9-8eba-90d578892858", "embedding": null, "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a805057fee31841cba092e63eb61b704af42513c8e2bf73b6bc44fc6d5e15712", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76561b4e-24e4-49d9-a25f-1655f512412e", "node_type": "1", "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "e987e45372129cc04e10b430a11703b9fc953e456f84e9ede669210e417d6ddb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9edb9de7-6ee9-421f-9e81-fb3d60d13fd5", "node_type": "1", "metadata": {}, "hash": "6a047fb80b647f9d1c5ff9bd46894c8163e94aa83da6925d975eae20530d97f3", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 1: Effectiveness of S2-Attn under different context lengths . \u2018Short\u2019 means 1/4 of the target\ncontext length, while \u2018Long\u2019 equals to the target context length. Models are fully fine-tuned upon\na Llama2 (Touvron et al., 2023b) model with 7B parameters on the RedPajama (Computer, 2023)\ndataset. Results are tested in perplexity on PG19 (Rae et al., 2020) validation split.\nSetting Position EmbeddingTraining Target Context Length\nAttention Shift 8192 16384 32768\nFull Attn\nPI (Chen et al., 2023)Long - 8.02 8.05 8.04\nShort Attn Short \u2717 8.29 8.83 9.47\nS2-Attn Short \u2713 8.04 8.03 8.08\nefficient approach, but somewhat lossy. It compresses long context inputs into retrieved tokens. Our\nmethod saves substantial fine-tuning costs, while preserving the quality of the original attention.", "start_char_idx": 0, "end_char_idx": 832, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9edb9de7-6ee9-421f-9e81-fb3d60d13fd5": {"__data__": {"id_": "9edb9de7-6ee9-421f-9e81-fb3d60d13fd5", "embedding": null, "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a805057fee31841cba092e63eb61b704af42513c8e2bf73b6bc44fc6d5e15712", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e28e121b-2c44-48c9-8eba-90d578892858", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "dba274dc1f9433bd6a903526574782edeb819036884623975080a2391b134a9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a4e4613-5acb-4380-9bde-c86134bcf165", "node_type": "1", "metadata": {}, "hash": "ff9a5ca11224909d017efb3b349c98438b5b782be2db4bf77b6ef5b239cee1e0", "class_name": "RelatedNodeInfo"}}, "text": "Models are fully fine-tuned upon\na Llama2 (Touvron et al., 2023b) model with 7B parameters on the RedPajama (Computer, 2023)\ndataset. Results are tested in perplexity on PG19 (Rae et al., 2020) validation split.\nSetting Position EmbeddingTraining Target Context Length\nAttention Shift 8192 16384 32768\nFull Attn\nPI (Chen et al., 2023)Long - 8.02 8.05 8.04\nShort Attn Short \u2717 8.29 8.83 9.47\nS2-Attn Short \u2713 8.04 8.03 8.08\nefficient approach, but somewhat lossy. It compresses long context inputs into retrieved tokens. Our\nmethod saves substantial fine-tuning costs, while preserving the quality of the original attention. Ours\nmaintain full access to the entire input via unmodified attention during inference.", "start_char_idx": 211, "end_char_idx": 921, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a4e4613-5acb-4380-9bde-c86134bcf165": {"__data__": {"id_": "3a4e4613-5acb-4380-9bde-c86134bcf165", "embedding": null, "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a805057fee31841cba092e63eb61b704af42513c8e2bf73b6bc44fc6d5e15712", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9edb9de7-6ee9-421f-9e81-fb3d60d13fd5", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "36adfd96403486653ca29e8aed2b8ede6cc45bf7c526f946871746219bf090e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12c32e20-ea7c-40df-b99f-b9cdd50428ed", "node_type": "1", "metadata": {}, "hash": "85675c8ed07d4e7fc63b3ebebe6e91b682f56c63758f278ec8cfbf25b72dd8fd", "class_name": "RelatedNodeInfo"}}, "text": "Results are tested in perplexity on PG19 (Rae et al., 2020) validation split.\nSetting Position EmbeddingTraining Target Context Length\nAttention Shift 8192 16384 32768\nFull Attn\nPI (Chen et al., 2023)Long - 8.02 8.05 8.04\nShort Attn Short \u2717 8.29 8.83 9.47\nS2-Attn Short \u2713 8.04 8.03 8.08\nefficient approach, but somewhat lossy. It compresses long context inputs into retrieved tokens. Our\nmethod saves substantial fine-tuning costs, while preserving the quality of the original attention. Ours\nmaintain full access to the entire input via unmodified attention during inference.\nSome literature focuses on the position embedding modification of LLMs for long context extension,\nincluding Position Interpolation (Chen et al., 2023), NTK-aware (ntk, 2023), Yarn (Peng et al., 2023),\npositional Skipping (Zhu et al., 2023), and methods based on out-of-distribution analysis (Han et al.,\n2023).", "start_char_idx": 345, "end_char_idx": 1233, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12c32e20-ea7c-40df-b99f-b9cdd50428ed": {"__data__": {"id_": "12c32e20-ea7c-40df-b99f-b9cdd50428ed", "embedding": null, "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a805057fee31841cba092e63eb61b704af42513c8e2bf73b6bc44fc6d5e15712", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a4e4613-5acb-4380-9bde-c86134bcf165", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "bc4ba28b97290aff33d221860f90e62ccf3cb937bb009656237e879fe04e71c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9abd727d-3ef9-4096-aa23-0c4fbed05e98", "node_type": "1", "metadata": {}, "hash": "30077cc16aca16d7a28cc0da3f6b1136a5f564a5951ffb48207e462c808574ec", "class_name": "RelatedNodeInfo"}}, "text": "It compresses long context inputs into retrieved tokens. Our\nmethod saves substantial fine-tuning costs, while preserving the quality of the original attention. Ours\nmaintain full access to the entire input via unmodified attention during inference.\nSome literature focuses on the position embedding modification of LLMs for long context extension,\nincluding Position Interpolation (Chen et al., 2023), NTK-aware (ntk, 2023), Yarn (Peng et al., 2023),\npositional Skipping (Zhu et al., 2023), and methods based on out-of-distribution analysis (Han et al.,\n2023). Our method focuses on efficient fine-tuning and retaining the original architecture during\ninference, which is orthogonal to these position embedding methods.\nEfficient Fine-tuning. This work is based on LoRA (Hu et al., 2022), a classical efficient fine-tuning\napproach.", "start_char_idx": 672, "end_char_idx": 1505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9abd727d-3ef9-4096-aa23-0c4fbed05e98": {"__data__": {"id_": "9abd727d-3ef9-4096-aa23-0c4fbed05e98", "embedding": null, "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a805057fee31841cba092e63eb61b704af42513c8e2bf73b6bc44fc6d5e15712", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12c32e20-ea7c-40df-b99f-b9cdd50428ed", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a594d77697d07c7771822152d73a6c024359fcdf7e3a6760a5cf49574a7c31bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b304b51-214e-4e96-9ae5-bfd8b35cde6b", "node_type": "1", "metadata": {}, "hash": "b9557e2cda70d8b14603859d17a52294a412f7b6d6b35f1fe8728701c595e8e3", "class_name": "RelatedNodeInfo"}}, "text": "It compresses long context inputs into retrieved tokens. Our\nmethod saves substantial fine-tuning costs, while preserving the quality of the original attention. Ours\nmaintain full access to the entire input via unmodified attention during inference.\nSome literature focuses on the position embedding modification of LLMs for long context extension,\nincluding Position Interpolation (Chen et al., 2023), NTK-aware (ntk, 2023), Yarn (Peng et al., 2023),\npositional Skipping (Zhu et al., 2023), and methods based on out-of-distribution analysis (Han et al.,\n2023). Our method focuses on efficient fine-tuning and retaining the original architecture during\ninference, which is orthogonal to these position embedding methods.\nEfficient Fine-tuning. This work is based on LoRA (Hu et al., 2022), a classical efficient fine-tuning\napproach. In addition to LoRA (Hu et al., 2022), there are many other parameter-efficient fine-tuning\nmethods, including prompt tuning (Lester et al., 2021), prefix tuning (Li & Liang, 2021), hidden state\ntuning (Liu et al., 2022), bias tuning (Zaken et al., 2022), and masked weight learning (Sung et al.,\n2021).", "start_char_idx": 672, "end_char_idx": 1809, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b304b51-214e-4e96-9ae5-bfd8b35cde6b": {"__data__": {"id_": "6b304b51-214e-4e96-9ae5-bfd8b35cde6b", "embedding": null, "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a805057fee31841cba092e63eb61b704af42513c8e2bf73b6bc44fc6d5e15712", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9abd727d-3ef9-4096-aa23-0c4fbed05e98", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "468db5a670d33c1ff852c7e31be40fac52e5adba0b2bc547f2d4a4caade04df0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "923f20c4-8403-47b8-ad81-87bcc3e811a4", "node_type": "1", "metadata": {}, "hash": "b95eb6eb61de220097f7a492e901f67293c1c68a8f9d1c9fa62c4e78325758bf", "class_name": "RelatedNodeInfo"}}, "text": "Our method focuses on efficient fine-tuning and retaining the original architecture during\ninference, which is orthogonal to these position embedding methods.\nEfficient Fine-tuning. This work is based on LoRA (Hu et al., 2022), a classical efficient fine-tuning\napproach. In addition to LoRA (Hu et al., 2022), there are many other parameter-efficient fine-tuning\nmethods, including prompt tuning (Lester et al., 2021), prefix tuning (Li & Liang, 2021), hidden state\ntuning (Liu et al., 2022), bias tuning (Zaken et al., 2022), and masked weight learning (Sung et al.,\n2021). Input-tuning (An et al., 2022) introduces an adapter to tune input embedding. Although the\ninput embedding layers are also trainable in ours, this is not enough for long context extension. We\nmake a comprehensive analysis on layer types in experiments, in Table 2. Existing work (Chen et al.,\n2022) shows sparse masks can effectively save training costs and avoid performance drops.\n3 L ONG LORA\n3.1 B ACKGROUND\nTransformer.", "start_char_idx": 1234, "end_char_idx": 2234, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "923f20c4-8403-47b8-ad81-87bcc3e811a4": {"__data__": {"id_": "923f20c4-8403-47b8-ad81-87bcc3e811a4", "embedding": null, "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a805057fee31841cba092e63eb61b704af42513c8e2bf73b6bc44fc6d5e15712", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b304b51-214e-4e96-9ae5-bfd8b35cde6b", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "cd0a8aa378241ae7cab9a081fc0ed234a1e4f31ba3bc66883fbff5b083d9d66d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9f15394-3e6a-4dfc-a217-ae33b09c0db9", "node_type": "1", "metadata": {}, "hash": "cbc2cbe0ef72bc546183fb10a0df4d0fee7b9fcd3886e9561796df799dd4a69c", "class_name": "RelatedNodeInfo"}}, "text": "In addition to LoRA (Hu et al., 2022), there are many other parameter-efficient fine-tuning\nmethods, including prompt tuning (Lester et al., 2021), prefix tuning (Li & Liang, 2021), hidden state\ntuning (Liu et al., 2022), bias tuning (Zaken et al., 2022), and masked weight learning (Sung et al.,\n2021). Input-tuning (An et al., 2022) introduces an adapter to tune input embedding. Although the\ninput embedding layers are also trainable in ours, this is not enough for long context extension. We\nmake a comprehensive analysis on layer types in experiments, in Table 2. Existing work (Chen et al.,\n2022) shows sparse masks can effectively save training costs and avoid performance drops.\n3 L ONG LORA\n3.1 B ACKGROUND\nTransformer. LLMs are typically built with transformers. Taking Llama2 (Touvron et al., 2023b)\nfor example, as shown in Figure 2, an LLM model consists of an embedding input layer and a number\nof decoder layers. Each decoder layer comprises a self-attention module.", "start_char_idx": 1506, "end_char_idx": 2487, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9f15394-3e6a-4dfc-a217-ae33b09c0db9": {"__data__": {"id_": "c9f15394-3e6a-4dfc-a217-ae33b09c0db9", "embedding": null, "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a805057fee31841cba092e63eb61b704af42513c8e2bf73b6bc44fc6d5e15712", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "923f20c4-8403-47b8-ad81-87bcc3e811a4", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "638cd89e662bf9a77787d4fdc6ea70f6ede83b5b6ebe40478045fa628fd3bbc5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b9780a4-31e2-48cd-afba-128f1e61d040", "node_type": "1", "metadata": {}, "hash": "78328dbaa7911323ca4c748a07c19c3f25f99ffbc29760f874e73abb968a678c", "class_name": "RelatedNodeInfo"}}, "text": "Input-tuning (An et al., 2022) introduces an adapter to tune input embedding. Although the\ninput embedding layers are also trainable in ours, this is not enough for long context extension. We\nmake a comprehensive analysis on layer types in experiments, in Table 2. Existing work (Chen et al.,\n2022) shows sparse masks can effectively save training costs and avoid performance drops.\n3 L ONG LORA\n3.1 B ACKGROUND\nTransformer. LLMs are typically built with transformers. Taking Llama2 (Touvron et al., 2023b)\nfor example, as shown in Figure 2, an LLM model consists of an embedding input layer and a number\nof decoder layers. Each decoder layer comprises a self-attention module. It maps input features\ninto a set of queries, keys, and values {q, k, v}, via linear projection layers with weight matrices\n{Wq, Wk, Wv}. Given {q, k, v}, it computes the outputs oas\no= softmax( qkT)v (1)\nThe outputs are then projected by a linear layer with a weight matrix Wo. And MLP layers are\nfollowed.", "start_char_idx": 1810, "end_char_idx": 2795, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b9780a4-31e2-48cd-afba-128f1e61d040": {"__data__": {"id_": "1b9780a4-31e2-48cd-afba-128f1e61d040", "embedding": null, "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a805057fee31841cba092e63eb61b704af42513c8e2bf73b6bc44fc6d5e15712", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9f15394-3e6a-4dfc-a217-ae33b09c0db9", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "dba50f957ad9f740d24666271b22bc58c790278d1c4bf300a42361a640575457", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3705a049-24a2-4419-8593-395783c26699", "node_type": "1", "metadata": {}, "hash": "5c3a1358355c66ecb4a7d90a3cf8f440efcd523721ef6a698858d495deaff6cb", "class_name": "RelatedNodeInfo"}}, "text": "Existing work (Chen et al.,\n2022) shows sparse masks can effectively save training costs and avoid performance drops.\n3 L ONG LORA\n3.1 B ACKGROUND\nTransformer. LLMs are typically built with transformers. Taking Llama2 (Touvron et al., 2023b)\nfor example, as shown in Figure 2, an LLM model consists of an embedding input layer and a number\nof decoder layers. Each decoder layer comprises a self-attention module. It maps input features\ninto a set of queries, keys, and values {q, k, v}, via linear projection layers with weight matrices\n{Wq, Wk, Wv}. Given {q, k, v}, it computes the outputs oas\no= softmax( qkT)v (1)\nThe outputs are then projected by a linear layer with a weight matrix Wo. And MLP layers are\nfollowed. Before and after self-attention modules, layer normalization (Ba et al., 2016) is applied. A\nfinal normalization is conducted after all decoder layers.\nFor long sequences, self-attention struggles with computation cost, which is quadratic to the sequence\nlength.", "start_char_idx": 2075, "end_char_idx": 3058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3705a049-24a2-4419-8593-395783c26699": {"__data__": {"id_": "3705a049-24a2-4419-8593-395783c26699", "embedding": null, "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a805057fee31841cba092e63eb61b704af42513c8e2bf73b6bc44fc6d5e15712", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b9780a4-31e2-48cd-afba-128f1e61d040", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c66cc4f3261952ed962cd4187f094a38ced5acd6d2e9b818aea19cdc1362ba31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ff668f4-b456-40cf-9343-12ff2ae87b15", "node_type": "1", "metadata": {}, "hash": "2c1340b436fff082c3e8cae3e5437e38a07a7a0922afbbef5e43659780ad8569", "class_name": "RelatedNodeInfo"}}, "text": "Each decoder layer comprises a self-attention module. It maps input features\ninto a set of queries, keys, and values {q, k, v}, via linear projection layers with weight matrices\n{Wq, Wk, Wv}. Given {q, k, v}, it computes the outputs oas\no= softmax( qkT)v (1)\nThe outputs are then projected by a linear layer with a weight matrix Wo. And MLP layers are\nfollowed. Before and after self-attention modules, layer normalization (Ba et al., 2016) is applied. A\nfinal normalization is conducted after all decoder layers.\nFor long sequences, self-attention struggles with computation cost, which is quadratic to the sequence\nlength. This dramatically slows down the training procedure and increases GPU memory costs.\nLow-rank Adaptation. LoRA (Hu et al., 2022) hypothesizes that the weight updates in pre-trained\nmodels have a low intrinsic rank during adaptation. For a pre-trained weight matrix W\u2208Rd\u00d7k, it is\nupdated with a low-rank decomposition W+ \u2206W=W+BA, where B\u2208Rd\u00d7randA\u2208Rr\u00d7k.", "start_char_idx": 2434, "end_char_idx": 3409, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ff668f4-b456-40cf-9343-12ff2ae87b15": {"__data__": {"id_": "5ff668f4-b456-40cf-9343-12ff2ae87b15", "embedding": null, "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a805057fee31841cba092e63eb61b704af42513c8e2bf73b6bc44fc6d5e15712", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3705a049-24a2-4419-8593-395783c26699", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "f72c783455dd8f55fcafd1ce4b8e7e51637b615ee9bb09bea686b2fc29db08e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5470b4e-a719-4c21-ada0-b6e39c62cac9", "node_type": "1", "metadata": {}, "hash": "82a54cfefaeda881f6a7c2a1cdce8595b9794eb3914ee3d09532c5226ce1f000", "class_name": "RelatedNodeInfo"}}, "text": "And MLP layers are\nfollowed. Before and after self-attention modules, layer normalization (Ba et al., 2016) is applied. A\nfinal normalization is conducted after all decoder layers.\nFor long sequences, self-attention struggles with computation cost, which is quadratic to the sequence\nlength. This dramatically slows down the training procedure and increases GPU memory costs.\nLow-rank Adaptation. LoRA (Hu et al., 2022) hypothesizes that the weight updates in pre-trained\nmodels have a low intrinsic rank during adaptation. For a pre-trained weight matrix W\u2208Rd\u00d7k, it is\nupdated with a low-rank decomposition W+ \u2206W=W+BA, where B\u2208Rd\u00d7randA\u2208Rr\u00d7k.\nThe rank r\u226amin(d, k). During training, Wis frozen with no gradient updates, while A and B are\ntrainable. This is the reason why LoRA training is much more efficient than full fine-tuning.\nIn the Transformer structure, LoRA only adapts the attention weights ( Wq, Wk, Wv, Wo) and freezes\nall other layers, including MLP and normalization layers.", "start_char_idx": 2767, "end_char_idx": 3754, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5470b4e-a719-4c21-ada0-b6e39c62cac9": {"__data__": {"id_": "e5470b4e-a719-4c21-ada0-b6e39c62cac9", "embedding": null, "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36c1daaa-43c5-42e8-b0ba-bbe215b2e595", "node_type": "4", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a805057fee31841cba092e63eb61b704af42513c8e2bf73b6bc44fc6d5e15712", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ff668f4-b456-40cf-9343-12ff2ae87b15", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "2e23c7a5163621b4cc34c31ddec160dbcb2f51f669be5983aa857e7a02d89944", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0df911a4-79c9-4fb7-8323-3cc195a4aaf0", "node_type": "1", "metadata": {}, "hash": "f713175b5f62d185f07b0f1b1b4c2d71d5f812220f391bc47a7c3d14596d89b5", "class_name": "RelatedNodeInfo"}}, "text": "This dramatically slows down the training procedure and increases GPU memory costs.\nLow-rank Adaptation. LoRA (Hu et al., 2022) hypothesizes that the weight updates in pre-trained\nmodels have a low intrinsic rank during adaptation. For a pre-trained weight matrix W\u2208Rd\u00d7k, it is\nupdated with a low-rank decomposition W+ \u2206W=W+BA, where B\u2208Rd\u00d7randA\u2208Rr\u00d7k.\nThe rank r\u226amin(d, k). During training, Wis frozen with no gradient updates, while A and B are\ntrainable. This is the reason why LoRA training is much more efficient than full fine-tuning.\nIn the Transformer structure, LoRA only adapts the attention weights ( Wq, Wk, Wv, Wo) and freezes\nall other layers, including MLP and normalization layers. This manner is simple and parameter-\nefficient. However, we empirically show that only low-rank adaptation in attention weights does not\nwork for long context extension.\n4", "start_char_idx": 3059, "end_char_idx": 3926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0df911a4-79c9-4fb7-8323-3cc195a4aaf0": {"__data__": {"id_": "0df911a4-79c9-4fb7-8323-3cc195a4aaf0", "embedding": null, "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "66c31170505404d19d1ae297f62cb6988b8b2d2255225bed758c5888c684cdac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5470b4e-a719-4c21-ada0-b6e39c62cac9", "node_type": "1", "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "518b7d24f7af801c6801d64e7892b4d934b370bd339f77cca4fad9cd23d9b2b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c00dc7c3-d38f-4011-8a3a-810e1409300a", "node_type": "1", "metadata": {}, "hash": "875b6d4e3222094420af92f2855abd038b09eda214055f5fda230be7dfb881e2", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nAlgorithm 1: Pseudocode of S2-Attn in PyTorch-like style.", "start_char_idx": 0, "end_char_idx": 102, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c00dc7c3-d38f-4011-8a3a-810e1409300a": {"__data__": {"id_": "c00dc7c3-d38f-4011-8a3a-810e1409300a", "embedding": null, "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "66c31170505404d19d1ae297f62cb6988b8b2d2255225bed758c5888c684cdac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0df911a4-79c9-4fb7-8323-3cc195a4aaf0", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "e1757b735606f37d9069650760ece8927a800e3ccea0ee8ffcf8745bbb7cd2d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "236923d0-f07d-4f86-af8b-c3e106eda8de", "node_type": "1", "metadata": {}, "hash": "fe196402d8966048df7c6728e1e87227e648c58f16bc51858da484ec873d5254", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nAlgorithm 1: Pseudocode of S2-Attn in PyTorch-like style.\n# B: batch size; S: sequence length or number of tokens; G: group size;\n# H: number of attention heads; D: dimension of each attention head\n# qkv in shape (B, N, 3, H, D), projected queries, keys, and values\n# Key line 1: split qkv on H into 2 chunks, and shift G/2 on N\nqkv = cat((qkv.chunk(2, 3)[0], qkv.chunk(2, 3)[1].roll(-G/2, 1)), 3).view(B *N/G,G,3,H,D)\n# standard self-attention function\nout = self_attn(qkv)\n# out in shape (B, N, H, D)\n# Key line 2: split out on H into 2 chunks, and then roll back G/2 on N\nout = cat((out.chunk(2, 2)[0], out.chunk(2, 2)[1].roll(G/2, 1)), 2)\ncat: concatenation; chunk : split into the specified number of chunks; roll : roll the tensor along the given dimension.", "start_char_idx": 0, "end_char_idx": 808, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "236923d0-f07d-4f86-af8b-c3e106eda8de": {"__data__": {"id_": "236923d0-f07d-4f86-af8b-c3e106eda8de", "embedding": null, "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "66c31170505404d19d1ae297f62cb6988b8b2d2255225bed758c5888c684cdac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c00dc7c3-d38f-4011-8a3a-810e1409300a", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "b0fd2f0d0a9bc949eccaded954a7bb32a22f388185d34d2266321250e8affde0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab098901-752f-4dc0-ac9d-8027175549ac", "node_type": "1", "metadata": {}, "hash": "ca0e92ee56605f860451666470ec49b474b46e3d01efdab570937c1e0654e884", "class_name": "RelatedNodeInfo"}}, "text": "3.2 S HIFTED SPARSE ATTENTION\nStandard self-attention costs O(n2)computations, making LLMs on long sequences high memory\ncost and slow. To avoid this issue during training, we propose Shifted Sparse Attention (S2-Attn), as\nshown in Figure 2. In the following, we make a pilot study and explain our design step by step.\nPilot Study. In Table 1, we build up a standard baseline that is trained and tested with full attention\nand fine-tuning, which presents consistently good quality in various context lengths. The first trial\nis to train with short attention, only pattern 1 in Figure 2. As we know for a long context, the high\ncost mainly comes from self-attention modules. Thus, in this trial, since the input is long, we split\ninto several groups in self-attention. For example, the model takes 8192 tokens as input in both the\ntraining and testing stages, but self-attention is conducted in each group with a 2048 size. The group\nnumber is 4, as ablated in Section B.2 in the appendix.", "start_char_idx": 809, "end_char_idx": 1797, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab098901-752f-4dc0-ac9d-8027175549ac": {"__data__": {"id_": "ab098901-752f-4dc0-ac9d-8027175549ac", "embedding": null, "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "66c31170505404d19d1ae297f62cb6988b8b2d2255225bed758c5888c684cdac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "236923d0-f07d-4f86-af8b-c3e106eda8de", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "8e0183b98c22fbb8ef4ca49484c6ee639e20a4a185a40d724e3d86d03e740ff2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d21c6dc7-34ff-4315-9adf-36d7707170fd", "node_type": "1", "metadata": {}, "hash": "104cffa4760245fac30985c1e5ca9f6642bd4b2e9895a7ad83dcd98b13c75343", "class_name": "RelatedNodeInfo"}}, "text": "In the following, we make a pilot study and explain our design step by step.\nPilot Study. In Table 1, we build up a standard baseline that is trained and tested with full attention\nand fine-tuning, which presents consistently good quality in various context lengths. The first trial\nis to train with short attention, only pattern 1 in Figure 2. As we know for a long context, the high\ncost mainly comes from self-attention modules. Thus, in this trial, since the input is long, we split\ninto several groups in self-attention. For example, the model takes 8192 tokens as input in both the\ntraining and testing stages, but self-attention is conducted in each group with a 2048 size. The group\nnumber is 4, as ablated in Section B.2 in the appendix. This pattern is efficient but still does not work\nin a very long context, as shown in Table 1. The perplexity becomes larger as the context length\nincreases. The reason behind this is that there is no information exchange between different groups.\nTo introduce communication between groups, we include a shifted pattern, as shown in Figure 2.", "start_char_idx": 1051, "end_char_idx": 2140, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d21c6dc7-34ff-4315-9adf-36d7707170fd": {"__data__": {"id_": "d21c6dc7-34ff-4315-9adf-36d7707170fd", "embedding": null, "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "66c31170505404d19d1ae297f62cb6988b8b2d2255225bed758c5888c684cdac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab098901-752f-4dc0-ac9d-8027175549ac", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "1718ddaef682b74957d988b13d75ca9e7318efc7eb76d2607fdc18e4ee385d5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b44ef511-8e2d-43b0-8bd2-f2a3029f4eba", "node_type": "1", "metadata": {}, "hash": "948e874d353e9719ce241ad31119d2e07e1096719e55e8e8176574b2a3de09e7", "class_name": "RelatedNodeInfo"}}, "text": "The first trial\nis to train with short attention, only pattern 1 in Figure 2. As we know for a long context, the high\ncost mainly comes from self-attention modules. Thus, in this trial, since the input is long, we split\ninto several groups in self-attention. For example, the model takes 8192 tokens as input in both the\ntraining and testing stages, but self-attention is conducted in each group with a 2048 size. The group\nnumber is 4, as ablated in Section B.2 in the appendix. This pattern is efficient but still does not work\nin a very long context, as shown in Table 1. The perplexity becomes larger as the context length\nincreases. The reason behind this is that there is no information exchange between different groups.\nTo introduce communication between groups, we include a shifted pattern, as shown in Figure 2. We\nshift the group partition by half group size in half attention heads. Taking the overall 8192 context\nlength for example, in pattern 1, the first group conducts self-attention from 1stto 2048thtokens.", "start_char_idx": 1318, "end_char_idx": 2344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b44ef511-8e2d-43b0-8bd2-f2a3029f4eba": {"__data__": {"id_": "b44ef511-8e2d-43b0-8bd2-f2a3029f4eba", "embedding": null, "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "66c31170505404d19d1ae297f62cb6988b8b2d2255225bed758c5888c684cdac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d21c6dc7-34ff-4315-9adf-36d7707170fd", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6cf029ca7d981dd30fd7cf2c74786bbfdd8a7855faa2460c39f29399c754d0a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea51b84f-10cd-4948-b5ea-e5bd7f8f7608", "node_type": "1", "metadata": {}, "hash": "a389ab7b49428a2ef0dc429ad08173813d0be94939af5e3d61046fd35d45be6c", "class_name": "RelatedNodeInfo"}}, "text": "For example, the model takes 8192 tokens as input in both the\ntraining and testing stages, but self-attention is conducted in each group with a 2048 size. The group\nnumber is 4, as ablated in Section B.2 in the appendix. This pattern is efficient but still does not work\nin a very long context, as shown in Table 1. The perplexity becomes larger as the context length\nincreases. The reason behind this is that there is no information exchange between different groups.\nTo introduce communication between groups, we include a shifted pattern, as shown in Figure 2. We\nshift the group partition by half group size in half attention heads. Taking the overall 8192 context\nlength for example, in pattern 1, the first group conducts self-attention from 1stto 2048thtokens.\nIn Pattern 2, the group partition is shifted by 1024. The first attention group begins from 1025th\nand ends at 3072thtokens, while the first and the last 1024 tokens belong to the same group. We\nuse patterns 1 and 2 in each half self-attention heads respectively.", "start_char_idx": 1577, "end_char_idx": 2608, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea51b84f-10cd-4948-b5ea-e5bd7f8f7608": {"__data__": {"id_": "ea51b84f-10cd-4948-b5ea-e5bd7f8f7608", "embedding": null, "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "66c31170505404d19d1ae297f62cb6988b8b2d2255225bed758c5888c684cdac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b44ef511-8e2d-43b0-8bd2-f2a3029f4eba", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "23a7f57c35f98cb34d519153d44469411cc00e7ee4cf9eda49fd45446573cd55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25a0fcef-3040-44c3-86fa-b0bbfdbbcf34", "node_type": "1", "metadata": {}, "hash": "2f6cc2bf0ce0f4904dc1e663dc443877e3e49bbe501dbfdd61cd1afd4f9a3552", "class_name": "RelatedNodeInfo"}}, "text": "This pattern is efficient but still does not work\nin a very long context, as shown in Table 1. The perplexity becomes larger as the context length\nincreases. The reason behind this is that there is no information exchange between different groups.\nTo introduce communication between groups, we include a shifted pattern, as shown in Figure 2. We\nshift the group partition by half group size in half attention heads. Taking the overall 8192 context\nlength for example, in pattern 1, the first group conducts self-attention from 1stto 2048thtokens.\nIn Pattern 2, the group partition is shifted by 1024. The first attention group begins from 1025th\nand ends at 3072thtokens, while the first and the last 1024 tokens belong to the same group. We\nuse patterns 1 and 2 in each half self-attention heads respectively. This manner does not increase\nadditional computation costs but enables the information flow between different groups. We show\nthat it gets close to the standard attention baseline in Table 1.\nConsistency to Full Attention. Existing efficient attention designs can also improve the efficiency\nof long-context LLMs.", "start_char_idx": 1798, "end_char_idx": 2922, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25a0fcef-3040-44c3-86fa-b0bbfdbbcf34": {"__data__": {"id_": "25a0fcef-3040-44c3-86fa-b0bbfdbbcf34", "embedding": null, "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "66c31170505404d19d1ae297f62cb6988b8b2d2255225bed758c5888c684cdac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea51b84f-10cd-4948-b5ea-e5bd7f8f7608", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c1e787c457157f78e536e92d901f596d4eaaf112d59cddf6dfeb97dcd8f6e2b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a211fd5c-8abc-45da-8e14-505855b4539f", "node_type": "1", "metadata": {}, "hash": "fae0eae760c36649d1b475139f896b15d8e5b21cb555c6a2108eb41f66e20a6d", "class_name": "RelatedNodeInfo"}}, "text": "We\nshift the group partition by half group size in half attention heads. Taking the overall 8192 context\nlength for example, in pattern 1, the first group conducts self-attention from 1stto 2048thtokens.\nIn Pattern 2, the group partition is shifted by 1024. The first attention group begins from 1025th\nand ends at 3072thtokens, while the first and the last 1024 tokens belong to the same group. We\nuse patterns 1 and 2 in each half self-attention heads respectively. This manner does not increase\nadditional computation costs but enables the information flow between different groups. We show\nthat it gets close to the standard attention baseline in Table 1.\nConsistency to Full Attention. Existing efficient attention designs can also improve the efficiency\nof long-context LLMs. However, most of them are not suitable for long-context fine-tuning. Because,\nthese transformers (Qiu et al., 2020; Child et al., 2019), designed for training from scratch, have gaps\nto the standard full attention, which is used in pre-training.", "start_char_idx": 2141, "end_char_idx": 3168, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a211fd5c-8abc-45da-8e14-505855b4539f": {"__data__": {"id_": "a211fd5c-8abc-45da-8e14-505855b4539f", "embedding": null, "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "66c31170505404d19d1ae297f62cb6988b8b2d2255225bed758c5888c684cdac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25a0fcef-3040-44c3-86fa-b0bbfdbbcf34", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "90f7955a90e677123f4a641b2d2276113bbbc2193a3d6f20288d8107ea0ad55e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "957eeeb9-ab77-446d-964a-d5839b53adf7", "node_type": "1", "metadata": {}, "hash": "8cd17e7bbc287a880007cd0c192b7030f993cb03ebfb6caa478d075e80415aea", "class_name": "RelatedNodeInfo"}}, "text": "In Pattern 2, the group partition is shifted by 1024. The first attention group begins from 1025th\nand ends at 3072thtokens, while the first and the last 1024 tokens belong to the same group. We\nuse patterns 1 and 2 in each half self-attention heads respectively. This manner does not increase\nadditional computation costs but enables the information flow between different groups. We show\nthat it gets close to the standard attention baseline in Table 1.\nConsistency to Full Attention. Existing efficient attention designs can also improve the efficiency\nof long-context LLMs. However, most of them are not suitable for long-context fine-tuning. Because,\nthese transformers (Qiu et al., 2020; Child et al., 2019), designed for training from scratch, have gaps\nto the standard full attention, which is used in pre-training. In Table 6, we show that S2-Attn not\nonly enables efficient fine-tuning but also supports fullattention testing . Although other attentions\ncan also be used in long context fine-tuning, models must be tested with the attention used during\nfine-tuning.", "start_char_idx": 2345, "end_char_idx": 3420, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "957eeeb9-ab77-446d-964a-d5839b53adf7": {"__data__": {"id_": "957eeeb9-ab77-446d-964a-d5839b53adf7", "embedding": null, "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "66c31170505404d19d1ae297f62cb6988b8b2d2255225bed758c5888c684cdac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a211fd5c-8abc-45da-8e14-505855b4539f", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "fad493d2bb9b107eabf21af8e91198c013b45294e974ae72463285ff110a5cab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3081212b-b031-4faf-90cf-3ac781ab69f6", "node_type": "1", "metadata": {}, "hash": "cc9df262299eaf1496f615ade2a63a5cc39b8145970a714b1856d3f41f0c81a3", "class_name": "RelatedNodeInfo"}}, "text": "This manner does not increase\nadditional computation costs but enables the information flow between different groups. We show\nthat it gets close to the standard attention baseline in Table 1.\nConsistency to Full Attention. Existing efficient attention designs can also improve the efficiency\nof long-context LLMs. However, most of them are not suitable for long-context fine-tuning. Because,\nthese transformers (Qiu et al., 2020; Child et al., 2019), designed for training from scratch, have gaps\nto the standard full attention, which is used in pre-training. In Table 6, we show that S2-Attn not\nonly enables efficient fine-tuning but also supports fullattention testing . Although other attentions\ncan also be used in long context fine-tuning, models must be tested with the attention used during\nfine-tuning. Shifting prevents models from being over-fitted to specific attention patterns.\nEasy Implementation. S2-Attn is easy to implement. It involves only two steps: (1) shifting tokens\nin half attention heads, and (2) transposing features from token dimension to batch dimension. Two\nlines of code are enough.", "start_char_idx": 2609, "end_char_idx": 3724, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3081212b-b031-4faf-90cf-3ac781ab69f6": {"__data__": {"id_": "3081212b-b031-4faf-90cf-3ac781ab69f6", "embedding": null, "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "66c31170505404d19d1ae297f62cb6988b8b2d2255225bed758c5888c684cdac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "957eeeb9-ab77-446d-964a-d5839b53adf7", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "aca12f4a04cccb78fa38cdff6232bc63a2693bcd74d64d0141cc221f83d1108a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2768a8d-1fe1-4c3f-86c8-333f21e2080d", "node_type": "1", "metadata": {}, "hash": "f0c265c91b441c447507cba6bbab991012275237b1f017a6694ba1c437ff903a", "class_name": "RelatedNodeInfo"}}, "text": "However, most of them are not suitable for long-context fine-tuning. Because,\nthese transformers (Qiu et al., 2020; Child et al., 2019), designed for training from scratch, have gaps\nto the standard full attention, which is used in pre-training. In Table 6, we show that S2-Attn not\nonly enables efficient fine-tuning but also supports fullattention testing . Although other attentions\ncan also be used in long context fine-tuning, models must be tested with the attention used during\nfine-tuning. Shifting prevents models from being over-fitted to specific attention patterns.\nEasy Implementation. S2-Attn is easy to implement. It involves only two steps: (1) shifting tokens\nin half attention heads, and (2) transposing features from token dimension to batch dimension. Two\nlines of code are enough. We provide a PyTorch-style code in Algorithm 1.\n3.3 I MPROVED LORA FOR LONG CONTEXT\nLoRA (Hu et al., 2022) is an efficient and popular manner for adapting LLMs to other datasets.", "start_char_idx": 2923, "end_char_idx": 3903, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2768a8d-1fe1-4c3f-86c8-333f21e2080d": {"__data__": {"id_": "d2768a8d-1fe1-4c3f-86c8-333f21e2080d", "embedding": null, "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "66c31170505404d19d1ae297f62cb6988b8b2d2255225bed758c5888c684cdac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3081212b-b031-4faf-90cf-3ac781ab69f6", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "aa5a07a45c4bf025e5fa203c13b2b240d919db239db78165c3d54c664ab15bd5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5dd64e0e-f03e-4080-b0b6-087d1d58f192", "node_type": "1", "metadata": {}, "hash": "5842d85cbf95a32b6d7e2931e571523a6a77188a09be7caaadaf245c5902b281", "class_name": "RelatedNodeInfo"}}, "text": "In Table 6, we show that S2-Attn not\nonly enables efficient fine-tuning but also supports fullattention testing . Although other attentions\ncan also be used in long context fine-tuning, models must be tested with the attention used during\nfine-tuning. Shifting prevents models from being over-fitted to specific attention patterns.\nEasy Implementation. S2-Attn is easy to implement. It involves only two steps: (1) shifting tokens\nin half attention heads, and (2) transposing features from token dimension to batch dimension. Two\nlines of code are enough. We provide a PyTorch-style code in Algorithm 1.\n3.3 I MPROVED LORA FOR LONG CONTEXT\nLoRA (Hu et al., 2022) is an efficient and popular manner for adapting LLMs to other datasets. It\nsaves much trainable parameters and memory cost, compared to full fine-tuning. However, adapting\nLLMs from short context length to long is not easy. We empirically observe an obvious gap between\nLoRA and full fine-tuning.", "start_char_idx": 3169, "end_char_idx": 4128, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5dd64e0e-f03e-4080-b0b6-087d1d58f192": {"__data__": {"id_": "5dd64e0e-f03e-4080-b0b6-087d1d58f192", "embedding": null, "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da08e13e-b1f1-495a-aa53-69625e6f7ef2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "66c31170505404d19d1ae297f62cb6988b8b2d2255225bed758c5888c684cdac", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2768a8d-1fe1-4c3f-86c8-333f21e2080d", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "aa28262bf01fa86d16e461f24ba8781a0c8aaecc31e1dadaa6bd81f2d75ecf8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d782522-bd9f-4b8e-b157-3ffc10792f55", "node_type": "1", "metadata": {}, "hash": "3f9da1a8e9a328cf57b7985688397a311bee2f1fef31515f27869581dda79047", "class_name": "RelatedNodeInfo"}}, "text": "Shifting prevents models from being over-fitted to specific attention patterns.\nEasy Implementation. S2-Attn is easy to implement. It involves only two steps: (1) shifting tokens\nin half attention heads, and (2) transposing features from token dimension to batch dimension. Two\nlines of code are enough. We provide a PyTorch-style code in Algorithm 1.\n3.3 I MPROVED LORA FOR LONG CONTEXT\nLoRA (Hu et al., 2022) is an efficient and popular manner for adapting LLMs to other datasets. It\nsaves much trainable parameters and memory cost, compared to full fine-tuning. However, adapting\nLLMs from short context length to long is not easy. We empirically observe an obvious gap between\nLoRA and full fine-tuning. As shown in Table 2, the gap between LoRA and full fine-tuning grows\nas the target context length becomes larger. And LoRA with larger ranks cannot reduce the gap.\n5", "start_char_idx": 3421, "end_char_idx": 4294, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d782522-bd9f-4b8e-b157-3ffc10792f55": {"__data__": {"id_": "5d782522-bd9f-4b8e-b157-3ffc10792f55", "embedding": null, "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "58014fba4b5e7e50ea702c9c1297186a1940f527c691085df70e28ef425d64d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5dd64e0e-f03e-4080-b0b6-087d1d58f192", "node_type": "1", "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c85cdba00eaf9957a96032642afb90cb026124c9171a23b3d936f94a4d4581e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c76348b-a7fd-49a6-a3af-aee53fb94002", "node_type": "1", "metadata": {}, "hash": "616bbf2204161d293ae646ca211cd3cb3c857be5e17166e5be136158db263456", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 2: Finetuning normalization and embedding layers is crucial for low-rank long-context\nadaptation . Llama2 7B (Touvron et al., 2023b) models with the proposed S2-Attn are trained on the\nRedPajama (Computer, 2023) dataset. The target context length is 32768. \u2018+ Normal / Embed\u2019 means\nnormalization or embedding layers are trainable. Perplexity results are evaluated on PG19 (Rae et al.,\n2020) validation set. For long context adaptation, there is a large performance gap between standard\nLoRA (Hu et al., 2022) and full fine-tuning. Without trainable normalization or embeddings, larger\nranks in LoRA can not close this gap.", "start_char_idx": 0, "end_char_idx": 673, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c76348b-a7fd-49a6-a3af-aee53fb94002": {"__data__": {"id_": "5c76348b-a7fd-49a6-a3af-aee53fb94002", "embedding": null, "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "58014fba4b5e7e50ea702c9c1297186a1940f527c691085df70e28ef425d64d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d782522-bd9f-4b8e-b157-3ffc10792f55", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "3ed484b4fbc586ecbf3c14c122b07fabfa5dfed998d480d32f2fac592612ee4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d519cfc-bf1c-4e06-a673-6dc744e8035b", "node_type": "1", "metadata": {}, "hash": "bf298900aa1e33a910e7e8b0d8bbddcd855c22640db9ec76863f6d1613eeb4f3", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 2: Finetuning normalization and embedding layers is crucial for low-rank long-context\nadaptation . Llama2 7B (Touvron et al., 2023b) models with the proposed S2-Attn are trained on the\nRedPajama (Computer, 2023) dataset. The target context length is 32768. \u2018+ Normal / Embed\u2019 means\nnormalization or embedding layers are trainable. Perplexity results are evaluated on PG19 (Rae et al.,\n2020) validation set. For long context adaptation, there is a large performance gap between standard\nLoRA (Hu et al., 2022) and full fine-tuning. Without trainable normalization or embeddings, larger\nranks in LoRA can not close this gap.\nMethod Full FTLoRA (rank) LoRA (rank = 8)\n8 16 32 64 128 256 + Norm + Embed + Norm & Embed\nPPL 8.08 11.44 11.82 11.92 11.96 11.97 11.98 10.49 8.29 8.12\nTable 3: Perplexity evaluation on proof-pile (Rae et al., 2020) test split.", "start_char_idx": 0, "end_char_idx": 901, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d519cfc-bf1c-4e06-a673-6dc744e8035b": {"__data__": {"id_": "5d519cfc-bf1c-4e06-a673-6dc744e8035b", "embedding": null, "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "58014fba4b5e7e50ea702c9c1297186a1940f527c691085df70e28ef425d64d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c76348b-a7fd-49a6-a3af-aee53fb94002", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "769e05d53a744d48b16de435f754e4328dc9646da9f0fdf605cc4ceed78aa92f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b9a34ee-890d-4680-96c6-08291072e510", "node_type": "1", "metadata": {}, "hash": "55bc0d9a166201fc5af0ea35762aca1ab82c71124177eea66863b6089a4482a9", "class_name": "RelatedNodeInfo"}}, "text": "\u2018+ Normal / Embed\u2019 means\nnormalization or embedding layers are trainable. Perplexity results are evaluated on PG19 (Rae et al.,\n2020) validation set. For long context adaptation, there is a large performance gap between standard\nLoRA (Hu et al., 2022) and full fine-tuning. Without trainable normalization or embeddings, larger\nranks in LoRA can not close this gap.\nMethod Full FTLoRA (rank) LoRA (rank = 8)\n8 16 32 64 128 256 + Norm + Embed + Norm & Embed\nPPL 8.08 11.44 11.82 11.92 11.96 11.97 11.98 10.49 8.29 8.12\nTable 3: Perplexity evaluation on proof-pile (Rae et al., 2020) test split. S2-Attn: Shifted Sparse\nAttention. LoRA+: improved LoRA.", "start_char_idx": 308, "end_char_idx": 958, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b9a34ee-890d-4680-96c6-08291072e510": {"__data__": {"id_": "9b9a34ee-890d-4680-96c6-08291072e510", "embedding": null, "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "58014fba4b5e7e50ea702c9c1297186a1940f527c691085df70e28ef425d64d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d519cfc-bf1c-4e06-a673-6dc744e8035b", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "8d04ae33765b7c433c2875f26882493086b5aac50c77d2be0fb4828a51ad2c16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0913454-32c5-4aaa-9f79-7b51c472ab48", "node_type": "1", "metadata": {}, "hash": "cf779febeab1d8d0537af8929ba739a66dddacc5584e3da86c7dd9bd5c384393", "class_name": "RelatedNodeInfo"}}, "text": "Perplexity results are evaluated on PG19 (Rae et al.,\n2020) validation set. For long context adaptation, there is a large performance gap between standard\nLoRA (Hu et al., 2022) and full fine-tuning. Without trainable normalization or embeddings, larger\nranks in LoRA can not close this gap.\nMethod Full FTLoRA (rank) LoRA (rank = 8)\n8 16 32 64 128 256 + Norm + Embed + Norm & Embed\nPPL 8.08 11.44 11.82 11.92 11.96 11.97 11.98 10.49 8.29 8.12\nTable 3: Perplexity evaluation on proof-pile (Rae et al., 2020) test split. S2-Attn: Shifted Sparse\nAttention. LoRA+: improved LoRA. We fine-tune Llama2 (Touvron et al., 2023b) in 7B and 13B\nmodel sizes on the RedPajama (Computer, 2023) dataset under 8k-32k context lengths.", "start_char_idx": 382, "end_char_idx": 1100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0913454-32c5-4aaa-9f79-7b51c472ab48": {"__data__": {"id_": "a0913454-32c5-4aaa-9f79-7b51c472ab48", "embedding": null, "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "58014fba4b5e7e50ea702c9c1297186a1940f527c691085df70e28ef425d64d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b9a34ee-890d-4680-96c6-08291072e510", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "b9576dd3bcfbefe746d733bf707adecf23766c69887471d1eb50ff86212a7adf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "986bf8e8-4913-45bc-bdf3-c06dd56449c2", "node_type": "1", "metadata": {}, "hash": "9c4381b5b7987f00819ff5dd660b27b3efeed67bd8ae7cb42b9ce612cf8a1f42", "class_name": "RelatedNodeInfo"}}, "text": "Without trainable normalization or embeddings, larger\nranks in LoRA can not close this gap.\nMethod Full FTLoRA (rank) LoRA (rank = 8)\n8 16 32 64 128 256 + Norm + Embed + Norm & Embed\nPPL 8.08 11.44 11.82 11.92 11.96 11.97 11.98 10.49 8.29 8.12\nTable 3: Perplexity evaluation on proof-pile (Rae et al., 2020) test split. S2-Attn: Shifted Sparse\nAttention. LoRA+: improved LoRA. We fine-tune Llama2 (Touvron et al., 2023b) in 7B and 13B\nmodel sizes on the RedPajama (Computer, 2023) dataset under 8k-32k context lengths. We show\nthat our method achieves comparable performance to the full attention or full FT baselines, with\nbetter efficiency. We use the same training setting as the model evaluated on PG19 (Rae et al., 2020)\nintroduced in Section B.1 in the appendix.", "start_char_idx": 582, "end_char_idx": 1350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "986bf8e8-4913-45bc-bdf3-c06dd56449c2": {"__data__": {"id_": "986bf8e8-4913-45bc-bdf3-c06dd56449c2", "embedding": null, "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "58014fba4b5e7e50ea702c9c1297186a1940f527c691085df70e28ef425d64d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0913454-32c5-4aaa-9f79-7b51c472ab48", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "7842935a7d39c2972045de8e323142b1997a42511cc4471b0307452d3c113f81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46060bcc-6f1d-44ce-a6f7-1918fc4f1fed", "node_type": "1", "metadata": {}, "hash": "2d7121117423f108ad2f05367b842c6bb59823f67fc3c31dfd8f9a6b0ea21a8d", "class_name": "RelatedNodeInfo"}}, "text": "S2-Attn: Shifted Sparse\nAttention. LoRA+: improved LoRA. We fine-tune Llama2 (Touvron et al., 2023b) in 7B and 13B\nmodel sizes on the RedPajama (Computer, 2023) dataset under 8k-32k context lengths. We show\nthat our method achieves comparable performance to the full attention or full FT baselines, with\nbetter efficiency. We use the same training setting as the model evaluated on PG19 (Rae et al., 2020)\nintroduced in Section B.1 in the appendix.\nSizeTraining\nContext LengthLongLoRA Evaluation Context Length\nS2-Attn LoRA+2048 4096 8192 16384 32768\n7B81923.14 2.85 2.66 - -\n\u2713 3.15 2.86 2.68 - -\n\u2713 \u2713 3.20 2.91 2.72 - -\n16384\u2713 3.17 2.87 2.68 2.55 -\n\u2713 \u2713 3.17 2.87 2.66 2.", "start_char_idx": 902, "end_char_idx": 1572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46060bcc-6f1d-44ce-a6f7-1918fc4f1fed": {"__data__": {"id_": "46060bcc-6f1d-44ce-a6f7-1918fc4f1fed", "embedding": null, "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "58014fba4b5e7e50ea702c9c1297186a1940f527c691085df70e28ef425d64d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "986bf8e8-4913-45bc-bdf3-c06dd56449c2", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "88f96961198c7d5e95cd8571960716b59d8f84a4fc37d62730cd19aa45721cb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48c35703-9920-42c5-b53a-7a097340cac9", "node_type": "1", "metadata": {}, "hash": "52517e2270657266858c50396141ca50d3586bf4f326f15d4e07456b361497b5", "class_name": "RelatedNodeInfo"}}, "text": "We show\nthat our method achieves comparable performance to the full attention or full FT baselines, with\nbetter efficiency. We use the same training setting as the model evaluated on PG19 (Rae et al., 2020)\nintroduced in Section B.1 in the appendix.\nSizeTraining\nContext LengthLongLoRA Evaluation Context Length\nS2-Attn LoRA+2048 4096 8192 16384 32768\n7B81923.14 2.85 2.66 - -\n\u2713 3.15 2.86 2.68 - -\n\u2713 \u2713 3.20 2.91 2.72 - -\n16384\u2713 3.17 2.87 2.68 2.55 -\n\u2713 \u2713 3.17 2.87 2.66 2.51 -\n32768\u2713 3.20 2.90 2.69 2.54 2.49\n\u2713 \u2713 3.35 3.01 2.78 2.61 2.50\n13B81922.96 2.69 2.53 - -\n\u2713 3.01 2.74 2.", "start_char_idx": 1101, "end_char_idx": 1678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48c35703-9920-42c5-b53a-7a097340cac9": {"__data__": {"id_": "48c35703-9920-42c5-b53a-7a097340cac9", "embedding": null, "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "58014fba4b5e7e50ea702c9c1297186a1940f527c691085df70e28ef425d64d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46060bcc-6f1d-44ce-a6f7-1918fc4f1fed", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11ebefeef83346d4c5ffbf743ef57c4918439bebde2ef207160a145131081752", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97709ca3-af29-48ca-9747-e35511cbd4de", "node_type": "1", "metadata": {}, "hash": "71fbaf06c59f77baefd6fb90e97b00b3ea5aa85f1842b712417f11991a21236a", "class_name": "RelatedNodeInfo"}}, "text": "14 2.85 2.66 - -\n\u2713 3.15 2.86 2.68 - -\n\u2713 \u2713 3.20 2.91 2.72 - -\n16384\u2713 3.17 2.87 2.68 2.55 -\n\u2713 \u2713 3.17 2.87 2.66 2.51 -\n32768\u2713 3.20 2.90 2.69 2.54 2.49\n\u2713 \u2713 3.35 3.01 2.78 2.61 2.50\n13B81922.96 2.69 2.53 - -\n\u2713 3.01 2.74 2.57 - -\n\u2713 \u2713 3.04 2.77 2.60 - -\n16384\u2713 2.99 2.72 2.53 2.40 -\n\u2713 \u2713 3.03 2.74 2.55 2.41 -\n32768\u2713 3.04 2.75 2.56 2.42 2.33\n\u2713 \u2713 3.05 2.", "start_char_idx": 1461, "end_char_idx": 1806, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97709ca3-af29-48ca-9747-e35511cbd4de": {"__data__": {"id_": "97709ca3-af29-48ca-9747-e35511cbd4de", "embedding": null, "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "58014fba4b5e7e50ea702c9c1297186a1940f527c691085df70e28ef425d64d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48c35703-9920-42c5-b53a-7a097340cac9", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "fbd7f2c4ce6d39fb349a7d3da96714bbdc86c24ea53e6ea42fd2f9493eae48bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "805402fb-4247-4a68-904f-542031e4c451", "node_type": "1", "metadata": {}, "hash": "d3e0615b00711ad96a00a629a72230e0761321163cf06de628c33d998af0b3d9", "class_name": "RelatedNodeInfo"}}, "text": "68 2.55 -\n\u2713 \u2713 3.17 2.87 2.66 2.51 -\n32768\u2713 3.20 2.90 2.69 2.54 2.49\n\u2713 \u2713 3.35 3.01 2.78 2.61 2.50\n13B81922.96 2.69 2.53 - -\n\u2713 3.01 2.74 2.57 - -\n\u2713 \u2713 3.04 2.77 2.60 - -\n16384\u2713 2.99 2.72 2.53 2.40 -\n\u2713 \u2713 3.03 2.74 2.55 2.41 -\n32768\u2713 3.04 2.75 2.56 2.42 2.33\n\u2713 \u2713 3.05 2.76 2.57 2.42 2.32\nTo bridge this gap, we open embedding and normalization layers for training. As shown in Table 2,\nthey occupy limited parameters but make effects for long context adaptation.", "start_char_idx": 1541, "end_char_idx": 1998, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "805402fb-4247-4a68-904f-542031e4c451": {"__data__": {"id_": "805402fb-4247-4a68-904f-542031e4c451", "embedding": null, "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "58014fba4b5e7e50ea702c9c1297186a1940f527c691085df70e28ef425d64d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97709ca3-af29-48ca-9747-e35511cbd4de", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "847f0a91cbc9dbba5234bba2498707cde646d4070759b767992f27f0062a7f31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4af015e1-7c98-4d3c-8c15-4f2b6b080213", "node_type": "1", "metadata": {}, "hash": "859625d4c5ea09de171cc5ee7557fdbdafef178754b883eb7b1ca0a016c9a0b2", "class_name": "RelatedNodeInfo"}}, "text": "49\n\u2713 \u2713 3.35 3.01 2.78 2.61 2.50\n13B81922.96 2.69 2.53 - -\n\u2713 3.01 2.74 2.57 - -\n\u2713 \u2713 3.04 2.77 2.60 - -\n16384\u2713 2.99 2.72 2.53 2.40 -\n\u2713 \u2713 3.03 2.74 2.55 2.41 -\n32768\u2713 3.04 2.75 2.56 2.42 2.33\n\u2713 \u2713 3.05 2.76 2.57 2.42 2.32\nTo bridge this gap, we open embedding and normalization layers for training. As shown in Table 2,\nthey occupy limited parameters but make effects for long context adaptation. Especially for normal-\nization layers, the parameters are only 0.004% in the whole Llama2 7B. We denote this improved\nversion of LoRA as LoRA+in experiments.", "start_char_idx": 1606, "end_char_idx": 2156, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4af015e1-7c98-4d3c-8c15-4f2b6b080213": {"__data__": {"id_": "4af015e1-7c98-4d3c-8c15-4f2b6b080213", "embedding": null, "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "58014fba4b5e7e50ea702c9c1297186a1940f527c691085df70e28ef425d64d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "805402fb-4247-4a68-904f-542031e4c451", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "d123867d506158dc0f28b23f634665eb4cceed87b6a0b2d12df71b5f2b56275b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ff29eca-6e3f-4a30-9db3-76e56ead421c", "node_type": "1", "metadata": {}, "hash": "e1203feaabbd2f3e672e2dae75d95a4dbe9f7c7fe8d4ba532b68529dec8b5b98", "class_name": "RelatedNodeInfo"}}, "text": "01 2.74 2.57 - -\n\u2713 \u2713 3.04 2.77 2.60 - -\n16384\u2713 2.99 2.72 2.53 2.40 -\n\u2713 \u2713 3.03 2.74 2.55 2.41 -\n32768\u2713 3.04 2.75 2.56 2.42 2.33\n\u2713 \u2713 3.05 2.76 2.57 2.42 2.32\nTo bridge this gap, we open embedding and normalization layers for training. As shown in Table 2,\nthey occupy limited parameters but make effects for long context adaptation. Especially for normal-\nization layers, the parameters are only 0.004% in the whole Llama2 7B. We denote this improved\nversion of LoRA as LoRA+in experiments.\n4 E XPERIMENT\n4.1 E XPERIMENTAL SETTINGS\nModels We extend the pre-trained 7B, 13B, and 70B Llama2 (Touvron et al., 2023b) models.", "start_char_idx": 1668, "end_char_idx": 2286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ff29eca-6e3f-4a30-9db3-76e56ead421c": {"__data__": {"id_": "6ff29eca-6e3f-4a30-9db3-76e56ead421c", "embedding": null, "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "58014fba4b5e7e50ea702c9c1297186a1940f527c691085df70e28ef425d64d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4af015e1-7c98-4d3c-8c15-4f2b6b080213", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "ae6803f1591ff529fd4d77dbcc8e4562a0f137ed5061da6d273aace3bd7f34f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56b85528-65a0-42d8-be31-64dcd6523a70", "node_type": "1", "metadata": {}, "hash": "2c25b730f088b4ca3a78ce732638aa30f3b2eb2a9218d716be23b4c919fd5632", "class_name": "RelatedNodeInfo"}}, "text": "03 2.74 2.55 2.41 -\n32768\u2713 3.04 2.75 2.56 2.42 2.33\n\u2713 \u2713 3.05 2.76 2.57 2.42 2.32\nTo bridge this gap, we open embedding and normalization layers for training. As shown in Table 2,\nthey occupy limited parameters but make effects for long context adaptation. Especially for normal-\nization layers, the parameters are only 0.004% in the whole Llama2 7B. We denote this improved\nversion of LoRA as LoRA+in experiments.\n4 E XPERIMENT\n4.1 E XPERIMENTAL SETTINGS\nModels We extend the pre-trained 7B, 13B, and 70B Llama2 (Touvron et al., 2023b) models. The\nmaximum extended context window sizes are up to 100k for 7B models, 65536 for 13B models,\nand 32768 for 70B models. The position indices for these models are re-scaled with Position\nInterpolation (Chen et al., 2023).", "start_char_idx": 1743, "end_char_idx": 2507, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56b85528-65a0-42d8-be31-64dcd6523a70": {"__data__": {"id_": "56b85528-65a0-42d8-be31-64dcd6523a70", "embedding": null, "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "58014fba4b5e7e50ea702c9c1297186a1940f527c691085df70e28ef425d64d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ff29eca-6e3f-4a30-9db3-76e56ead421c", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "14b2769b87c3c8158f9299bb1e90b46a98f33f755dc95acc2062064be091e0bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0ac23c6-63b2-4280-8b14-b87114224ee4", "node_type": "1", "metadata": {}, "hash": "2260db10ac5628388ceb4fa21ab03186dff7fb75aa4c0b083d91787a29dc86c1", "class_name": "RelatedNodeInfo"}}, "text": "32\nTo bridge this gap, we open embedding and normalization layers for training. As shown in Table 2,\nthey occupy limited parameters but make effects for long context adaptation. Especially for normal-\nization layers, the parameters are only 0.004% in the whole Llama2 7B. We denote this improved\nversion of LoRA as LoRA+in experiments.\n4 E XPERIMENT\n4.1 E XPERIMENTAL SETTINGS\nModels We extend the pre-trained 7B, 13B, and 70B Llama2 (Touvron et al., 2023b) models. The\nmaximum extended context window sizes are up to 100k for 7B models, 65536 for 13B models,\nand 32768 for 70B models. The position indices for these models are re-scaled with Position\nInterpolation (Chen et al., 2023).\nTraining Procedure We follow most training hyper-parameters in Position Interpolation (Chen\net al., 2023), except that our batch size is smaller as we use a single 8 \u00d7A100 GPUs machine in some\ncases. All models are fine-tuned via the next token prediction objective.", "start_char_idx": 1821, "end_char_idx": 2774, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0ac23c6-63b2-4280-8b14-b87114224ee4": {"__data__": {"id_": "e0ac23c6-63b2-4280-8b14-b87114224ee4", "embedding": null, "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "58014fba4b5e7e50ea702c9c1297186a1940f527c691085df70e28ef425d64d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56b85528-65a0-42d8-be31-64dcd6523a70", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "f21abec561024f8eac19b14b87a8ff56a927006330ceed25c21b0c28803eec1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8998f858-59f9-4878-b26a-519dfc30d18a", "node_type": "1", "metadata": {}, "hash": "c14ff5a5364055b2433787c9c9afd60719d59d0c3ff39334b15986909399b92f", "class_name": "RelatedNodeInfo"}}, "text": "We denote this improved\nversion of LoRA as LoRA+in experiments.\n4 E XPERIMENT\n4.1 E XPERIMENTAL SETTINGS\nModels We extend the pre-trained 7B, 13B, and 70B Llama2 (Touvron et al., 2023b) models. The\nmaximum extended context window sizes are up to 100k for 7B models, 65536 for 13B models,\nand 32768 for 70B models. The position indices for these models are re-scaled with Position\nInterpolation (Chen et al., 2023).\nTraining Procedure We follow most training hyper-parameters in Position Interpolation (Chen\net al., 2023), except that our batch size is smaller as we use a single 8 \u00d7A100 GPUs machine in some\ncases. All models are fine-tuned via the next token prediction objective. We use AdamW (Loshchilov\n& Hutter, 2019) with \u03b21= 0.9and\u03b22= 0.95.", "start_char_idx": 2093, "end_char_idx": 2840, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8998f858-59f9-4878-b26a-519dfc30d18a": {"__data__": {"id_": "8998f858-59f9-4878-b26a-519dfc30d18a", "embedding": null, "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "58014fba4b5e7e50ea702c9c1297186a1940f527c691085df70e28ef425d64d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0ac23c6-63b2-4280-8b14-b87114224ee4", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "eca4e0f52b0f0d02c73b24840d1285f265a2fbe52c36b920a03f467ee1efa1de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "323c6721-b9d4-4ff2-9359-ed912adde66f", "node_type": "1", "metadata": {}, "hash": "b7e6392403a2719d6f8bcdf84ba197a0ff98c6ed988dcbce01b8721cf8830c32", "class_name": "RelatedNodeInfo"}}, "text": "The\nmaximum extended context window sizes are up to 100k for 7B models, 65536 for 13B models,\nand 32768 for 70B models. The position indices for these models are re-scaled with Position\nInterpolation (Chen et al., 2023).\nTraining Procedure We follow most training hyper-parameters in Position Interpolation (Chen\net al., 2023), except that our batch size is smaller as we use a single 8 \u00d7A100 GPUs machine in some\ncases. All models are fine-tuned via the next token prediction objective. We use AdamW (Loshchilov\n& Hutter, 2019) with \u03b21= 0.9and\u03b22= 0.95. The learning rate is set to 2\u00d710\u22125for 7B and 13B\nmodels, and 10\u22125for 70B models. We also use a linear learning rate warmup. The weight decay is\n6", "start_char_idx": 2287, "end_char_idx": 2986, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "323c6721-b9d4-4ff2-9359-ed912adde66f": {"__data__": {"id_": "323c6721-b9d4-4ff2-9359-ed912adde66f", "embedding": null, "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d6d4703-f15a-41d4-9b36-b455938824f6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "74fed6190a90bf72e874bd898bc96859f048532fcff22effd34a399d3095d6c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8998f858-59f9-4878-b26a-519dfc30d18a", "node_type": "1", "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "80c157689e0b5debc9d5e20515192022677fecb70eafe45a8f0b62e044589c01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14ad5045-be95-4ff8-bbfc-125da14a28d9", "node_type": "1", "metadata": {}, "hash": "bb18e96c961a386125a9a22502d39f84c7f257826281dd360c8596efbd0618ac", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 4: Maximum context length that we can fine-tune for various model sizes on a single 8 \u00d7\nA100 machine. We use the same training and evaluation settings as in Table 3. We use Flash-\nAttention2 (Dao, 2023) and DeepSpeed (Rasley et al., 2020) in stage 3 during fine-tuning. With\nLongLoRA, the maximum context length for 7B, 13B, and 70B models are 100k, 64k, and 32k\nrespectively. Evaluation on PG19 (Rae et al., 2020) is in Section B.1 in the appendix.", "start_char_idx": 0, "end_char_idx": 500, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14ad5045-be95-4ff8-bbfc-125da14a28d9": {"__data__": {"id_": "14ad5045-be95-4ff8-bbfc-125da14a28d9", "embedding": null, "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d6d4703-f15a-41d4-9b36-b455938824f6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "74fed6190a90bf72e874bd898bc96859f048532fcff22effd34a399d3095d6c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "323c6721-b9d4-4ff2-9359-ed912adde66f", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "2b3ef16da055edecf1f2d6a9f358508a8d1e7a414da27773638d80f33a58b659", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc638e13-ca57-41b3-b18f-85c23a16001a", "node_type": "1", "metadata": {}, "hash": "689fba4e5cb28cd853d821316e11d4eea8e880c630be03e4767945b93f6261b4", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 4: Maximum context length that we can fine-tune for various model sizes on a single 8 \u00d7\nA100 machine. We use the same training and evaluation settings as in Table 3. We use Flash-\nAttention2 (Dao, 2023) and DeepSpeed (Rasley et al., 2020) in stage 3 during fine-tuning. With\nLongLoRA, the maximum context length for 7B, 13B, and 70B models are 100k, 64k, and 32k\nrespectively. Evaluation on PG19 (Rae et al., 2020) is in Section B.1 in the appendix.\nSizeTraining\nContext LengthEvaluation Context Length\n2048 4096 8192 16384 32768 65536 100,000\n7B 100,000 3.36 3.01 2.78 2.60 2.58 2.57 2.52\n13B 65536 3.20 2.88 2.66 2.50 2.39 2.38 -\n70B 32768 2.84 2.57 2.39 2.26 2.17 - -\nTable 5: Topic retrieval evaluation with LongChat (Li et al., 2023).", "start_char_idx": 0, "end_char_idx": 790, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc638e13-ca57-41b3-b18f-85c23a16001a": {"__data__": {"id_": "cc638e13-ca57-41b3-b18f-85c23a16001a", "embedding": null, "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d6d4703-f15a-41d4-9b36-b455938824f6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "74fed6190a90bf72e874bd898bc96859f048532fcff22effd34a399d3095d6c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14ad5045-be95-4ff8-bbfc-125da14a28d9", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "bb262682255a683d7a9baa83b526d54d580c9cf46544dddfede8175cd6671b4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cfe1d74a-d20f-4001-a222-5568f3b77b28", "node_type": "1", "metadata": {}, "hash": "7fc23dece1cead8b084d37d13717d6099866e930c47abf05c69c9a8d0aba9b43", "class_name": "RelatedNodeInfo"}}, "text": "Evaluation on PG19 (Rae et al., 2020) is in Section B.1 in the appendix.\nSizeTraining\nContext LengthEvaluation Context Length\n2048 4096 8192 16384 32768 65536 100,000\n7B 100,000 3.36 3.01 2.78 2.60 2.58 2.57 2.52\n13B 65536 3.20 2.88 2.66 2.50 2.39 2.38 -\n70B 32768 2.84 2.57 2.39 2.26 2.17 - -\nTable 5: Topic retrieval evaluation with LongChat (Li et al., 2023). We compare our model to\nother open-source long-context LLMs. This task involves retrieving target topics from a very long\nconversation with around 3k, 6k, 10k, 13k, and 16k context lengths. As some questions in the\nevaluation set are longer than 16k, our model is fine-tuned upon Llama2 13B.", "start_char_idx": 428, "end_char_idx": 1082, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfe1d74a-d20f-4001-a222-5568f3b77b28": {"__data__": {"id_": "cfe1d74a-d20f-4001-a222-5568f3b77b28", "embedding": null, "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d6d4703-f15a-41d4-9b36-b455938824f6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "74fed6190a90bf72e874bd898bc96859f048532fcff22effd34a399d3095d6c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc638e13-ca57-41b3-b18f-85c23a16001a", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "9fcdcd6e81346fec19870abc3f250cb5cc87ff636e95f2a01a983cd5ec6904a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e10ba924-2c64-46ad-9d62-61cefae5022a", "node_type": "1", "metadata": {}, "hash": "31afc8fe14556b4c9d570d14256eaa2b0ec69af20a9c0a141c861791d24143a2", "class_name": "RelatedNodeInfo"}}, "text": "We compare our model to\nother open-source long-context LLMs. This task involves retrieving target topics from a very long\nconversation with around 3k, 6k, 10k, 13k, and 16k context lengths. As some questions in the\nevaluation set are longer than 16k, our model is fine-tuned upon Llama2 13B. It achieves comparable\nperformance to the state-of-the-art LongChat-13B (Li et al., 2023) with a lower fine-tuning cost.", "start_char_idx": 791, "end_char_idx": 1203, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e10ba924-2c64-46ad-9d62-61cefae5022a": {"__data__": {"id_": "e10ba924-2c64-46ad-9d62-61cefae5022a", "embedding": null, "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d6d4703-f15a-41d4-9b36-b455938824f6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "74fed6190a90bf72e874bd898bc96859f048532fcff22effd34a399d3095d6c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfe1d74a-d20f-4001-a222-5568f3b77b28", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "dfe025ea4f85a9d030a8e7e5755c9a07f9b83bb1076f0b078482a31131cc48ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6e95a98-e8e5-4fd4-867a-4cc5a95e7fb7", "node_type": "1", "metadata": {}, "hash": "0557dd544d92379b4f2752be8d31553debecfdf41d803862b401550e47d9b0c0", "class_name": "RelatedNodeInfo"}}, "text": "We compare our model to\nother open-source long-context LLMs. This task involves retrieving target topics from a very long\nconversation with around 3k, 6k, 10k, 13k, and 16k context lengths. As some questions in the\nevaluation set are longer than 16k, our model is fine-tuned upon Llama2 13B. It achieves comparable\nperformance to the state-of-the-art LongChat-13B (Li et al., 2023) with a lower fine-tuning cost.\nEvaluation Context 3k 6k 10k 13k 16k\nChatGLM2-6B (Du et al., 2022) 0.88 0.46 0.02 0.02 0.02\nMPT-30B-chat (Team, 2023a) 0.96 1.0 0.76 - -\nMPT-7B-storywriter (Team, 2023b) 0.46 0.46 0.28 0.34 0.36\nLongChat-13B (Li et al., 2023) 1.0 1.0 1.0 0.98 0.9\nOurs-13B 1.0 0.98 0.98 0.98 0.94\nzero.", "start_char_idx": 791, "end_char_idx": 1489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6e95a98-e8e5-4fd4-867a-4cc5a95e7fb7": {"__data__": {"id_": "b6e95a98-e8e5-4fd4-867a-4cc5a95e7fb7", "embedding": null, "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d6d4703-f15a-41d4-9b36-b455938824f6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "74fed6190a90bf72e874bd898bc96859f048532fcff22effd34a399d3095d6c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e10ba924-2c64-46ad-9d62-61cefae5022a", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "ad6138fffe632784048384aa5423a98b508724cb9fcf6097c3afb580021e2db8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3af3c46-4db9-49da-95a6-8e417c96f1ca", "node_type": "1", "metadata": {}, "hash": "2a40dcb8363d5cdfa5ec7d5e9614631897ccf8d1d178b692f071cd85f5a90d8c", "class_name": "RelatedNodeInfo"}}, "text": "Evaluation Context 3k 6k 10k 13k 16k\nChatGLM2-6B (Du et al., 2022) 0.88 0.46 0.02 0.02 0.02\nMPT-30B-chat (Team, 2023a) 0.96 1.0 0.76 - -\nMPT-7B-storywriter (Team, 2023b) 0.46 0.46 0.28 0.34 0.36\nLongChat-13B (Li et al., 2023) 1.0 1.0 1.0 0.98 0.9\nOurs-13B 1.0 0.98 0.98 0.98 0.94\nzero. We set the per-device batch size as 1 and gradient accumulation steps as 8, which means that\nthe global batch size equals 64, using 8 GPUs. We train our models for 1000 steps.\nDatasets We use the Redpajama (Computer, 2023) dataset for training.", "start_char_idx": 1204, "end_char_idx": 1734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3af3c46-4db9-49da-95a6-8e417c96f1ca": {"__data__": {"id_": "e3af3c46-4db9-49da-95a6-8e417c96f1ca", "embedding": null, "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d6d4703-f15a-41d4-9b36-b455938824f6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "74fed6190a90bf72e874bd898bc96859f048532fcff22effd34a399d3095d6c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6e95a98-e8e5-4fd4-867a-4cc5a95e7fb7", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "7fe7bfe66d1dd80872712fe5d5fb75f1a1bb9c8bda57ea02768638ef477c2ac8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a628b0a-6d21-47fc-8a0b-7f7ab999c8bc", "node_type": "1", "metadata": {}, "hash": "7cce2ece221e15f5390ee49f6ceeb0fd1ac3f72a93cddb4531046d3b505a22bc", "class_name": "RelatedNodeInfo"}}, "text": "We set the per-device batch size as 1 and gradient accumulation steps as 8, which means that\nthe global batch size equals 64, using 8 GPUs. We train our models for 1000 steps.\nDatasets We use the Redpajama (Computer, 2023) dataset for training. We evaluate the long-\nsequence language modeling performance of our fine-tuned models on the book corpus dataset\nPG19 (Rae et al., 2020) and the cleaned Arxiv Math proof-pile dataset (Azerbayev et al., 2022). We\nuse the test split of PG19 (Rae et al., 2020), consisting of 100 documents. For the proof-pile dataset,\nwe also use the test split of it for evaluation. We follow Position Interpolation (Chen et al., 2023)\nfor proof-pile data processing. We evaluate perplexity by using a sliding window approach with\nS= 256 , following (Press et al., 2022).\n4.2 M AINRESULTS\nLong-sequence Language Modeling.", "start_char_idx": 1490, "end_char_idx": 2338, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a628b0a-6d21-47fc-8a0b-7f7ab999c8bc": {"__data__": {"id_": "2a628b0a-6d21-47fc-8a0b-7f7ab999c8bc", "embedding": null, "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d6d4703-f15a-41d4-9b36-b455938824f6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "74fed6190a90bf72e874bd898bc96859f048532fcff22effd34a399d3095d6c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3af3c46-4db9-49da-95a6-8e417c96f1ca", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "615cfcd7cf9a47468ec5a9fd040c463bb6601428bd86edc58329b2152fff9689", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a6cf700-64c7-4d3e-a15a-8283746cbb3e", "node_type": "1", "metadata": {}, "hash": "03d3a7d027d2831d0420c7055361328364e143ce82e6f573f79921eacc815355", "class_name": "RelatedNodeInfo"}}, "text": "We train our models for 1000 steps.\nDatasets We use the Redpajama (Computer, 2023) dataset for training. We evaluate the long-\nsequence language modeling performance of our fine-tuned models on the book corpus dataset\nPG19 (Rae et al., 2020) and the cleaned Arxiv Math proof-pile dataset (Azerbayev et al., 2022). We\nuse the test split of PG19 (Rae et al., 2020), consisting of 100 documents. For the proof-pile dataset,\nwe also use the test split of it for evaluation. We follow Position Interpolation (Chen et al., 2023)\nfor proof-pile data processing. We evaluate perplexity by using a sliding window approach with\nS= 256 , following (Press et al., 2022).\n4.2 M AINRESULTS\nLong-sequence Language Modeling. In Table 3, we report the perplexity for our models and\nbaseline on proof-pile (Azerbayev et al., 2022) and PG19 datasets. Under certain training context\nlengths, our models achieve better perplexity with longer context sizes.", "start_char_idx": 1630, "end_char_idx": 2565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a6cf700-64c7-4d3e-a15a-8283746cbb3e": {"__data__": {"id_": "3a6cf700-64c7-4d3e-a15a-8283746cbb3e", "embedding": null, "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d6d4703-f15a-41d4-9b36-b455938824f6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "74fed6190a90bf72e874bd898bc96859f048532fcff22effd34a399d3095d6c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a628b0a-6d21-47fc-8a0b-7f7ab999c8bc", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6a8a7734ee0bcc815d9bbd7e96daf9f0bb6a6dda2847d176993ffd3ffc26c96b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6092d867-37f3-4da3-83e9-024adacf4dc3", "node_type": "1", "metadata": {}, "hash": "3305d7b790ddca82e4e6b86cc4c9238b1d73044fb9111c95d76f74e8b6c962fa", "class_name": "RelatedNodeInfo"}}, "text": "We\nuse the test split of PG19 (Rae et al., 2020), consisting of 100 documents. For the proof-pile dataset,\nwe also use the test split of it for evaluation. We follow Position Interpolation (Chen et al., 2023)\nfor proof-pile data processing. We evaluate perplexity by using a sliding window approach with\nS= 256 , following (Press et al., 2022).\n4.2 M AINRESULTS\nLong-sequence Language Modeling. In Table 3, we report the perplexity for our models and\nbaseline on proof-pile (Azerbayev et al., 2022) and PG19 datasets. Under certain training context\nlengths, our models achieve better perplexity with longer context sizes. This indicates the effectiveness\nof our efficient fine-tuning method. In Table 3, for the same training and evaluation context length\ncases, the perplexity decreases as the context size increases. By increasing the context window size\nfrom 8192 to 32768, for the Llama2 7B model, we observe that the perplexity gets better from 2.72\nto 2.50 by -0.22.", "start_char_idx": 1944, "end_char_idx": 2916, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6092d867-37f3-4da3-83e9-024adacf4dc3": {"__data__": {"id_": "6092d867-37f3-4da3-83e9-024adacf4dc3", "embedding": null, "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d6d4703-f15a-41d4-9b36-b455938824f6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "74fed6190a90bf72e874bd898bc96859f048532fcff22effd34a399d3095d6c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a6cf700-64c7-4d3e-a15a-8283746cbb3e", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "b41f5eba854fccdcd855bb3a4f15aae4d35a9c106e9deb325c8781533ff66881", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50220c6d-4184-4db7-8317-c54f2530f230", "node_type": "1", "metadata": {}, "hash": "adf4622705b5a212105a94f330c91f930b8e5e9ffde3e7867505bfc5bd0a8f7e", "class_name": "RelatedNodeInfo"}}, "text": "We evaluate perplexity by using a sliding window approach with\nS= 256 , following (Press et al., 2022).\n4.2 M AINRESULTS\nLong-sequence Language Modeling. In Table 3, we report the perplexity for our models and\nbaseline on proof-pile (Azerbayev et al., 2022) and PG19 datasets. Under certain training context\nlengths, our models achieve better perplexity with longer context sizes. This indicates the effectiveness\nof our efficient fine-tuning method. In Table 3, for the same training and evaluation context length\ncases, the perplexity decreases as the context size increases. By increasing the context window size\nfrom 8192 to 32768, for the Llama2 7B model, we observe that the perplexity gets better from 2.72\nto 2.50 by -0.22. For Llama2 13B model, we observe that the perplexity reduces by -0.28.\nIn Table 4, we further examine the maximum context length that we can fine-tune on a single 8 \u00d7\nA100 machine.", "start_char_idx": 2185, "end_char_idx": 3097, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50220c6d-4184-4db7-8317-c54f2530f230": {"__data__": {"id_": "50220c6d-4184-4db7-8317-c54f2530f230", "embedding": null, "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d6d4703-f15a-41d4-9b36-b455938824f6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "74fed6190a90bf72e874bd898bc96859f048532fcff22effd34a399d3095d6c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6092d867-37f3-4da3-83e9-024adacf4dc3", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6c8f4a8627e888d3cb7939a73e0e0b0f3c1491b74061abe0f750b4185f0e51b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e6f3598-1f75-4c43-8198-3406a01623e6", "node_type": "1", "metadata": {}, "hash": "6420473df71990c8d1b357a17062ecd8694f8a8c1a0e0e76f82826fb79567b35", "class_name": "RelatedNodeInfo"}}, "text": "Under certain training context\nlengths, our models achieve better perplexity with longer context sizes. This indicates the effectiveness\nof our efficient fine-tuning method. In Table 3, for the same training and evaluation context length\ncases, the perplexity decreases as the context size increases. By increasing the context window size\nfrom 8192 to 32768, for the Llama2 7B model, we observe that the perplexity gets better from 2.72\nto 2.50 by -0.22. For Llama2 13B model, we observe that the perplexity reduces by -0.28.\nIn Table 4, we further examine the maximum context length that we can fine-tune on a single 8 \u00d7\nA100 machine. We extend Llama2 7B, 13B, and 70B to 100k, 65536, and 32768 context length\nrespectively. LongLoRA achieves promising results on these extremely large settings. In addition,\nwe find some perplexity degradation on small context sizes for the extended models. This is a known\nlimitation of Position Interpolation (Chen et al., 2023).\nRetrieval-based Evaluation.", "start_char_idx": 2462, "end_char_idx": 3456, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e6f3598-1f75-4c43-8198-3406a01623e6": {"__data__": {"id_": "0e6f3598-1f75-4c43-8198-3406a01623e6", "embedding": null, "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d6d4703-f15a-41d4-9b36-b455938824f6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "74fed6190a90bf72e874bd898bc96859f048532fcff22effd34a399d3095d6c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50220c6d-4184-4db7-8317-c54f2530f230", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "7c46fe5a0231e702dad75c5c72767f594b2e0ae0c5046a1c2389a0f8806e98f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76be207d-bb54-4c0e-a819-2ff07f84f5a5", "node_type": "1", "metadata": {}, "hash": "e2d92a5be8fe139b8b59d89741f2ff3dd6c522d15683c224a8b663fc831d4354", "class_name": "RelatedNodeInfo"}}, "text": "For Llama2 13B model, we observe that the perplexity reduces by -0.28.\nIn Table 4, we further examine the maximum context length that we can fine-tune on a single 8 \u00d7\nA100 machine. We extend Llama2 7B, 13B, and 70B to 100k, 65536, and 32768 context length\nrespectively. LongLoRA achieves promising results on these extremely large settings. In addition,\nwe find some perplexity degradation on small context sizes for the extended models. This is a known\nlimitation of Position Interpolation (Chen et al., 2023).\nRetrieval-based Evaluation. We conduct experiments on retrieval in long contexts. In Table 5, we\ncompare our model with other open LLMs on the topic retrieval task introduced in LongChat (Li et al.,\n2023). This task is to retrieve the target topic from a very long conversation, with lengths varying\nfrom 3k, 6k, 10k, 13k, to 16k.", "start_char_idx": 2917, "end_char_idx": 3759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76be207d-bb54-4c0e-a819-2ff07f84f5a5": {"__data__": {"id_": "76be207d-bb54-4c0e-a819-2ff07f84f5a5", "embedding": null, "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d6d4703-f15a-41d4-9b36-b455938824f6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "74fed6190a90bf72e874bd898bc96859f048532fcff22effd34a399d3095d6c2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e6f3598-1f75-4c43-8198-3406a01623e6", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "fb52afc3fbad5b0e216dca1fa74db98483fdfb2396c6a20a6db2555eaa770ca1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "883c8cba-2a6d-48b4-98ac-02211a140eae", "node_type": "1", "metadata": {}, "hash": "ab80d3ff1a4bb3318d8685168a54e5cc04dde7a3ba371c150932e416227f6234", "class_name": "RelatedNodeInfo"}}, "text": "We extend Llama2 7B, 13B, and 70B to 100k, 65536, and 32768 context length\nrespectively. LongLoRA achieves promising results on these extremely large settings. In addition,\nwe find some perplexity degradation on small context sizes for the extended models. This is a known\nlimitation of Position Interpolation (Chen et al., 2023).\nRetrieval-based Evaluation. We conduct experiments on retrieval in long contexts. In Table 5, we\ncompare our model with other open LLMs on the topic retrieval task introduced in LongChat (Li et al.,\n2023). This task is to retrieve the target topic from a very long conversation, with lengths varying\nfrom 3k, 6k, 10k, 13k, to 16k. As some questions in LongChat (Li et al., 2023) are longer than 16k,\nwe fine-tuned Llama2 13B with a context length of 18k. The training cost is similar to that for 16k.\n7", "start_char_idx": 3098, "end_char_idx": 3931, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "883c8cba-2a6d-48b4-98ac-02211a140eae": {"__data__": {"id_": "883c8cba-2a6d-48b4-98ac-02211a140eae", "embedding": null, "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82569dcb-23d6-4577-baed-6323c76765ea", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "017169e57c627ecb68cc28d6022ea21305b3e50cb4af772a64397ee25d225a33", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76be207d-bb54-4c0e-a819-2ff07f84f5a5", "node_type": "1", "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "2fa75c3696bd844cc322730bf658fd293c487819f24289469a126955444a4bc5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3618613-2d30-4a24-a6dd-3b21bc2c69fa", "node_type": "1", "metadata": {}, "hash": "210ed750016dfd46cf5c62cd17608aef4d564f9fa1fa2d9f4d732f242a7d1321", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n0%20%40%60%80%100%2k4k6k8k10k12k14k16k18k20k22k24k26k28k30k32k34k36k38k40k42k44k46k48kPasskey Retrieval Accuracy\nLlama2 7BLlama2 7B (extended PI)Ours 7BOurs 7B (extended PI)\nFigure 4: Accuracy comparison on passkey retrieval between Llama2 7B and our 7B model fine-tuned\non 32768 context length. Our model presents no retrieval accuracy degradation until 33k or 34k,\nwhich exceeds the context length. It can further enhance its capability of long sequence modeling\nthrough a straightforward extension of position embeddings, without additional fine-tuning.", "start_char_idx": 0, "end_char_idx": 601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3618613-2d30-4a24-a6dd-3b21bc2c69fa": {"__data__": {"id_": "f3618613-2d30-4a24-a6dd-3b21bc2c69fa", "embedding": null, "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82569dcb-23d6-4577-baed-6323c76765ea", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "017169e57c627ecb68cc28d6022ea21305b3e50cb4af772a64397ee25d225a33", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "883c8cba-2a6d-48b4-98ac-02211a140eae", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "19a688210aac0bfc5ffb038bceb077be83edc5b9d43742e144455fb6d3962afb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a77fa51f-9413-43e1-a600-67f0b804f708", "node_type": "1", "metadata": {}, "hash": "c9755a86f7ad52464eae8cddc14189c5215318619608ce7bfe62136861c00f1c", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\n0%20%40%60%80%100%2k4k6k8k10k12k14k16k18k20k22k24k26k28k30k32k34k36k38k40k42k44k46k48kPasskey Retrieval Accuracy\nLlama2 7BLlama2 7B (extended PI)Ours 7BOurs 7B (extended PI)\nFigure 4: Accuracy comparison on passkey retrieval between Llama2 7B and our 7B model fine-tuned\non 32768 context length. Our model presents no retrieval accuracy degradation until 33k or 34k,\nwhich exceeds the context length. It can further enhance its capability of long sequence modeling\nthrough a straightforward extension of position embeddings, without additional fine-tuning.\n0%20%40%60%80%100%2k4k6k8k10k12k14k16k18k20k22k24k26k28k30k32k34k36k38k40k42k44k46k48kPasskey Retrieval Accuracy\nLlama2 7BOurs 7B 32kOurs 7B 32k (extended PI to 48k)15.828.638.168.158.148.128.118.18.088.048.0215.828.178.18.078.068.037.997.997.967.957.94681012141601002003004005006007008009001000Perplexity along \ufb01ne-tuning steps\nFull FTLoRA+Fine-tuning stepsPerplexity\nFigure 5: Ablation on fine-tuning steps in both full fine-tuning and LoRA+.", "start_char_idx": 0, "end_char_idx": 1046, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a77fa51f-9413-43e1-a600-67f0b804f708": {"__data__": {"id_": "a77fa51f-9413-43e1-a600-67f0b804f708", "embedding": null, "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82569dcb-23d6-4577-baed-6323c76765ea", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "017169e57c627ecb68cc28d6022ea21305b3e50cb4af772a64397ee25d225a33", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3618613-2d30-4a24-a6dd-3b21bc2c69fa", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4e419210353d3ee20605357a184abebc39292382f763893ab02636eda32ebc4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b864fc11-4aa6-4152-a231-4106015bd133", "node_type": "1", "metadata": {}, "hash": "c96f3079d44982b817bdd5be92f780c571286a591fc8165480dfae1aa8cdaae7", "class_name": "RelatedNodeInfo"}}, "text": "We fine-tune Llama2 (Tou-\nvron et al., 2023b) 7B with the proposed S2-Attn. The target context length is 8192. We use RedPa-\njama (Computer, 2023) for training and PG19 (Rae et al., 2020) validation set for perplexity testing.\nFull fine-tuning converges faster than LoRA+at the beginning, but the final performance gap is small.\nOur model achieves comparable performance to LongChat-13B (Li et al., 2023), the state-of-the-art\nmodel in this task. Unlike LongChat-13B (Li et al., 2023), which is fully fine-tuned on self-collected\nlong context conversation text, our model is efficiently adapted on RedPajama (Computer, 2023) via\nnext-token generation. Our model even slightly outperforms LongChat-13B in the 16k evaluation.\nIn Figure 4, we present the passkey retrieval accuracy of our model, following Landmark Atten-\ntion (Mohtashami & Jaggi, 2023).", "start_char_idx": 1047, "end_char_idx": 1898, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b864fc11-4aa6-4152-a231-4106015bd133": {"__data__": {"id_": "b864fc11-4aa6-4152-a231-4106015bd133", "embedding": null, "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82569dcb-23d6-4577-baed-6323c76765ea", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "017169e57c627ecb68cc28d6022ea21305b3e50cb4af772a64397ee25d225a33", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a77fa51f-9413-43e1-a600-67f0b804f708", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "69ac91abc25403eae09e6ff8a755952e98443e334febcd75b32a11a205334201", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28b19a65-bbcd-46ea-9a28-997dcfb1cb92", "node_type": "1", "metadata": {}, "hash": "2285be311e00f17a96abf3b1aa0ba740b7dac5b9597d69c6c578d5edff531d70", "class_name": "RelatedNodeInfo"}}, "text": "Full fine-tuning converges faster than LoRA+at the beginning, but the final performance gap is small.\nOur model achieves comparable performance to LongChat-13B (Li et al., 2023), the state-of-the-art\nmodel in this task. Unlike LongChat-13B (Li et al., 2023), which is fully fine-tuned on self-collected\nlong context conversation text, our model is efficiently adapted on RedPajama (Computer, 2023) via\nnext-token generation. Our model even slightly outperforms LongChat-13B in the 16k evaluation.\nIn Figure 4, we present the passkey retrieval accuracy of our model, following Landmark Atten-\ntion (Mohtashami & Jaggi, 2023). This task has also been adopted by other literature (Chen et al.,\n2023; Tworkowski et al., 2023). In this task, the models need to find a random passkey hidden in a\nlong document. We show the document format is in Section A.2 in the appendix.", "start_char_idx": 1274, "end_char_idx": 2141, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28b19a65-bbcd-46ea-9a28-997dcfb1cb92": {"__data__": {"id_": "28b19a65-bbcd-46ea-9a28-997dcfb1cb92", "embedding": null, "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82569dcb-23d6-4577-baed-6323c76765ea", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "017169e57c627ecb68cc28d6022ea21305b3e50cb4af772a64397ee25d225a33", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b864fc11-4aa6-4152-a231-4106015bd133", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "7aabf4e3adce17f76f8807b0f49e91d8bfe8bf4e13a0336cc485ab01b3b4e8b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26d047f5-8d47-4472-904d-ede7a2ea58d5", "node_type": "1", "metadata": {}, "hash": "84ad2bdb96c6f5ddf2250683ef554c26c3b8d36b21c93ef8824a118458414ad9", "class_name": "RelatedNodeInfo"}}, "text": "Unlike LongChat-13B (Li et al., 2023), which is fully fine-tuned on self-collected\nlong context conversation text, our model is efficiently adapted on RedPajama (Computer, 2023) via\nnext-token generation. Our model even slightly outperforms LongChat-13B in the 16k evaluation.\nIn Figure 4, we present the passkey retrieval accuracy of our model, following Landmark Atten-\ntion (Mohtashami & Jaggi, 2023). This task has also been adopted by other literature (Chen et al.,\n2023; Tworkowski et al., 2023). In this task, the models need to find a random passkey hidden in a\nlong document. We show the document format is in Section A.2 in the appendix. We study Llama2\n7B (Touvron et al., 2023b) and our LongLoRA model which fine-tunes Llama2 7B with 32768\ncontext length. We test the passkey retrieval accuracy from 1k to 34k, with an interval of roughly 1k\n(as the sentence length can not be precisely controlled).", "start_char_idx": 1494, "end_char_idx": 2405, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26d047f5-8d47-4472-904d-ede7a2ea58d5": {"__data__": {"id_": "26d047f5-8d47-4472-904d-ede7a2ea58d5", "embedding": null, "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82569dcb-23d6-4577-baed-6323c76765ea", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "017169e57c627ecb68cc28d6022ea21305b3e50cb4af772a64397ee25d225a33", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28b19a65-bbcd-46ea-9a28-997dcfb1cb92", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "1a588242980a16c33d0e8035b3f8180eca04b3e2034d827c207bb8533f894721", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e53ef68-3986-4eef-a7e6-17f4dd76f2d5", "node_type": "1", "metadata": {}, "hash": "124b6fc19897863f11c64edb6ce7b2f818f923ade565c8ddb2928a195d1f5163", "class_name": "RelatedNodeInfo"}}, "text": "In Figure 4, we present the passkey retrieval accuracy of our model, following Landmark Atten-\ntion (Mohtashami & Jaggi, 2023). This task has also been adopted by other literature (Chen et al.,\n2023; Tworkowski et al., 2023). In this task, the models need to find a random passkey hidden in a\nlong document. We show the document format is in Section A.2 in the appendix. We study Llama2\n7B (Touvron et al., 2023b) and our LongLoRA model which fine-tunes Llama2 7B with 32768\ncontext length. We test the passkey retrieval accuracy from 1k to 34k, with an interval of roughly 1k\n(as the sentence length can not be precisely controlled). For each document length, we test the model\n10 times with different random passkey values. Our model achieves reasonable passkey retrieval\naccuracy until 33k or 34k. Without further fine-tuning, We modify the max position embeddings to\n48k in the position interpolation, which is the Ours 7B (extended PI) in Figure 4.", "start_char_idx": 1771, "end_char_idx": 2724, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8e53ef68-3986-4eef-a7e6-17f4dd76f2d5": {"__data__": {"id_": "8e53ef68-3986-4eef-a7e6-17f4dd76f2d5", "embedding": null, "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82569dcb-23d6-4577-baed-6323c76765ea", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "017169e57c627ecb68cc28d6022ea21305b3e50cb4af772a64397ee25d225a33", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26d047f5-8d47-4472-904d-ede7a2ea58d5", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "f43f048ba571d8a68c7bb5a00566bc058f090ee0e1ff3eb19408c19a2af93144", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd1738d3-7246-46bf-8432-f8407638bd16", "node_type": "1", "metadata": {}, "hash": "749424565a3a95a3b8b68116c5bd8180c4d088bca0d4a1380690142b499a5d5a", "class_name": "RelatedNodeInfo"}}, "text": "In this task, the models need to find a random passkey hidden in a\nlong document. We show the document format is in Section A.2 in the appendix. We study Llama2\n7B (Touvron et al., 2023b) and our LongLoRA model which fine-tunes Llama2 7B with 32768\ncontext length. We test the passkey retrieval accuracy from 1k to 34k, with an interval of roughly 1k\n(as the sentence length can not be precisely controlled). For each document length, we test the model\n10 times with different random passkey values. Our model achieves reasonable passkey retrieval\naccuracy until 33k or 34k. Without further fine-tuning, We modify the max position embeddings to\n48k in the position interpolation, which is the Ours 7B (extended PI) in Figure 4. We show that this\nmodel can handle longer documents by simply extending the position interpolation.", "start_char_idx": 1997, "end_char_idx": 2824, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd1738d3-7246-46bf-8432-f8407638bd16": {"__data__": {"id_": "fd1738d3-7246-46bf-8432-f8407638bd16", "embedding": null, "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82569dcb-23d6-4577-baed-6323c76765ea", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "017169e57c627ecb68cc28d6022ea21305b3e50cb4af772a64397ee25d225a33", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e53ef68-3986-4eef-a7e6-17f4dd76f2d5", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "bbef0b2ada41c520b2de0db2bbfdeb3f7d90d841c64fc4a2e0d6d0b3ba5b5233", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccd06bf4-eb4b-4ca0-ae9f-a9335b625d6a", "node_type": "1", "metadata": {}, "hash": "fa20b7cb9c31ae33ca98b4fd3dd336a2c74a3146a98b4a28fe5020d5a4119277", "class_name": "RelatedNodeInfo"}}, "text": "We show the document format is in Section A.2 in the appendix. We study Llama2\n7B (Touvron et al., 2023b) and our LongLoRA model which fine-tunes Llama2 7B with 32768\ncontext length. We test the passkey retrieval accuracy from 1k to 34k, with an interval of roughly 1k\n(as the sentence length can not be precisely controlled). For each document length, we test the model\n10 times with different random passkey values. Our model achieves reasonable passkey retrieval\naccuracy until 33k or 34k. Without further fine-tuning, We modify the max position embeddings to\n48k in the position interpolation, which is the Ours 7B (extended PI) in Figure 4. We show that this\nmodel can handle longer documents by simply extending the position interpolation. As the dashed\norange line in Figure 4, the model, fine-tuned on 32k context length, presents moderate retrieval\nability (60%-90% accuracy) in the range of 33k to 45k.", "start_char_idx": 2079, "end_char_idx": 2991, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ccd06bf4-eb4b-4ca0-ae9f-a9335b625d6a": {"__data__": {"id_": "ccd06bf4-eb4b-4ca0-ae9f-a9335b625d6a", "embedding": null, "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82569dcb-23d6-4577-baed-6323c76765ea", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "017169e57c627ecb68cc28d6022ea21305b3e50cb4af772a64397ee25d225a33", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd1738d3-7246-46bf-8432-f8407638bd16", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "e50a55e8f59b060ca108b72621e0ddf2a1b4f7d312f903f46eec25f6b6319985", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89c05dfe-cb96-4016-adfb-82c9fea8ee1b", "node_type": "1", "metadata": {}, "hash": "13115b55c08ba135687f2e246dde4709043175b5e02ac529b15526ee3d9cde75", "class_name": "RelatedNodeInfo"}}, "text": "We test the passkey retrieval accuracy from 1k to 34k, with an interval of roughly 1k\n(as the sentence length can not be precisely controlled). For each document length, we test the model\n10 times with different random passkey values. Our model achieves reasonable passkey retrieval\naccuracy until 33k or 34k. Without further fine-tuning, We modify the max position embeddings to\n48k in the position interpolation, which is the Ours 7B (extended PI) in Figure 4. We show that this\nmodel can handle longer documents by simply extending the position interpolation. As the dashed\norange line in Figure 4, the model, fine-tuned on 32k context length, presents moderate retrieval\nability (60%-90% accuracy) in the range of 33k to 45k. Even with the position interpolation extended,\nLlama2 7B suffers from a sharp accuracy degradation (dashed blue line) after the 4k context length.\n4.3 A BLATION STUDY\nIn this section, we introduce ablation studies on the number of fine-tuning steps and attention patterns.", "start_char_idx": 2262, "end_char_idx": 3264, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89c05dfe-cb96-4016-adfb-82c9fea8ee1b": {"__data__": {"id_": "89c05dfe-cb96-4016-adfb-82c9fea8ee1b", "embedding": null, "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82569dcb-23d6-4577-baed-6323c76765ea", "node_type": "4", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "017169e57c627ecb68cc28d6022ea21305b3e50cb4af772a64397ee25d225a33", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccd06bf4-eb4b-4ca0-ae9f-a9335b625d6a", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "f4dee5eae212e47d00bbd53a0e7b49a0a7996c5c4a8bcbe636c9e4ed577f6b92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b34a56a-227c-4d94-bcf9-697b67721093", "node_type": "1", "metadata": {}, "hash": "4addab6ab191aaacf7f34f8252ba794aa17258aeff26b8ed46233725e298571b", "class_name": "RelatedNodeInfo"}}, "text": "Our model achieves reasonable passkey retrieval\naccuracy until 33k or 34k. Without further fine-tuning, We modify the max position embeddings to\n48k in the position interpolation, which is the Ours 7B (extended PI) in Figure 4. We show that this\nmodel can handle longer documents by simply extending the position interpolation. As the dashed\norange line in Figure 4, the model, fine-tuned on 32k context length, presents moderate retrieval\nability (60%-90% accuracy) in the range of 33k to 45k. Even with the position interpolation extended,\nLlama2 7B suffers from a sharp accuracy degradation (dashed blue line) after the 4k context length.\n4.3 A BLATION STUDY\nIn this section, we introduce ablation studies on the number of fine-tuning steps and attention patterns.\nOther experimental results including ablations on group sizes, attention variants, and efficiency\nanalysis are Section B in the appendix.\nAblation on Fine-tuning Steps. We report the relationship between perplexity and fine-tuning\nsteps for a Llama2 7B model extending to the 8192 context length on the PG19 validation set, in\n8", "start_char_idx": 2497, "end_char_idx": 3593, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b34a56a-227c-4d94-bcf9-697b67721093": {"__data__": {"id_": "6b34a56a-227c-4d94-bcf9-697b67721093", "embedding": null, "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c4879cdc148df642fb2e8850b7972c139e6a3050e57ea297b07820f2ccfd7999", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89c05dfe-cb96-4016-adfb-82c9fea8ee1b", "node_type": "1", "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "b460878c54e5b1962fa6733bbc25957a67d828ac6957880fa96b194e2cd20c42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc9dfecc-eff1-4e2f-8b32-cced235246c2", "node_type": "1", "metadata": {}, "hash": "e924722a6c9dabdd4c24e8e7bfc685c1937e9b686805695578725267005434f0", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 6: Comparisons among S2-Attn and alternative attention patterns during fine-tuning . We\nadapt a Llama2 7B model to 32768 context length with different attention patterns and improved\nLoRA at training time. We include four typical efficient attention designs, e.g., shift, dilate (Ding\net al., 2023), block sparse (Qiu et al., 2020), stride sparse (Child et al., 2019) for comparison. \u2018 cro.\nheads / layers \u2019 means to swap different attention settings across attention heads or sequential layers .\nTaking S2-Attn as an example, \u2018 cro. layers \u2019 is to swap between w/ and w/o shift in sequential\nself-attention layers. \u2018 only P1/P2 \u2019 means all attention heads use pattern 1 (all no shift) or Pattern\n2 (all shift) in Figure 2. We visualize the patterns of different attention in Figure 7 in the appendix.\nFor each attention pattern, we evaluate its performance under two protocols. In the first row, we use\nsparse attention in both training and testing.", "start_char_idx": 0, "end_char_idx": 1001, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc9dfecc-eff1-4e2f-8b32-cced235246c2": {"__data__": {"id_": "dc9dfecc-eff1-4e2f-8b32-cced235246c2", "embedding": null, "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c4879cdc148df642fb2e8850b7972c139e6a3050e57ea297b07820f2ccfd7999", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b34a56a-227c-4d94-bcf9-697b67721093", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "e22cd7d332cef592cd066640125412b630bf7d64f85611b3398a8d4c0a2f3be8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b3f3a06-4d14-4dcd-821d-7fb2d00097e6", "node_type": "1", "metadata": {}, "hash": "2a889db2f927ace39dd48993fa9e2a9ed743d23399ef3ad606e8a72238eac557", "class_name": "RelatedNodeInfo"}}, "text": "We include four typical efficient attention designs, e.g., shift, dilate (Ding\net al., 2023), block sparse (Qiu et al., 2020), stride sparse (Child et al., 2019) for comparison. \u2018 cro.\nheads / layers \u2019 means to swap different attention settings across attention heads or sequential layers .\nTaking S2-Attn as an example, \u2018 cro. layers \u2019 is to swap between w/ and w/o shift in sequential\nself-attention layers. \u2018 only P1/P2 \u2019 means all attention heads use pattern 1 (all no shift) or Pattern\n2 (all shift) in Figure 2. We visualize the patterns of different attention in Figure 7 in the appendix.\nFor each attention pattern, we evaluate its performance under two protocols. In the first row, we use\nsparse attention in both training and testing. In the second row, we use full attention for testing.\nTest w/\nFull-AttnS2-Attn Dilate Block sparse Stride sparse\ncro. heads cro. layers only P1. only P2. cro. heads cro. heads cro.", "start_char_idx": 257, "end_char_idx": 1182, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b3f3a06-4d14-4dcd-821d-7fb2d00097e6": {"__data__": {"id_": "0b3f3a06-4d14-4dcd-821d-7fb2d00097e6", "embedding": null, "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c4879cdc148df642fb2e8850b7972c139e6a3050e57ea297b07820f2ccfd7999", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc9dfecc-eff1-4e2f-8b32-cced235246c2", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "71ca244837e31b3dfa82927db92b23e555ff61b2744efc958738e5a6b76ba9a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "836f43f4-551b-4bb5-8cff-34115d7bf0ef", "node_type": "1", "metadata": {}, "hash": "2a3d0fd33a36d3911e15bfc1a2529f1b73f6d37b1266214ddcb8a05f8cf1e916", "class_name": "RelatedNodeInfo"}}, "text": "\u2018 cro.\nheads / layers \u2019 means to swap different attention settings across attention heads or sequential layers .\nTaking S2-Attn as an example, \u2018 cro. layers \u2019 is to swap between w/ and w/o shift in sequential\nself-attention layers. \u2018 only P1/P2 \u2019 means all attention heads use pattern 1 (all no shift) or Pattern\n2 (all shift) in Figure 2. We visualize the patterns of different attention in Figure 7 in the appendix.\nFor each attention pattern, we evaluate its performance under two protocols. In the first row, we use\nsparse attention in both training and testing. In the second row, we use full attention for testing.\nTest w/\nFull-AttnS2-Attn Dilate Block sparse Stride sparse\ncro. heads cro. layers only P1. only P2. cro. heads cro. heads cro. heads\n\u2717 8.64 8.63 9.17 9.64 8.75 11.49 32.81\n\u2713 8.12 9.70 8.39 9.81 11.78 8.30 24.03\nFigure 5.", "start_char_idx": 435, "end_char_idx": 1276, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "836f43f4-551b-4bb5-8cff-34115d7bf0ef": {"__data__": {"id_": "836f43f4-551b-4bb5-8cff-34115d7bf0ef", "embedding": null, "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c4879cdc148df642fb2e8850b7972c139e6a3050e57ea297b07820f2ccfd7999", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b3f3a06-4d14-4dcd-821d-7fb2d00097e6", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "3f5162d6d79cbea27166bfffc6891864680f2bb64d324ba430fe9af20b656b07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7813656b-3df4-4491-bdf9-ac57c909e464", "node_type": "1", "metadata": {}, "hash": "f7d456636c65fc15db610d3b537d7baca718248f5e771a9d3650a4c841f300d8", "class_name": "RelatedNodeInfo"}}, "text": "We visualize the patterns of different attention in Figure 7 in the appendix.\nFor each attention pattern, we evaluate its performance under two protocols. In the first row, we use\nsparse attention in both training and testing. In the second row, we use full attention for testing.\nTest w/\nFull-AttnS2-Attn Dilate Block sparse Stride sparse\ncro. heads cro. layers only P1. only P2. cro. heads cro. heads cro. heads\n\u2717 8.64 8.63 9.17 9.64 8.75 11.49 32.81\n\u2713 8.12 9.70 8.39 9.81 11.78 8.30 24.03\nFigure 5. We see that without fine-tuning, at step 0, the model has a limited long context capability,\ne.g., 15.82 perplexity. We show that the perplexity drops quickly. Full fine-tuning converges faster\nthan low-rank training. They come closer after 200 steps, without a large gap at the end.\nAttention Patterns.", "start_char_idx": 775, "end_char_idx": 1580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7813656b-3df4-4491-bdf9-ac57c909e464": {"__data__": {"id_": "7813656b-3df4-4491-bdf9-ac57c909e464", "embedding": null, "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c4879cdc148df642fb2e8850b7972c139e6a3050e57ea297b07820f2ccfd7999", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "836f43f4-551b-4bb5-8cff-34115d7bf0ef", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "b31394bda3b213081553d3dfcb777e68e4eb347852f03456e3873159ef9f6aee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1d36aa7-e929-4a23-9d01-012fa2492d00", "node_type": "1", "metadata": {}, "hash": "bc9d856b0046a36f3ab31efdf728d2cbac4702a204f8eb0967b875bb98033aa2", "class_name": "RelatedNodeInfo"}}, "text": "Test w/\nFull-AttnS2-Attn Dilate Block sparse Stride sparse\ncro. heads cro. layers only P1. only P2. cro. heads cro. heads cro. heads\n\u2717 8.64 8.63 9.17 9.64 8.75 11.49 32.81\n\u2713 8.12 9.70 8.39 9.81 11.78 8.30 24.03\nFigure 5. We see that without fine-tuning, at step 0, the model has a limited long context capability,\ne.g., 15.82 perplexity. We show that the perplexity drops quickly. Full fine-tuning converges faster\nthan low-rank training. They come closer after 200 steps, without a large gap at the end.\nAttention Patterns. In Table 6, we show the effects of different attention patterns during fine-\ntuning.", "start_char_idx": 1056, "end_char_idx": 1665, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1d36aa7-e929-4a23-9d01-012fa2492d00": {"__data__": {"id_": "c1d36aa7-e929-4a23-9d01-012fa2492d00", "embedding": null, "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c4879cdc148df642fb2e8850b7972c139e6a3050e57ea297b07820f2ccfd7999", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7813656b-3df4-4491-bdf9-ac57c909e464", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4b77e53ab8c15f8dd3b8c9d101d80583f6fa66d4ec76caef48598e74042a3928", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e121a8c4-2990-4a29-9807-184d42608a7e", "node_type": "1", "metadata": {}, "hash": "ef66d309225bb700c62f3b34d84da2f14025a6788ac6c8c0451e07e0eb7ee2d9", "class_name": "RelatedNodeInfo"}}, "text": "heads cro. layers only P1. only P2. cro. heads cro. heads cro. heads\n\u2717 8.64 8.63 9.17 9.64 8.75 11.49 32.81\n\u2713 8.12 9.70 8.39 9.81 11.78 8.30 24.03\nFigure 5. We see that without fine-tuning, at step 0, the model has a limited long context capability,\ne.g., 15.82 perplexity. We show that the perplexity drops quickly. Full fine-tuning converges faster\nthan low-rank training. They come closer after 200 steps, without a large gap at the end.\nAttention Patterns. In Table 6, we show the effects of different attention patterns during fine-\ntuning. We fine-tune a Llama2 7B (Touvron et al., 2023b) model to 32768 context length on\nRedpajama (Computer, 2023) datasets and evaluate the perplexity on PG19 (Rae et al., 2020)\nvalidation set.", "start_char_idx": 1120, "end_char_idx": 1854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e121a8c4-2990-4a29-9807-184d42608a7e": {"__data__": {"id_": "e121a8c4-2990-4a29-9807-184d42608a7e", "embedding": null, "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c4879cdc148df642fb2e8850b7972c139e6a3050e57ea297b07820f2ccfd7999", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1d36aa7-e929-4a23-9d01-012fa2492d00", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a1727f64079e0962e098649096541a43caf9da38a3e41d79feb048f0fd9974f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0bc6f66d-2959-4dd7-a7ff-dbbb22d4eb17", "node_type": "1", "metadata": {}, "hash": "a787963de8b284d93335f737eccf5232c28375a80ee98671d4f38a25704b2b90", "class_name": "RelatedNodeInfo"}}, "text": "We see that without fine-tuning, at step 0, the model has a limited long context capability,\ne.g., 15.82 perplexity. We show that the perplexity drops quickly. Full fine-tuning converges faster\nthan low-rank training. They come closer after 200 steps, without a large gap at the end.\nAttention Patterns. In Table 6, we show the effects of different attention patterns during fine-\ntuning. We fine-tune a Llama2 7B (Touvron et al., 2023b) model to 32768 context length on\nRedpajama (Computer, 2023) datasets and evaluate the perplexity on PG19 (Rae et al., 2020)\nvalidation set. We first examine the manner of swapping among various settings. For the shift\noperation we used in LongLoRA, there are three choices: disabling it, shifting between sequential\nlayers, and shifting among attention heads. We show that shifting between layers is acceptable but not\nthe best. In addition, setting all attention heads as pattern 1 or pattern 2 does not work. In addition,\nwe empirically find that shifting left or right has little difference in performance.", "start_char_idx": 1277, "end_char_idx": 2324, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0bc6f66d-2959-4dd7-a7ff-dbbb22d4eb17": {"__data__": {"id_": "0bc6f66d-2959-4dd7-a7ff-dbbb22d4eb17", "embedding": null, "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c4879cdc148df642fb2e8850b7972c139e6a3050e57ea297b07820f2ccfd7999", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e121a8c4-2990-4a29-9807-184d42608a7e", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c60611c085ac945d096fbdaad663eb12d87b08fa5e5c2abbe8dbec90fd6b5a44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abdb4513-33cb-40a9-9228-81a8e59980ac", "node_type": "1", "metadata": {}, "hash": "ec52ae2aa168414c00e23ed07c0db3a8af01f0a5aea3e2031fe4db87b3ecfd20", "class_name": "RelatedNodeInfo"}}, "text": "Attention Patterns. In Table 6, we show the effects of different attention patterns during fine-\ntuning. We fine-tune a Llama2 7B (Touvron et al., 2023b) model to 32768 context length on\nRedpajama (Computer, 2023) datasets and evaluate the perplexity on PG19 (Rae et al., 2020)\nvalidation set. We first examine the manner of swapping among various settings. For the shift\noperation we used in LongLoRA, there are three choices: disabling it, shifting between sequential\nlayers, and shifting among attention heads. We show that shifting between layers is acceptable but not\nthe best. In addition, setting all attention heads as pattern 1 or pattern 2 does not work. In addition,\nwe empirically find that shifting left or right has little difference in performance.\nWe then test other types of efficient attention designs, including dilated attention (Ding et al., 2023),\nblock sparse attention (Qiu et al., 2020), and stride sparse attention (Child et al., 2019).", "start_char_idx": 1561, "end_char_idx": 2523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abdb4513-33cb-40a9-9228-81a8e59980ac": {"__data__": {"id_": "abdb4513-33cb-40a9-9228-81a8e59980ac", "embedding": null, "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c4879cdc148df642fb2e8850b7972c139e6a3050e57ea297b07820f2ccfd7999", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0bc6f66d-2959-4dd7-a7ff-dbbb22d4eb17", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "31f5d32c48bb1a5816072b76e9dfa6e89f06a29ed14d988d80b43a4f7e9b187f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "647e1f2c-ba7c-48d7-b0d4-433ccb43fbdd", "node_type": "1", "metadata": {}, "hash": "b0931d6b2b7ae1f36d1c81d876529c2bf91d9961e78d5e0c0af6cb2c3e25ff0d", "class_name": "RelatedNodeInfo"}}, "text": "We first examine the manner of swapping among various settings. For the shift\noperation we used in LongLoRA, there are three choices: disabling it, shifting between sequential\nlayers, and shifting among attention heads. We show that shifting between layers is acceptable but not\nthe best. In addition, setting all attention heads as pattern 1 or pattern 2 does not work. In addition,\nwe empirically find that shifting left or right has little difference in performance.\nWe then test other types of efficient attention designs, including dilated attention (Ding et al., 2023),\nblock sparse attention (Qiu et al., 2020), and stride sparse attention (Child et al., 2019). For dilated\nattention (Ding et al., 2023), we vary the dilate rate from 1 to 2 evenly among attention heads. For\nblock sparse attention (Qiu et al., 2020), we use n= 4 block-wise masking matrices in attention\nheads and move the block left to make it causal. Stride sparse attention (Child et al., 2019) contains\nboth local and stride patterns. These settings share similar computational costs.", "start_char_idx": 1855, "end_char_idx": 2917, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "647e1f2c-ba7c-48d7-b0d4-433ccb43fbdd": {"__data__": {"id_": "647e1f2c-ba7c-48d7-b0d4-433ccb43fbdd", "embedding": null, "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c4879cdc148df642fb2e8850b7972c139e6a3050e57ea297b07820f2ccfd7999", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abdb4513-33cb-40a9-9228-81a8e59980ac", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "83d2dcfb193d288ce659547a0d3180d073e925fe33c902196de607f996e1fcdd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6115c24-2088-48a8-92b5-6af7164c0388", "node_type": "1", "metadata": {}, "hash": "0c38ecbe75de7a55dce197a8a23d98517ff49d7c221593911c2cc989d40cd7c4", "class_name": "RelatedNodeInfo"}}, "text": "In addition, setting all attention heads as pattern 1 or pattern 2 does not work. In addition,\nwe empirically find that shifting left or right has little difference in performance.\nWe then test other types of efficient attention designs, including dilated attention (Ding et al., 2023),\nblock sparse attention (Qiu et al., 2020), and stride sparse attention (Child et al., 2019). For dilated\nattention (Ding et al., 2023), we vary the dilate rate from 1 to 2 evenly among attention heads. For\nblock sparse attention (Qiu et al., 2020), we use n= 4 block-wise masking matrices in attention\nheads and move the block left to make it causal. Stride sparse attention (Child et al., 2019) contains\nboth local and stride patterns. These settings share similar computational costs. We visualize\nthese patterns in Figure 7 in the appendix. These attention patterns are invented in training-from-\nscratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained\nLLMs (Touvron et al., 2023b), toward long context adaptation.", "start_char_idx": 2144, "end_char_idx": 3191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6115c24-2088-48a8-92b5-6af7164c0388": {"__data__": {"id_": "f6115c24-2088-48a8-92b5-6af7164c0388", "embedding": null, "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c4879cdc148df642fb2e8850b7972c139e6a3050e57ea297b07820f2ccfd7999", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "647e1f2c-ba7c-48d7-b0d4-433ccb43fbdd", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "165fc01862293dc33284b14f1c1d1f22ba6be7d1fdd19b50268903eb5a1814f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ad0faae-8a59-4ba0-9243-7958eb21e5d9", "node_type": "1", "metadata": {}, "hash": "5460b35e6879cec92811e6307b93b1fca6b708718d50e0c6c768c968b0f93725", "class_name": "RelatedNodeInfo"}}, "text": "For dilated\nattention (Ding et al., 2023), we vary the dilate rate from 1 to 2 evenly among attention heads. For\nblock sparse attention (Qiu et al., 2020), we use n= 4 block-wise masking matrices in attention\nheads and move the block left to make it causal. Stride sparse attention (Child et al., 2019) contains\nboth local and stride patterns. These settings share similar computational costs. We visualize\nthese patterns in Figure 7 in the appendix. These attention patterns are invented in training-from-\nscratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained\nLLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in\nfull fine-tuning but is not well with low-rank adaptation. Fine-tuning with stride sparse attention is\nharmful. They have a large gap to full attention, which is applied in the pre-training stage.\n5 C ONCLUSION\nIn this work, we propose LongLoRA that can efficiently extend the context length of LLMs to be\nsignificantly larger.", "start_char_idx": 2524, "end_char_idx": 3554, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ad0faae-8a59-4ba0-9243-7958eb21e5d9": {"__data__": {"id_": "5ad0faae-8a59-4ba0-9243-7958eb21e5d9", "embedding": null, "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c4879cdc148df642fb2e8850b7972c139e6a3050e57ea297b07820f2ccfd7999", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6115c24-2088-48a8-92b5-6af7164c0388", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "2dcad4510a7bcb540c8f3ab5ac7b4dfc0714a8c9915833b2b797954d951e3fb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5344355-c807-4bf5-b930-9d279d987236", "node_type": "1", "metadata": {}, "hash": "e0574c5f65275d6f00316515248e4a05edcbd8fb34d33b5140e0caaf390aa2ad", "class_name": "RelatedNodeInfo"}}, "text": "Stride sparse attention (Child et al., 2019) contains\nboth local and stride patterns. These settings share similar computational costs. We visualize\nthese patterns in Figure 7 in the appendix. These attention patterns are invented in training-from-\nscratch transformers. This experiment is to examine their capability of fine-tuning on pre-trained\nLLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in\nfull fine-tuning but is not well with low-rank adaptation. Fine-tuning with stride sparse attention is\nharmful. They have a large gap to full attention, which is applied in the pre-training stage.\n5 C ONCLUSION\nIn this work, we propose LongLoRA that can efficiently extend the context length of LLMs to be\nsignificantly larger. LongLoRA has less GPU memory cost and training time than standard full\nfine-tuning, with minimal accuracy compromise. At the architecture level, we propose S2-Attn\nto approximate the standard self-attention pattern during training. S2-Attn is easy to implement,\nrequiring only two lines of code.", "start_char_idx": 2782, "end_char_idx": 3850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5344355-c807-4bf5-b930-9d279d987236": {"__data__": {"id_": "d5344355-c807-4bf5-b930-9d279d987236", "embedding": null, "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c4879cdc148df642fb2e8850b7972c139e6a3050e57ea297b07820f2ccfd7999", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ad0faae-8a59-4ba0-9243-7958eb21e5d9", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "e7c7279fa19acc7c9a69e36ac0639f4f1f9457e400c89110bc7d06cc7ac9d0fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fdfe646-5a52-4849-a24a-02257c53f74a", "node_type": "1", "metadata": {}, "hash": "d310c1ff4eccd4d210bce61c641d221891cf487784d93cc41f98d8de1839622c", "class_name": "RelatedNodeInfo"}}, "text": "This experiment is to examine their capability of fine-tuning on pre-trained\nLLMs (Touvron et al., 2023b), toward long context adaptation. Dilated attention performs well in\nfull fine-tuning but is not well with low-rank adaptation. Fine-tuning with stride sparse attention is\nharmful. They have a large gap to full attention, which is applied in the pre-training stage.\n5 C ONCLUSION\nIn this work, we propose LongLoRA that can efficiently extend the context length of LLMs to be\nsignificantly larger. LongLoRA has less GPU memory cost and training time than standard full\nfine-tuning, with minimal accuracy compromise. At the architecture level, we propose S2-Attn\nto approximate the standard self-attention pattern during training. S2-Attn is easy to implement,\nrequiring only two lines of code. Moreover, models trained via S2-Attn retain the original standard\nattention architecture during inference, making most pre-existing infrastructure and optimization\nreusable. At the training level, we bridge the gap between LoRA and full fine-tuning with trainable\nnormalization and embedding.", "start_char_idx": 3053, "end_char_idx": 4143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fdfe646-5a52-4849-a24a-02257c53f74a": {"__data__": {"id_": "5fdfe646-5a52-4849-a24a-02257c53f74a", "embedding": null, "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c4879cdc148df642fb2e8850b7972c139e6a3050e57ea297b07820f2ccfd7999", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5344355-c807-4bf5-b930-9d279d987236", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "d5b5b81043ee13251b73c9fa9f18b6477365e26ce76f5be77906e0d2d703b333", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63d9a34c-5359-4681-9f9c-35ab18e34f66", "node_type": "1", "metadata": {}, "hash": "90d48fff08cee6064457af14e904fafb901930e269549212f65d018bf37397de", "class_name": "RelatedNodeInfo"}}, "text": "Fine-tuning with stride sparse attention is\nharmful. They have a large gap to full attention, which is applied in the pre-training stage.\n5 C ONCLUSION\nIn this work, we propose LongLoRA that can efficiently extend the context length of LLMs to be\nsignificantly larger. LongLoRA has less GPU memory cost and training time than standard full\nfine-tuning, with minimal accuracy compromise. At the architecture level, we propose S2-Attn\nto approximate the standard self-attention pattern during training. S2-Attn is easy to implement,\nrequiring only two lines of code. Moreover, models trained via S2-Attn retain the original standard\nattention architecture during inference, making most pre-existing infrastructure and optimization\nreusable. At the training level, we bridge the gap between LoRA and full fine-tuning with trainable\nnormalization and embedding. Our method can extend Llama2 7B to 100k context length and 70B\nmodel to 32k context length, on a single 8 \u00d7A100 machine. We also present a long instruction-\nfollowing dataset, LongAlpaca and conducted supervised fine-tuning with LongLoRA.", "start_char_idx": 3286, "end_char_idx": 4382, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63d9a34c-5359-4681-9f9c-35ab18e34f66": {"__data__": {"id_": "63d9a34c-5359-4681-9f9c-35ab18e34f66", "embedding": null, "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18ff9d18-48df-4362-bc4b-738fa5157cb1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c4879cdc148df642fb2e8850b7972c139e6a3050e57ea297b07820f2ccfd7999", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fdfe646-5a52-4849-a24a-02257c53f74a", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "266b362516909b99ed12878223271dce655a228210887d5ba5d9da8844ccc580", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04d7f7e6-d8df-4114-acd0-c50b96066c62", "node_type": "1", "metadata": {}, "hash": "6e00a0d556fdb1786a35556a54b092868ea24b4291545678599c4a3b0d139730", "class_name": "RelatedNodeInfo"}}, "text": "LongLoRA has less GPU memory cost and training time than standard full\nfine-tuning, with minimal accuracy compromise. At the architecture level, we propose S2-Attn\nto approximate the standard self-attention pattern during training. S2-Attn is easy to implement,\nrequiring only two lines of code. Moreover, models trained via S2-Attn retain the original standard\nattention architecture during inference, making most pre-existing infrastructure and optimization\nreusable. At the training level, we bridge the gap between LoRA and full fine-tuning with trainable\nnormalization and embedding. Our method can extend Llama2 7B to 100k context length and 70B\nmodel to 32k context length, on a single 8 \u00d7A100 machine. We also present a long instruction-\nfollowing dataset, LongAlpaca and conducted supervised fine-tuning with LongLoRA. We believe\nthat LongLoRA is a general method that could be compatible with more types of LLMs and position\nencodings. We plan to investigate these in future work.\nAcknowledgement We would like to thank Xiuyu Li and Bohao Peng for the helpful discussions.\n9", "start_char_idx": 3555, "end_char_idx": 4639, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04d7f7e6-d8df-4114-acd0-c50b96066c62": {"__data__": {"id_": "04d7f7e6-d8df-4114-acd0-c50b96066c62", "embedding": null, "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6f4102705c6161304466e1b9c91538e5af3a661097f9e5f47082186b09e98061", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63d9a34c-5359-4681-9f9c-35ab18e34f66", "node_type": "1", "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "75552c277c3e1e9e3499793faf25018448790e6853e34f84e1dd4116a1c57f06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ebcaa7f-a71c-4581-9fa5-bde810bbb3aa", "node_type": "1", "metadata": {}, "hash": "4ee62489d9f35536587e3512fd962db50bc39474d30233b9d3e5c66518951e39", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nREFERENCES\nNtk-aware scaled rope, 2023. URL https://www.reddit.com/r/LocalLLaMA/\ncomments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_\nhave/ .\nByeongjoo Ahn, Michael DeZeeuw, Ioannis Gkioulekas, and Aswin C. Sankaranarayanan. Neural\nkaleidoscopic space sculpting. In CVPR, pp. 4349\u20134358, 2023.\nChenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.\nL-eval: Instituting standardized evaluation for long context language models, 2023.\nShengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, and\nJian-Guang Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. CoRR ,\nabs/2203.03131, 2022.", "start_char_idx": 0, "end_char_idx": 730, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ebcaa7f-a71c-4581-9fa5-bde810bbb3aa": {"__data__": {"id_": "2ebcaa7f-a71c-4581-9fa5-bde810bbb3aa", "embedding": null, "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6f4102705c6161304466e1b9c91538e5af3a661097f9e5f47082186b09e98061", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04d7f7e6-d8df-4114-acd0-c50b96066c62", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "2be60053a2d248ad0a60b03a1e0304b1bd4c8d05efc726e8edf6bf7f3542c3f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c984d430-e376-4720-93fb-30b330251bba", "node_type": "1", "metadata": {}, "hash": "83b038c2f3c2d990cadf65483fdc6fd8e60ca9384208d4e664c65e9dce9a2195", "class_name": "RelatedNodeInfo"}}, "text": "Byeongjoo Ahn, Michael DeZeeuw, Ioannis Gkioulekas, and Aswin C. Sankaranarayanan. Neural\nkaleidoscopic space sculpting. In CVPR, pp. 4349\u20134358, 2023.\nChenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.\nL-eval: Instituting standardized evaluation for long context language models, 2023.\nShengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, and\nJian-Guang Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. CoRR ,\nabs/2203.03131, 2022.\nZhangir Azerbayev, Edward Ayers, and Bartosz Piotrowski. Proof-pile, 2022. URL https:\n//github.com/zhangir-azerbayev/proof-pile .\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization.", "start_char_idx": 196, "end_char_idx": 937, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c984d430-e376-4720-93fb-30b330251bba": {"__data__": {"id_": "c984d430-e376-4720-93fb-30b330251bba", "embedding": null, "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6f4102705c6161304466e1b9c91538e5af3a661097f9e5f47082186b09e98061", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ebcaa7f-a71c-4581-9fa5-bde810bbb3aa", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "9d0a4976530121db07ba05f3fd51c8dc643f20b921f8a647845ae311c70edf06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acab9c82-c85d-486b-a13d-0198006b8995", "node_type": "1", "metadata": {}, "hash": "1da737aab0c42b6a85db190977112975e651ef948337e52e1e1230fae713d663", "class_name": "RelatedNodeInfo"}}, "text": "Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.\nL-eval: Instituting standardized evaluation for long context language models, 2023.\nShengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, and\nJian-Guang Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. CoRR ,\nabs/2203.03131, 2022.\nZhangir Azerbayev, Edward Ayers, and Bartosz Piotrowski. Proof-pile, 2022. URL https:\n//github.com/zhangir-azerbayev/proof-pile .\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR ,\nabs/1607.06450, 2016.", "start_char_idx": 347, "end_char_idx": 966, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acab9c82-c85d-486b-a13d-0198006b8995": {"__data__": {"id_": "acab9c82-c85d-486b-a13d-0198006b8995", "embedding": null, "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6f4102705c6161304466e1b9c91538e5af3a661097f9e5f47082186b09e98061", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c984d430-e376-4720-93fb-30b330251bba", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "8cc9bda14c08614b3a07e585b759c0019a782b57283dd2c5f7b760b00a4b8cf1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0617e13-734e-47ae-970d-c4a9a26223ac", "node_type": "1", "metadata": {}, "hash": "2536dd723a8fd157fcfa7c5e706533c64ce4f11110d3e405cc8114d7c5bc939e", "class_name": "RelatedNodeInfo"}}, "text": "L-eval: Instituting standardized evaluation for long context language models, 2023.\nShengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, and\nJian-Guang Lou. Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. CoRR ,\nabs/2203.03131, 2022.\nZhangir Azerbayev, Edward Ayers, and Bartosz Piotrowski. Proof-pile, 2022. URL https:\n//github.com/zhangir-azerbayev/proof-pile .\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR ,\nabs/1607.06450, 2016.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,\nXiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual,\nmultitask benchmark for long context understanding.", "start_char_idx": 437, "end_char_idx": 1202, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0617e13-734e-47ae-970d-c4a9a26223ac": {"__data__": {"id_": "c0617e13-734e-47ae-970d-c4a9a26223ac", "embedding": null, "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6f4102705c6161304466e1b9c91538e5af3a661097f9e5f47082186b09e98061", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acab9c82-c85d-486b-a13d-0198006b8995", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "f18eb92c120aaa9ea80eddef92e111454c32c543ebaac1d630de363d00badde8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0470b26-6bee-4172-9cfe-3097f5dacc6e", "node_type": "1", "metadata": {}, "hash": "25b686ab153b0bf7aac56144258a4ab6672f3d780c85c0d3ef5e79e946e3ce68", "class_name": "RelatedNodeInfo"}}, "text": "Input-tuning: Adapting unfamiliar inputs to frozen pretrained models. CoRR ,\nabs/2203.03131, 2022.\nZhangir Azerbayev, Edward Ayers, and Bartosz Piotrowski. Proof-pile, 2022. URL https:\n//github.com/zhangir-azerbayev/proof-pile .\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR ,\nabs/1607.06450, 2016.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,\nXiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual,\nmultitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.\nCoRR, abs/2004.05150, 2020.", "start_char_idx": 632, "end_char_idx": 1360, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0470b26-6bee-4172-9cfe-3097f5dacc6e": {"__data__": {"id_": "d0470b26-6bee-4172-9cfe-3097f5dacc6e", "embedding": null, "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6f4102705c6161304466e1b9c91538e5af3a661097f9e5f47082186b09e98061", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0617e13-734e-47ae-970d-c4a9a26223ac", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "3695125021a6a47d7603e66adc229c361a1eede73aba08dba6e3d395da29594e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99fb4455-e3aa-46b5-ad00-69ac772d0f3c", "node_type": "1", "metadata": {}, "hash": "7bad6d0f82cb5350401c806ec9181d4bdbadf2af529f52fbbdc871450fdd6385", "class_name": "RelatedNodeInfo"}}, "text": "Proof-pile, 2022. URL https:\n//github.com/zhangir-azerbayev/proof-pile .\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR ,\nabs/1607.06450, 2016.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,\nXiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual,\nmultitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.\nCoRR, abs/2004.05150, 2020.\nAydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Recurrent memory transformer. In NeurIPS ,\n2022.", "start_char_idx": 788, "end_char_idx": 1462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99fb4455-e3aa-46b5-ad00-69ac772d0f3c": {"__data__": {"id_": "99fb4455-e3aa-46b5-ad00-69ac772d0f3c", "embedding": null, "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6f4102705c6161304466e1b9c91538e5af3a661097f9e5f47082186b09e98061", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0470b26-6bee-4172-9cfe-3097f5dacc6e", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "098010ce55a94328c9b2cd09450d6d1ea040f3a9e1e20fd499a79c6aa5f662e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "746269ca-7e0e-4c59-8d14-3b0e79ea2427", "node_type": "1", "metadata": {}, "hash": "9adc41eb5a21a171c5ddd07893368174a8e9eacf9a26322b86aaec9c7a7d71f3", "class_name": "RelatedNodeInfo"}}, "text": "Layer normalization. CoRR ,\nabs/1607.06450, 2016.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,\nXiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual,\nmultitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.\nCoRR, abs/2004.05150, 2020.\nAydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Recurrent memory transformer. In NeurIPS ,\n2022.\nBeidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher R \u00b4e.\nPixelated butterfly: Simple and efficient sparse training for neural network models. In ICLR , 2022.", "start_char_idx": 917, "end_char_idx": 1658, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "746269ca-7e0e-4c59-8d14-3b0e79ea2427": {"__data__": {"id_": "746269ca-7e0e-4c59-8d14-3b0e79ea2427", "embedding": null, "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6f4102705c6161304466e1b9c91538e5af3a661097f9e5f47082186b09e98061", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99fb4455-e3aa-46b5-ad00-69ac772d0f3c", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "e07ca809a5980c2a83b862207846e52990ef17cf81f27d4fd75b0f8e2240f2e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c8a50ed-b952-4be8-9d5c-b7068eb79d15", "node_type": "1", "metadata": {}, "hash": "bf5f07893f533c2ce60164555a6312de4d004fea079ccb114bb69b192670701f", "class_name": "RelatedNodeInfo"}}, "text": "Longbench: A bilingual,\nmultitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.\nCoRR, abs/2004.05150, 2020.\nAydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Recurrent memory transformer. In NeurIPS ,\n2022.\nBeidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher R \u00b4e.\nPixelated butterfly: Simple and efficient sparse training for neural network models. In ICLR , 2022.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of\nlarge language models via positional interpolation. CoRR, abs/2306.15595, 2023.", "start_char_idx": 1127, "end_char_idx": 1830, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c8a50ed-b952-4be8-9d5c-b7068eb79d15": {"__data__": {"id_": "5c8a50ed-b952-4be8-9d5c-b7068eb79d15", "embedding": null, "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6f4102705c6161304466e1b9c91538e5af3a661097f9e5f47082186b09e98061", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "746269ca-7e0e-4c59-8d14-3b0e79ea2427", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "5a4f0d5ca58ab6e49f535872b32b92bb5f6f98f4b05bf538365c4fdd1033f9e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1de9859f-5d33-495c-9ff5-ce4279c1dcb1", "node_type": "1", "metadata": {}, "hash": "e9e3e9bbe5d44b0f6583406524c7d3d1176f7e772f109ae552b48f02985f24c2", "class_name": "RelatedNodeInfo"}}, "text": "Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.\nCoRR, abs/2004.05150, 2020.\nAydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Recurrent memory transformer. In NeurIPS ,\n2022.\nBeidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher R \u00b4e.\nPixelated butterfly: Simple and efficient sparse training for neural network models. In ICLR , 2022.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of\nlarge language models via positional interpolation. CoRR, abs/2306.15595, 2023.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.", "start_char_idx": 1242, "end_char_idx": 1998, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1de9859f-5d33-495c-9ff5-ce4279c1dcb1": {"__data__": {"id_": "1de9859f-5d33-495c-9ff5-ce4279c1dcb1", "embedding": null, "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6f4102705c6161304466e1b9c91538e5af3a661097f9e5f47082186b09e98061", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c8a50ed-b952-4be8-9d5c-b7068eb79d15", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "724e666667346913b1c85d8c89cc255348b7621ccd58fb30a619cc83fe2c5946", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aca874eb-0210-4f88-abc9-4a489da741c5", "node_type": "1", "metadata": {}, "hash": "49b67690242c19315f4d8897e10bf73333525a3425c7b21b06779787389cc29c", "class_name": "RelatedNodeInfo"}}, "text": "Recurrent memory transformer. In NeurIPS ,\n2022.\nBeidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher R \u00b4e.\nPixelated butterfly: Simple and efficient sparse training for neural network models. In ICLR , 2022.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of\nlarge language models via positional interpolation. CoRR, abs/2306.15595, 2023.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n//lmsys.org/blog/2023-03-30-vicuna/ .\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.", "start_char_idx": 1414, "end_char_idx": 2193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aca874eb-0210-4f88-abc9-4a489da741c5": {"__data__": {"id_": "aca874eb-0210-4f88-abc9-4a489da741c5", "embedding": null, "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6f4102705c6161304466e1b9c91538e5af3a661097f9e5f47082186b09e98061", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1de9859f-5d33-495c-9ff5-ce4279c1dcb1", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "79b99f6609753e342b4ab3215c80895a95d479ba17a291e52cbdd1845d8920be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35a5af8e-e596-461f-982c-4d2312a039a1", "node_type": "1", "metadata": {}, "hash": "5696796b352edfd31bbef45b3ac3d1a8bab16fe1068cce3fb5db208e6abaaea2", "class_name": "RelatedNodeInfo"}}, "text": "In ICLR , 2022.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of\nlarge language models via positional interpolation. CoRR, abs/2306.15595, 2023.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n//lmsys.org/blog/2023-03-30-vicuna/ .\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. CoRR, abs/1904.10509, 2019.\nTogether Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\nURLhttps://github.com/togethercomputer/RedPajama-Data .\nTri Dao.", "start_char_idx": 1643, "end_char_idx": 2433, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35a5af8e-e596-461f-982c-4d2312a039a1": {"__data__": {"id_": "35a5af8e-e596-461f-982c-4d2312a039a1", "embedding": null, "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6f4102705c6161304466e1b9c91538e5af3a661097f9e5f47082186b09e98061", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aca874eb-0210-4f88-abc9-4a489da741c5", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "78f2cffed1a3fbd0b4a099162787c533481790d80cb45c83172a6ea0b3ece3a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36bba7f4-2410-430d-ba22-7c76bdc3f638", "node_type": "1", "metadata": {}, "hash": "976cb02abf00979fef4b9d7ea45eade44e3fe05e8815cb4a1657d44d4b48cbd5", "class_name": "RelatedNodeInfo"}}, "text": "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n//lmsys.org/blog/2023-03-30-vicuna/ .\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. CoRR, abs/1904.10509, 2019.\nTogether Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\nURLhttps://github.com/togethercomputer/RedPajama-Data .\nTri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR ,\nabs/2307.08691, 2023.\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R \u00b4e.", "start_char_idx": 1831, "end_char_idx": 2617, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36bba7f4-2410-430d-ba22-7c76bdc3f638": {"__data__": {"id_": "36bba7f4-2410-430d-ba22-7c76bdc3f638", "embedding": null, "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6f4102705c6161304466e1b9c91538e5af3a661097f9e5f47082186b09e98061", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35a5af8e-e596-461f-982c-4d2312a039a1", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "7d6d0c9203a5154f32bf7fdb05f0b36cd75e8acf24e71aa3af69994573f5b26f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6751e9e-ef5c-46f6-8461-306bd2961104", "node_type": "1", "metadata": {}, "hash": "4e4d62765311163ede633ba881f834d406c5fe68f838349b5aa902e8189bd0b1", "class_name": "RelatedNodeInfo"}}, "text": "Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n//lmsys.org/blog/2023-03-30-vicuna/ .\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. CoRR, abs/1904.10509, 2019.\nTogether Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\nURLhttps://github.com/togethercomputer/RedPajama-Data .\nTri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR ,\nabs/2307.08691, 2023.\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R \u00b4e. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness. In NeurIPS, 2022.\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng,\nand Furu Wei.", "start_char_idx": 1999, "end_char_idx": 2818, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6751e9e-ef5c-46f6-8461-306bd2961104": {"__data__": {"id_": "f6751e9e-ef5c-46f6-8461-306bd2961104", "embedding": null, "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6f4102705c6161304466e1b9c91538e5af3a661097f9e5f47082186b09e98061", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36bba7f4-2410-430d-ba22-7c76bdc3f638", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "ee2b31048b442873e72bab2c12847c9ed0944f64ef5b6575b4252b9e65897d4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68f95953-57bf-4dfd-80f5-07998e1417a1", "node_type": "1", "metadata": {}, "hash": "36645fb4a41040b3c4c45eaf7f26e37dd782d7d5a47de2d6194b25fe29590786", "class_name": "RelatedNodeInfo"}}, "text": "Generating long sequences with sparse\ntransformers. CoRR, abs/1904.10509, 2019.\nTogether Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\nURLhttps://github.com/togethercomputer/RedPajama-Data .\nTri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR ,\nabs/2307.08691, 2023.\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R \u00b4e. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness. In NeurIPS, 2022.\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng,\nand Furu Wei. Longnet: Scaling transformers to 1, 000, 000, 000 tokens. CoRR , abs/2307.02486,\n2023.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.", "start_char_idx": 2194, "end_char_idx": 2993, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68f95953-57bf-4dfd-80f5-07998e1417a1": {"__data__": {"id_": "68f95953-57bf-4dfd-80f5-07998e1417a1", "embedding": null, "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6f4102705c6161304466e1b9c91538e5af3a661097f9e5f47082186b09e98061", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6751e9e-ef5c-46f6-8461-306bd2961104", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "64aa9fef1dee392f8225c1fafa269a32d854cea2091b63bcd7b2c7c0b90225c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4005cc8-bbe0-49d9-91ef-768a68541133", "node_type": "1", "metadata": {}, "hash": "09593bc174443ca14b76ffdbfe416b99bfe9c7a80c36926635974f81625c1a7b", "class_name": "RelatedNodeInfo"}}, "text": "Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR ,\nabs/2307.08691, 2023.\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R \u00b4e. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness. In NeurIPS, 2022.\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng,\nand Furu Wei. Longnet: Scaling transformers to 1, 000, 000, 000 tokens. CoRR , abs/2307.02486,\n2023.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm:\nGeneral language model pretraining with autoregressive blank infilling. In ACL , pp. 320\u2013335,\n2022.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.", "start_char_idx": 2425, "end_char_idx": 3171, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4005cc8-bbe0-49d9-91ef-768a68541133": {"__data__": {"id_": "f4005cc8-bbe0-49d9-91ef-768a68541133", "embedding": null, "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cc729e7a-34e8-45f6-abd5-d3b10024dcad", "node_type": "4", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6f4102705c6161304466e1b9c91538e5af3a661097f9e5f47082186b09e98061", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68f95953-57bf-4dfd-80f5-07998e1417a1", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "44556ac9c4e92e0f3c12da139c1242be74c137d26d00f7aae8d91eef02cf6a95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43f50844-054b-492f-942d-acb3350fbefc", "node_type": "1", "metadata": {}, "hash": "94a79c61c68cd321ba0561acf82a6984f4d1fbfe4f3879285d89b4130479cac7", "class_name": "RelatedNodeInfo"}}, "text": "Flashattention: Fast and\nmemory-efficient exact attention with io-awareness. In NeurIPS, 2022.\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng,\nand Furu Wei. Longnet: Scaling transformers to 1, 000, 000, 000 tokens. CoRR , abs/2307.02486,\n2023.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm:\nGeneral language model pretraining with autoregressive blank infilling. In ACL , pp. 320\u2013335,\n2022.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: retrieval-\naugmented language model pre-training. CoRR, abs/2002.08909, 2020.\n10", "start_char_idx": 2618, "end_char_idx": 3259, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43f50844-054b-492f-942d-acb3350fbefc": {"__data__": {"id_": "43f50844-054b-492f-942d-acb3350fbefc", "embedding": null, "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "528ba9d7376ff4c0cbb18f8bf530e07a651ea68cfb6314498447d0b6789204b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4005cc8-bbe0-49d9-91ef-768a68541133", "node_type": "1", "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a76aeab1974240bb6f25c6a175e61b6b3eae311947853283ac8272031c8fb459", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f621bc79-a3c3-4de2-a439-712b4c6aac99", "node_type": "1", "metadata": {}, "hash": "e3f67c3b8f0790125fea8e059b299925d719e67b6efd0e04d278cda3ec1e9799", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nChi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple\non-the-fly length generalization for large language models. CoRR, abs/2308.16137, 2023.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022.\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick,\nJane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with\nretrieval augmented language models. CoRR, abs/2208.03299, 2022.", "start_char_idx": 0, "end_char_idx": 653, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f621bc79-a3c3-4de2-a439-712b4c6aac99": {"__data__": {"id_": "f621bc79-a3c3-4de2-a439-712b4c6aac99", "embedding": null, "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "528ba9d7376ff4c0cbb18f8bf530e07a651ea68cfb6314498447d0b6789204b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43f50844-054b-492f-942d-acb3350fbefc", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "e3408ef76ef3c93780dd30227ad2995c05ebe2382bd56a57dde35984fa945fd7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ced4a45-235e-4e17-9756-76a019b810c7", "node_type": "1", "metadata": {}, "hash": "1fd990fe205a2299e240e9797546ebd5385360141a855b9e00d3bc1caf33604b", "class_name": "RelatedNodeInfo"}}, "text": "Lm-infinite: Simple\non-the-fly length generalization for large language models. CoRR, abs/2308.16137, 2023.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022.\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick,\nJane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with\nretrieval augmented language models. CoRR, abs/2208.03299, 2022.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov,\nDanqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In\nEMNLP, pp. 6769\u20136781, 2020.", "start_char_idx": 115, "end_char_idx": 864, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ced4a45-235e-4e17-9756-76a019b810c7": {"__data__": {"id_": "6ced4a45-235e-4e17-9756-76a019b810c7", "embedding": null, "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "528ba9d7376ff4c0cbb18f8bf530e07a651ea68cfb6314498447d0b6789204b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f621bc79-a3c3-4de2-a439-712b4c6aac99", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "e9353f5ebb98a5328f4b7982a91a0a2a62d4668531ae6d8a2635ba0d605d8d9b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5ed7ba3-b428-48aa-945c-49f33ff9fc3e", "node_type": "1", "metadata": {}, "hash": "a7645823f2a6ef04fc057781b79b805919e10f8359b0fd5fedc103f9306c847a", "class_name": "RelatedNodeInfo"}}, "text": "Lora: Low-rank adaptation of large language models. In ICLR, 2022.\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick,\nJane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with\nretrieval augmented language models. CoRR, abs/2208.03299, 2022.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov,\nDanqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In\nEMNLP, pp. 6769\u20136781, 2020.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In ICLR ,\n2020.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\ntuning.", "start_char_idx": 334, "end_char_idx": 1073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5ed7ba3-b428-48aa-945c-49f33ff9fc3e": {"__data__": {"id_": "f5ed7ba3-b428-48aa-945c-49f33ff9fc3e", "embedding": null, "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "528ba9d7376ff4c0cbb18f8bf530e07a651ea68cfb6314498447d0b6789204b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ced4a45-235e-4e17-9756-76a019b810c7", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "3c6de005e0514ddbea4abf27ae99f8b5a3ced2c15b250c361ec16512edb0a83d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9666f0af-601c-4cbc-9e4d-e0deaee4004b", "node_type": "1", "metadata": {}, "hash": "2678414ca8f565674adbd63ccd3d5fb2e00c07779f110658dca8889860510ac8", "class_name": "RelatedNodeInfo"}}, "text": "Few-shot learning with\nretrieval augmented language models. CoRR, abs/2208.03299, 2022.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov,\nDanqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In\nEMNLP, pp. 6769\u20136781, 2020.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In ICLR ,\n2020.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\ntuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.),\nEMNLP, pp. 3045\u20133059, 2021.\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica,\nXuezhe Ma, and Hao Zhang.", "start_char_idx": 566, "end_char_idx": 1305, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9666f0af-601c-4cbc-9e4d-e0deaee4004b": {"__data__": {"id_": "9666f0af-601c-4cbc-9e4d-e0deaee4004b", "embedding": null, "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "528ba9d7376ff4c0cbb18f8bf530e07a651ea68cfb6314498447d0b6789204b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5ed7ba3-b428-48aa-945c-49f33ff9fc3e", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a699ef150532126228d326aa19c9b3381c1b2557a76efa0952dc70c651936d97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e362da55-6dee-435a-81eb-add758acb95b", "node_type": "1", "metadata": {}, "hash": "01fea5cfae0ce60d8e6a44a80c8c7ab40bc268ee2c138c1c7bba4b1841e8e43e", "class_name": "RelatedNodeInfo"}}, "text": "Dense passage retrieval for open-domain question answering. In\nEMNLP, pp. 6769\u20136781, 2020.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In ICLR ,\n2020.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\ntuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.),\nEMNLP, pp. 3045\u20133059, 2021.\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica,\nXuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?,\nJune 2023. URL https://lmsys.org/blog/2023-06-29-longchat .\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.", "start_char_idx": 774, "end_char_idx": 1521, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e362da55-6dee-435a-81eb-add758acb95b": {"__data__": {"id_": "e362da55-6dee-435a-81eb-add758acb95b", "embedding": null, "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "528ba9d7376ff4c0cbb18f8bf530e07a651ea68cfb6314498447d0b6789204b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9666f0af-601c-4cbc-9e4d-e0deaee4004b", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6c559af02bb2656ff73d00507e1304b0f041fae9c98933803ff1c5ab87498f6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3eb9743b-8b07-481e-b59d-e88bb1bdc2bf", "node_type": "1", "metadata": {}, "hash": "49ac4ae144fb2051c3358458c242ab6651c1ed500dc75dc348ab0a11df80229b", "class_name": "RelatedNodeInfo"}}, "text": "Reformer: The efficient transformer. In ICLR ,\n2020.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\ntuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.),\nEMNLP, pp. 3045\u20133059, 2021.\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica,\nXuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?,\nJune 2023. URL https://lmsys.org/blog/2023-06-29-longchat .\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In\nChengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), ACL, pp. 4582\u20134597, 2021.", "start_char_idx": 916, "end_char_idx": 1614, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3eb9743b-8b07-481e-b59d-e88bb1bdc2bf": {"__data__": {"id_": "3eb9743b-8b07-481e-b59d-e88bb1bdc2bf", "embedding": null, "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "528ba9d7376ff4c0cbb18f8bf530e07a651ea68cfb6314498447d0b6789204b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e362da55-6dee-435a-81eb-add758acb95b", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "aa9651be5fd895bbd97716a872d1293d02d74b72539b179523de31736b5e9bd8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d77696b6-e1ff-442a-b61d-4aad50045aff", "node_type": "1", "metadata": {}, "hash": "410aa7fdd33aafed063fc2c0aece28225a096e452e1c826a2a740b034c09cc08", "class_name": "RelatedNodeInfo"}}, "text": "In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.),\nEMNLP, pp. 3045\u20133059, 2021.\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica,\nXuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?,\nJune 2023. URL https://lmsys.org/blog/2023-06-29-longchat .\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In\nChengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), ACL, pp. 4582\u20134597, 2021.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin\nRaffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In\nNeurIPS, 2022.", "start_char_idx": 1074, "end_char_idx": 1820, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d77696b6-e1ff-442a-b61d-4aad50045aff": {"__data__": {"id_": "d77696b6-e1ff-442a-b61d-4aad50045aff", "embedding": null, "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "528ba9d7376ff4c0cbb18f8bf530e07a651ea68cfb6314498447d0b6789204b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3eb9743b-8b07-481e-b59d-e88bb1bdc2bf", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "88759c7a17d5903311debf9665f305819a633e60f2921ec8185acaba3d10c95a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5242756-de26-47c5-94bc-44dde66b0561", "node_type": "1", "metadata": {}, "hash": "543f66a7f9a0fdca495583b13fab29b6925800e266c6a0ed7073f9cf4214f00e", "class_name": "RelatedNodeInfo"}}, "text": "How long can open-source llms truly promise on context length?,\nJune 2023. URL https://lmsys.org/blog/2023-06-29-longchat .\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In\nChengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), ACL, pp. 4582\u20134597, 2021.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin\nRaffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In\nNeurIPS, 2022.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV , pp.\n9992\u201310002, 2021.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.", "start_char_idx": 1306, "end_char_idx": 2105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5242756-de26-47c5-94bc-44dde66b0561": {"__data__": {"id_": "c5242756-de26-47c5-94bc-44dde66b0561", "embedding": null, "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "528ba9d7376ff4c0cbb18f8bf530e07a651ea68cfb6314498447d0b6789204b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d77696b6-e1ff-442a-b61d-4aad50045aff", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "3861311d7243071d004890eef98058ddd3dc469ffc0c7f62566f5158393dc922", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18248ca3-ba28-40fd-876f-8e3ec4212582", "node_type": "1", "metadata": {}, "hash": "60ce7464a6767c2d0b0bcc5b649fef8ab7c3e91fa619c77f4ddc037f78712414", "class_name": "RelatedNodeInfo"}}, "text": "In\nChengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), ACL, pp. 4582\u20134597, 2021.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin\nRaffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In\nNeurIPS, 2022.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV , pp.\n9992\u201310002, 2021.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. Peft: State-\nof-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/\npeft , 2022.", "start_char_idx": 1522, "end_char_idx": 2298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18248ca3-ba28-40fd-876f-8e3ec4212582": {"__data__": {"id_": "18248ca3-ba28-40fd-876f-8e3ec4212582", "embedding": null, "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "528ba9d7376ff4c0cbb18f8bf530e07a651ea68cfb6314498447d0b6789204b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5242756-de26-47c5-94bc-44dde66b0561", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "51ab1f06545d5d3f3cf8f74357410885e2ecc4d1bba834480245b36b18c11614", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9fb5076-864e-47c0-b894-055604a57a49", "node_type": "1", "metadata": {}, "hash": "7eef3f280cccb1fb220c83250d3316a57419a5e010ddd98b82a61f6b42787b28", "class_name": "RelatedNodeInfo"}}, "text": "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In\nNeurIPS, 2022.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV , pp.\n9992\u201310002, 2021.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. Peft: State-\nof-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/\npeft , 2022.\nAmirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context\nlength for transformers. CoRR, abs/2305.16300, 2023.", "start_char_idx": 1714, "end_char_idx": 2442, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9fb5076-864e-47c0-b894-055604a57a49": {"__data__": {"id_": "e9fb5076-864e-47c0-b894-055604a57a49", "embedding": null, "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "528ba9d7376ff4c0cbb18f8bf530e07a651ea68cfb6314498447d0b6789204b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18248ca3-ba28-40fd-876f-8e3ec4212582", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "1a1fb2acfb9c9a7c21e38af8869901fea02b32df61ffe8fd8e043aa127cb6603", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0563b1b-1269-44ab-9e7a-9e5fca4458b5", "node_type": "1", "metadata": {}, "hash": "071e6d6bb671041b82ee811c4589b0c9045657daebd58b022f869c57f2340c3c", "class_name": "RelatedNodeInfo"}}, "text": "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV , pp.\n9992\u201310002, 2021.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. Peft: State-\nof-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/\npeft , 2022.\nAmirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context\nlength for transformers. CoRR, abs/2305.16300, 2023.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nK\u00a8opf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.", "start_char_idx": 1821, "end_char_idx": 2763, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0563b1b-1269-44ab-9e7a-9e5fca4458b5": {"__data__": {"id_": "f0563b1b-1269-44ab-9e7a-9e5fca4458b5", "embedding": null, "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "528ba9d7376ff4c0cbb18f8bf530e07a651ea68cfb6314498447d0b6789204b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9fb5076-864e-47c0-b894-055604a57a49", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "de2e8137a8b617b3cf6ba4bdb1f35d071c728adb5d2ade50538e4b07fdb84713", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7193e7dc-58cc-4716-b2fd-2d1fc87a641d", "node_type": "1", "metadata": {}, "hash": "e007309cdff9d273b885530559de3f4ee26fcce662fdddb6ce658ad9980e2515", "class_name": "RelatedNodeInfo"}}, "text": "Peft: State-\nof-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/\npeft , 2022.\nAmirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context\nlength for transformers. CoRR, abs/2305.16300, 2023.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nK\u00a8opf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-\nperformance deep learning library. In NeurIPS, pp. 8024\u20138035, 2019.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window\nextension of large language models.", "start_char_idx": 2189, "end_char_idx": 2998, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7193e7dc-58cc-4716-b2fd-2d1fc87a641d": {"__data__": {"id_": "7193e7dc-58cc-4716-b2fd-2d1fc87a641d", "embedding": null, "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "528ba9d7376ff4c0cbb18f8bf530e07a651ea68cfb6314498447d0b6789204b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0563b1b-1269-44ab-9e7a-9e5fca4458b5", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "24356b771773552db38829d16c082b72ff0b5f3e81fbfa8010dab51150edc1ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95504dfc-80ff-4330-a72c-2bced8191393", "node_type": "1", "metadata": {}, "hash": "77bf7c048d86e6a3bd08ea2b2eaf826621687c88658fb805082d1675da46d5f3", "class_name": "RelatedNodeInfo"}}, "text": "CoRR, abs/2305.16300, 2023.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nK\u00a8opf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-\nperformance deep learning library. In NeurIPS, pp. 8024\u20138035, 2019.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window\nextension of large language models. CoRR, abs/2309.00071, 2023.\nOfir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases\nenables input length extrapolation. In ICLR, 2022.", "start_char_idx": 2415, "end_char_idx": 3173, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95504dfc-80ff-4330-a72c-2bced8191393": {"__data__": {"id_": "95504dfc-80ff-4330-a72c-2bced8191393", "embedding": null, "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd6a3964-9ede-4b76-8b13-3d149bf79ae2", "node_type": "4", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "528ba9d7376ff4c0cbb18f8bf530e07a651ea68cfb6314498447d0b6789204b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7193e7dc-58cc-4716-b2fd-2d1fc87a641d", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "c8647b1ee32b495864db3fb9914bab8ab243f6cbc5b252927225ea0464a156c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1340b64-5420-4948-9ad9-6393298eabd8", "node_type": "1", "metadata": {}, "hash": "133fbe98e5e17ac9ee5a491deebe2fbc3b31fb13854214d0c39964c72c3c958b", "class_name": "RelatedNodeInfo"}}, "text": "Pytorch: An imperative style, high-\nperformance deep learning library. In NeurIPS, pp. 8024\u20138035, 2019.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window\nextension of large language models. CoRR, abs/2309.00071, 2023.\nOfir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases\nenables input length extrapolation. In ICLR, 2022.\nXiaojuan Qi, Renjie Liao, Jiaya Jia, Sanja Fidler, and Raquel Urtasun. 3d graph neural networks for\nRGBD semantic segmentation. In ICCV, pp. 5209\u20135218, 2017.\nJiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-\nattention for long document understanding. In EMNLP , volume EMNLP 2020 of Findings of\nACL, pp. 2555\u20132565, 2020.\n11", "start_char_idx": 2764, "end_char_idx": 3536, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1340b64-5420-4948-9ad9-6393298eabd8": {"__data__": {"id_": "f1340b64-5420-4948-9ad9-6393298eabd8", "embedding": null, "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95504dfc-80ff-4330-a72c-2bced8191393", "node_type": "1", "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "431e28cabcd97d32a8cd5ba6b72360565dd66d9bc52acdeb7126a1a8d85b6b83", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8621edb8-0760-4cdf-9a56-a18908ae090f", "node_type": "1", "metadata": {}, "hash": "ca62b2bf5435eb61b77953450dac190c123aa36b4384a4919bed38085cc0a644", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap.\nCompressive transformers for long-range sequence modelling. In ICLR, 2020.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System opti-\nmizations enable training deep learning models with over 100 billion parameters. In KDD , pp.\n3505\u20133506. ACM, 2020.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with\nrotary position embedding. CoRR, abs/2104.09864, 2021.\nYi-Lin Sung, Varun Nair, and Colin Raffel. Training neural networks with fixed sparse masks. In\nNeurIPS, pp. 24193\u201324205, 2021.\nMosaicML NLP Team. Introducing mpt-30b: Raising the bar for open-source foundation models,\n2023a.", "start_char_idx": 0, "end_char_idx": 794, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8621edb8-0760-4cdf-9a56-a18908ae090f": {"__data__": {"id_": "8621edb8-0760-4cdf-9a56-a18908ae090f", "embedding": null, "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1340b64-5420-4948-9ad9-6393298eabd8", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "271d1aa28a45ce2c43b2871274612b87e203372fd32494c8d5de2cccff40523e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4cd265f5-b681-4b82-895f-f89cb2be19cc", "node_type": "1", "metadata": {}, "hash": "559195572eed65f3168ce6f0e6365d95ca3f4a0b9e8416640d5274b0c7a0a3a0", "class_name": "RelatedNodeInfo"}}, "text": "Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System opti-\nmizations enable training deep learning models with over 100 billion parameters. In KDD , pp.\n3505\u20133506. ACM, 2020.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with\nrotary position embedding. CoRR, abs/2104.09864, 2021.\nYi-Lin Sung, Varun Nair, and Colin Raffel. Training neural networks with fixed sparse masks. In\nNeurIPS, pp. 24193\u201324205, 2021.\nMosaicML NLP Team. Introducing mpt-30b: Raising the bar for open-source foundation models,\n2023a. URL www.mosaicml.com/blog/mpt-30b .\nMosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable\nllms, 2023b. URL www.mosaicml.com/blog/mpt-7b .", "start_char_idx": 213, "end_char_idx": 969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4cd265f5-b681-4b82-895f-f89cb2be19cc": {"__data__": {"id_": "4cd265f5-b681-4b82-895f-f89cb2be19cc", "embedding": null, "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8621edb8-0760-4cdf-9a56-a18908ae090f", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "ee4e6b3efe30947ba3f039d4002fb0700f253889a3bb641efb82716380c29efe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e5c8e49-4007-4aca-98a2-09a288c1a90a", "node_type": "1", "metadata": {}, "hash": "76de44dab6b2b597645006751d395c21533db0a6caeb0eab3dc97cb5c6f4fb66", "class_name": "RelatedNodeInfo"}}, "text": "ACM, 2020.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with\nrotary position embedding. CoRR, abs/2104.09864, 2021.\nYi-Lin Sung, Varun Nair, and Colin Raffel. Training neural networks with fixed sparse masks. In\nNeurIPS, pp. 24193\u201324205, 2021.\nMosaicML NLP Team. Introducing mpt-30b: Raising the bar for open-source foundation models,\n2023a. URL www.mosaicml.com/blog/mpt-30b .\nMosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable\nllms, 2023b. URL www.mosaicml.com/blog/mpt-7b .\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth \u00b4ee\nLacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur \u00b4elien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample.", "start_char_idx": 408, "end_char_idx": 1207, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e5c8e49-4007-4aca-98a2-09a288c1a90a": {"__data__": {"id_": "3e5c8e49-4007-4aca-98a2-09a288c1a90a", "embedding": null, "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4cd265f5-b681-4b82-895f-f89cb2be19cc", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "0e54b508888b7ebb02b1e7e993ff7e2e916965b0f70306749ca178a2a2f27e1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d2d620d-e4ca-46a5-b9a8-9af7c3393959", "node_type": "1", "metadata": {}, "hash": "4a80040074bbcdd015a2a6338b94cf5b6642819fcdb76e48377e780b3dd2a735", "class_name": "RelatedNodeInfo"}}, "text": "In\nNeurIPS, pp. 24193\u201324205, 2021.\nMosaicML NLP Team. Introducing mpt-30b: Raising the bar for open-source foundation models,\n2023a. URL www.mosaicml.com/blog/mpt-30b .\nMosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable\nllms, 2023b. URL www.mosaicml.com/blog/mpt-7b .\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth \u00b4ee\nLacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur \u00b4elien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels. CoRR, abs/2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov,", "start_char_idx": 662, "end_char_idx": 1397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d2d620d-e4ca-46a5-b9a8-9af7c3393959": {"__data__": {"id_": "8d2d620d-e4ca-46a5-b9a8-9af7c3393959", "embedding": null, "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e5c8e49-4007-4aca-98a2-09a288c1a90a", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6417dd1253b56aa8dc42023d2b0ac595dacac86f26104485fa3db2b5286b3281", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "969a327c-df6d-43bb-8655-5daf7039e75f", "node_type": "1", "metadata": {}, "hash": "9d498a45155d6e89c75850623e5faf9458847c4d058dbaba8d73a24d50994407", "class_name": "RelatedNodeInfo"}}, "text": "MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable\nllms, 2023b. URL www.mosaicml.com/blog/mpt-7b .\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth \u00b4ee\nLacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur \u00b4elien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels. CoRR, abs/2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian\nCanton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,", "start_char_idx": 831, "end_char_idx": 1566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "969a327c-df6d-43bb-8655-5daf7039e75f": {"__data__": {"id_": "969a327c-df6d-43bb-8655-5daf7039e75f", "embedding": null, "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d2d620d-e4ca-46a5-b9a8-9af7c3393959", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "7e86bbf18661640dc96259596e97565b56b4d80585cc6829e6931162e367d89d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24a7bc2c-bd50-4ca3-9db3-2c1f6e9cd3bb", "node_type": "1", "metadata": {}, "hash": "bb5e8a880434c9c0f4ad40240f46d6b8fac709f782fa74a4f0dcd3038f31801d", "class_name": "RelatedNodeInfo"}}, "text": "Llama: Open and efficient foundation language\nmodels. CoRR, abs/2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian\nCanton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar\nHosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann,\nArtem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana\nLiskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor\nMolybog,", "start_char_idx": 1208, "end_char_idx": 1938, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24a7bc2c-bd50-4ca3-9db3-2c1f6e9cd3bb": {"__data__": {"id_": "24a7bc2c-bd50-4ca3-9db3-2c1f6e9cd3bb", "embedding": null, "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "969a327c-df6d-43bb-8655-5daf7039e75f", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "e2d6a9ffe8e54e78c7f1a678604661e401917bdac2f6a2bb7e08677bea175699", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26931cf2-0cf2-484b-a76a-9a2f2c43ba8e", "node_type": "1", "metadata": {}, "hash": "2d6891fdf9bbd0087dfd2dec67ececf5e54d9bb57aa537612a291e8c97895a4b", "class_name": "RelatedNodeInfo"}}, "text": "Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian\nCanton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar\nHosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann,\nArtem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana\nLiskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan\nSchelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\nRoss Taylor, Adina Williams,", "start_char_idx": 1398, "end_char_idx": 2141, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26931cf2-0cf2-484b-a76a-9a2f2c43ba8e": {"__data__": {"id_": "26931cf2-0cf2-484b-a76a-9a2f2c43ba8e", "embedding": null, "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24a7bc2c-bd50-4ca3-9db3-2c1f6e9cd3bb", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "98588cf9fc083a06b01d343d7e791677444f8ba7a6233a1f71af33705a56e864", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4091503-39ec-4971-9906-62c72ce7b27d", "node_type": "1", "metadata": {}, "hash": "bb0157568b2c15c3fa38e3bbf3ddc97c741505dab0aa36398b7c2c352f6f1282", "class_name": "RelatedNodeInfo"}}, "text": "Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar\nHosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann,\nArtem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana\nLiskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan\nSchelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\nRoss Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,\nAngela Fan, Melanie Kambadur, Sharan Narang, Aur \u00b4elien Rodriguez, Robert Stojnic, Sergey\nEdunov,", "start_char_idx": 1567, "end_char_idx": 2305, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4091503-39ec-4971-9906-62c72ce7b27d": {"__data__": {"id_": "d4091503-39ec-4971-9906-62c72ce7b27d", "embedding": null, "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26931cf2-0cf2-484b-a76a-9a2f2c43ba8e", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "902b245dbf09bc61ff1a4601a89c81a28db108bcd867a33dd5a1004de8f582f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87e0774b-9acb-44e2-bab1-c7ad6265dea7", "node_type": "1", "metadata": {}, "hash": "388d31f495cdf75118e6fa6c4e98f9858c70b30942b1f50242600cc1bf12825a", "class_name": "RelatedNodeInfo"}}, "text": "Madian Khabsa, Isabel Kloumann,\nArtem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana\nLiskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan\nSchelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\nRoss Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,\nAngela Fan, Melanie Kambadur, Sharan Narang, Aur \u00b4elien Rodriguez, Robert Stojnic, Sergey\nEdunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR ,\nabs/2307.09288, 2023b.", "start_char_idx": 1722, "end_char_idx": 2408, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87e0774b-9acb-44e2-bab1-c7ad6265dea7": {"__data__": {"id_": "87e0774b-9acb-44e2-bab1-c7ad6265dea7", "embedding": null, "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4091503-39ec-4971-9906-62c72ce7b27d", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "bcd243c0c7d3ceffc7caa8ddaafc4e08fa181c18eb7ccc9e23e0792ce2d77b69", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ecf66ed-0607-4e23-ae9c-dfbab7b1cd3e", "node_type": "1", "metadata": {}, "hash": "f633fcc52696ab7f43c6212c90f023a4febfa109c0c9d4a72aaa95d840c0a645", "class_name": "RelatedNodeInfo"}}, "text": "Jenya Lee, Diana\nLiskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan\nSchelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\nRoss Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,\nAngela Fan, Melanie Kambadur, Sharan Narang, Aur \u00b4elien Rodriguez, Robert Stojnic, Sergey\nEdunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR ,\nabs/2307.09288, 2023b.\nSzymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and\nPiotr Milos. Focused transformer: Contrastive training for context scaling.", "start_char_idx": 1824, "end_char_idx": 2574, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ecf66ed-0607-4e23-ae9c-dfbab7b1cd3e": {"__data__": {"id_": "2ecf66ed-0607-4e23-ae9c-dfbab7b1cd3e", "embedding": null, "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87e0774b-9acb-44e2-bab1-c7ad6265dea7", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a79558b8c99c1a035c85dc005f961a2268f0c89ebc75843863e2aa1fc233d80f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "971f1641-8cfd-4d7d-9794-6ef485058d15", "node_type": "1", "metadata": {}, "hash": "0696299fc8dab4c1f520c362b588cc71f7242272770bd85a99db057ed3778b16", "class_name": "RelatedNodeInfo"}}, "text": "Rashi Rungta, Kalyan Saladi, Alan\nSchelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\nRoss Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,\nAngela Fan, Melanie Kambadur, Sharan Narang, Aur \u00b4elien Rodriguez, Robert Stojnic, Sergey\nEdunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR ,\nabs/2307.09288, 2023b.\nSzymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and\nPiotr Milos. Focused transformer: Contrastive training for context scaling. CoRR , abs/2307.03170,\n2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin.", "start_char_idx": 1986, "end_char_idx": 2729, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "971f1641-8cfd-4d7d-9794-6ef485058d15": {"__data__": {"id_": "971f1641-8cfd-4d7d-9794-6ef485058d15", "embedding": null, "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ecf66ed-0607-4e23-ae9c-dfbab7b1cd3e", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "408c62036577df709eb7293c5d55ade711e92e41cc7f3ec5497f4d0cb35a349c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50c5b40d-57da-4c64-9744-c6e2650dfe23", "node_type": "1", "metadata": {}, "hash": "ad5564203add4235a8a8a591e2511461f389691802fa73d31c12c52e05d5e547", "class_name": "RelatedNodeInfo"}}, "text": "Zheng Yan, Iliyan Zarov, Yuchen Zhang,\nAngela Fan, Melanie Kambadur, Sharan Narang, Aur \u00b4elien Rodriguez, Robert Stojnic, Sergey\nEdunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR ,\nabs/2307.09288, 2023b.\nSzymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and\nPiotr Milos. Focused transformer: Contrastive training for context scaling. CoRR , abs/2307.03170,\n2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pp. 5998\u20136008, 2017.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\nwith linear complexity.", "start_char_idx": 2169, "end_char_idx": 2904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50c5b40d-57da-4c64-9744-c6e2650dfe23": {"__data__": {"id_": "50c5b40d-57da-4c64-9744-c6e2650dfe23", "embedding": null, "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "971f1641-8cfd-4d7d-9794-6ef485058d15", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "292a6c9124fb872eaf0c8ad8b81a926183326df83813fffd275f55f70a43a1bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e00b94e0-7373-4313-9913-dad5d7b9ac6c", "node_type": "1", "metadata": {}, "hash": "21034ba79fabf446292e058c8befd8885ddd161311bba5a6859820bda52520fa", "class_name": "RelatedNodeInfo"}}, "text": "CoRR ,\nabs/2307.09288, 2023b.\nSzymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and\nPiotr Milos. Focused transformer: Contrastive training for context scaling. CoRR , abs/2307.03170,\n2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pp. 5998\u20136008, 2017.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\nwith linear complexity. CoRR, abs/2006.04768, 2020.\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing trans-\nformers. In ICLR, 2022.", "start_char_idx": 2379, "end_char_idx": 3047, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e00b94e0-7373-4313-9913-dad5d7b9ac6c": {"__data__": {"id_": "e00b94e0-7373-4313-9913-dad5d7b9ac6c", "embedding": null, "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50c5b40d-57da-4c64-9744-c6e2650dfe23", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4405cd5cc187a6fcc9f5ec6d8fc5950b28867dba1f36cb925a245409006edd10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2b734de-2263-42da-9e8a-dbef54b8285b", "node_type": "1", "metadata": {}, "hash": "9f7866da0d32bb797828b0b46fa74ec0c365f2e1c5e7b476f0e4a54a39d1666b", "class_name": "RelatedNodeInfo"}}, "text": "Focused transformer: Contrastive training for context scaling. CoRR , abs/2307.03170,\n2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pp. 5998\u20136008, 2017.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\nwith linear complexity. CoRR, abs/2006.04768, 2020.\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing trans-\nformers. In ICLR, 2022.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\nOnta \u02dcn\u00b4on, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed.", "start_char_idx": 2512, "end_char_idx": 3217, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2b734de-2263-42da-9e8a-dbef54b8285b": {"__data__": {"id_": "f2b734de-2263-42da-9e8a-dbef54b8285b", "embedding": null, "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e00b94e0-7373-4313-9913-dad5d7b9ac6c", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "aa3fba98e209a7b597facaae936a50bb0957f38d91b8531a4826c6167b6ea1c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "816c37a6-ad7d-4e5a-8c47-f2ba4a2b551a", "node_type": "1", "metadata": {}, "hash": "68d9adc55b0852da1bfe29f8aec1c199cc0d5ecca12a679c4f5d5be035cb1052", "class_name": "RelatedNodeInfo"}}, "text": "Attention is all you need. In NeurIPS, pp. 5998\u20136008, 2017.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\nwith linear complexity. CoRR, abs/2006.04768, 2020.\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing trans-\nformers. In ICLR, 2022.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\nOnta \u02dcn\u00b4on, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird:\nTransformers for longer sequences. In NeurIPS, 2020.\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models.", "start_char_idx": 2730, "end_char_idx": 3425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "816c37a6-ad7d-4e5a-8c47-f2ba4a2b551a": {"__data__": {"id_": "816c37a6-ad7d-4e5a-8c47-f2ba4a2b551a", "embedding": null, "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2b734de-2263-42da-9e8a-dbef54b8285b", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "87446e0b30c2fffc5def3781f93f97b83fc4ff5e4cedb26ffd8986b06e3bbb1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbaaae45-f6aa-4e2b-a980-1bbec6ef76ff", "node_type": "1", "metadata": {}, "hash": "cb29468c5395e115d691d59a1281d0d606e9888022c97ad91a0e265a161100d2", "class_name": "RelatedNodeInfo"}}, "text": "Linformer: Self-attention\nwith linear complexity. CoRR, abs/2006.04768, 2020.\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing trans-\nformers. In ICLR, 2022.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\nOnta \u02dcn\u00b4on, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird:\nTransformers for longer sequences. In NeurIPS, 2020.\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models. In Smaranda Muresan, Preslav Nakov, and Aline\nVillavicencio (eds.), ACL, pp. 1\u20139, 2022.\nMian Zhang, Lifeng Jin, Linfeng Song, Haitao Mi, Wenliang Chen, and Dong Yu.", "start_char_idx": 2855, "end_char_idx": 3590, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbaaae45-f6aa-4e2b-a980-1bbec6ef76ff": {"__data__": {"id_": "bbaaae45-f6aa-4e2b-a980-1bbec6ef76ff", "embedding": null, "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "816c37a6-ad7d-4e5a-8c47-f2ba4a2b551a", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "34afc950e63c551b2e429978cdce94b9d30d0f4927b140ee32b907c5715d3d45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdb0f1b2-e9fd-4595-b93e-ef551b0c2aed", "node_type": "1", "metadata": {}, "hash": "a36070929633c9c8ee23152ac97506d0522d620a1abdc43eec5dfd96928c8d79", "class_name": "RelatedNodeInfo"}}, "text": "In ICLR, 2022.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\nOnta \u02dcn\u00b4on, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird:\nTransformers for longer sequences. In NeurIPS, 2020.\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models. In Smaranda Muresan, Preslav Nakov, and Aline\nVillavicencio (eds.), ACL, pp. 1\u20139, 2022.\nMian Zhang, Lifeng Jin, Linfeng Song, Haitao Mi, Wenliang Chen, and Dong Yu. Safeconv:\nExplaining and correcting conversational unsafe behavior. In ACL, pp. 22\u201335, 2023.\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li.", "start_char_idx": 3033, "end_char_idx": 3764, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdb0f1b2-e9fd-4595-b93e-ef551b0c2aed": {"__data__": {"id_": "bdb0f1b2-e9fd-4595-b93e-ef551b0c2aed", "embedding": null, "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "52fb437e-fae3-4323-9973-bf7c1dd46d9e", "node_type": "4", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "11a4d140e5cc743ae4a626cdc5091869417619fa555f8d266b666481429af46e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbaaae45-f6aa-4e2b-a980-1bbec6ef76ff", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "bc213584f40cf761c70a82c678416ed3380398ca97b7d90c69a5694e7ed73a13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3733d4a4-aa62-4509-b31c-3421108ccea1", "node_type": "1", "metadata": {}, "hash": "93a092bef28d2e1f15d54ffc1f9446a87e2162853d8d625f488be6abe260af7e", "class_name": "RelatedNodeInfo"}}, "text": "Big bird:\nTransformers for longer sequences. In NeurIPS, 2020.\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models. In Smaranda Muresan, Preslav Nakov, and Aline\nVillavicencio (eds.), ACL, pp. 1\u20139, 2022.\nMian Zhang, Lifeng Jin, Linfeng Song, Haitao Mi, Wenliang Chen, and Dong Yu. Safeconv:\nExplaining and correcting conversational unsafe behavior. In ACL, pp. 22\u201335, 2023.\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose:\nEfficient context window extension of llms via positional skip-wise training, 2023.\n12", "start_char_idx": 3218, "end_char_idx": 3857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3733d4a4-aa62-4509-b31c-3421108ccea1": {"__data__": {"id_": "3733d4a4-aa62-4509-b31c-3421108ccea1", "embedding": null, "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "d693abb8777af6cf42a9663b9aa339505146e1acad10011ffcd728c8dcc0541d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdb0f1b2-e9fd-4595-b93e-ef551b0c2aed", "node_type": "1", "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "46b567f52a3da4cbdba0a50789449dc5999c60beac4e9b4aaea0f698685f6fd7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b90c6b6-2bef-4321-ae32-ce7239f3a15b", "node_type": "1", "metadata": {}, "hash": "98edf0654916ed1131be869e75f7e23effcadf90d4b6aa717a6c6d0d26d4fd62", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nAPPENDIX\nA S ETTINGS\nA.1 E NVIRONMENTS\nAll our experiments are conducted on an 8\u00d7A100 machine. We train all models using Py-\nTorch (Paszke et al., 2019) with the DeepSpeed (Rasley et al., 2020) and Flash-Attention2 (Dao,\n2023). By default, we use DeepSpeed (Rasley et al., 2020) in stage 2 and use stage 3 for the maximum\ncontext length experiments. Gradient checkpoint is used by default, which is a common technique\nin the Peft codebase (Mangrulkar et al., 2022). Note that sometimes, 8\u00d7A100 GPUs might not be\nnecessary and 3090 Ti GPUs are acceptable, like fine-tuning 7B models to 8192 context size.\nA.2 F ORMAT OF PASSKEY RETRIEVAL\nWe follow existing literature (Mohtashami & Jaggi, 2023; Tworkowski et al., 2023; Chen et al., 2023)\nfor the document format of passkey retrieval.", "start_char_idx": 0, "end_char_idx": 828, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b90c6b6-2bef-4321-ae32-ce7239f3a15b": {"__data__": {"id_": "1b90c6b6-2bef-4321-ae32-ce7239f3a15b", "embedding": null, "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "d693abb8777af6cf42a9663b9aa339505146e1acad10011ffcd728c8dcc0541d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3733d4a4-aa62-4509-b31c-3421108ccea1", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "56bcffb43519cbdfd7d78e191255b3925b4168ea4e59bb1808ac7e88c269db9b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "332609b6-6cc3-4580-ae50-def58c52b9b8", "node_type": "1", "metadata": {}, "hash": "ddde1112bbe9dfbd29bd9b2370818c8ec2c0fc4f062426d678de770868d34287", "class_name": "RelatedNodeInfo"}}, "text": "By default, we use DeepSpeed (Rasley et al., 2020) in stage 2 and use stage 3 for the maximum\ncontext length experiments. Gradient checkpoint is used by default, which is a common technique\nin the Peft codebase (Mangrulkar et al., 2022). Note that sometimes, 8\u00d7A100 GPUs might not be\nnecessary and 3090 Ti GPUs are acceptable, like fine-tuning 7B models to 8192 context size.\nA.2 F ORMAT OF PASSKEY RETRIEVAL\nWe follow existing literature (Mohtashami & Jaggi, 2023; Tworkowski et al., 2023; Chen et al., 2023)\nfor the document format of passkey retrieval. The document has the following format:\nThere is an important info hidden inside a lot of irrelevant text.\nFind it and memorize them. I will quiz you about the important\ninformation there.\nThe grass is green. The sky is blue. The sun is yellow. Here we\ngo. There and back again. (repeat Mtimes)\nThe pass key is 12362 . Remember it. 12362 is the pass key.", "start_char_idx": 273, "end_char_idx": 1182, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "332609b6-6cc3-4580-ae50-def58c52b9b8": {"__data__": {"id_": "332609b6-6cc3-4580-ae50-def58c52b9b8", "embedding": null, "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "d693abb8777af6cf42a9663b9aa339505146e1acad10011ffcd728c8dcc0541d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b90c6b6-2bef-4321-ae32-ce7239f3a15b", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "52a5ad6e50052c5a9d7ef1a2e7950b900123b2ebf3e7cc5c3d963076803a9763", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8740f887-2e78-4cba-acbe-a8229f0792d7", "node_type": "1", "metadata": {}, "hash": "b7b65421711a468bcdeb7f3e732decda7a0cc88e194ec3749a137618024c1bc6", "class_name": "RelatedNodeInfo"}}, "text": "Note that sometimes, 8\u00d7A100 GPUs might not be\nnecessary and 3090 Ti GPUs are acceptable, like fine-tuning 7B models to 8192 context size.\nA.2 F ORMAT OF PASSKEY RETRIEVAL\nWe follow existing literature (Mohtashami & Jaggi, 2023; Tworkowski et al., 2023; Chen et al., 2023)\nfor the document format of passkey retrieval. The document has the following format:\nThere is an important info hidden inside a lot of irrelevant text.\nFind it and memorize them. I will quiz you about the important\ninformation there.\nThe grass is green. The sky is blue. The sun is yellow. Here we\ngo. There and back again. (repeat Mtimes)\nThe pass key is 12362 . Remember it. 12362 is the pass key.\nThe grass is green. The sky is blue. The sun is yellow. Here we\ngo. There and back again. (repeat Ntimes)\nWhat is the pass key? The pass key is\nThe document length varies with the value of MandN.12362 is the passkey number to retrieve.", "start_char_idx": 511, "end_char_idx": 1418, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8740f887-2e78-4cba-acbe-a8229f0792d7": {"__data__": {"id_": "8740f887-2e78-4cba-acbe-a8229f0792d7", "embedding": null, "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "d693abb8777af6cf42a9663b9aa339505146e1acad10011ffcd728c8dcc0541d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "332609b6-6cc3-4580-ae50-def58c52b9b8", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "66051016c3e2aeddd7bcab3e648f585a606b77bf9ef35abe2fd5dcaaaf62864e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ba668ab-0283-43a8-b628-40ef6a79be8d", "node_type": "1", "metadata": {}, "hash": "80101a2179793a51f13ccf3c5c0f856fff73268016f15d517b96df9cb4f70a0e", "class_name": "RelatedNodeInfo"}}, "text": "The document has the following format:\nThere is an important info hidden inside a lot of irrelevant text.\nFind it and memorize them. I will quiz you about the important\ninformation there.\nThe grass is green. The sky is blue. The sun is yellow. Here we\ngo. There and back again. (repeat Mtimes)\nThe pass key is 12362 . Remember it. 12362 is the pass key.\nThe grass is green. The sky is blue. The sun is yellow. Here we\ngo. There and back again. (repeat Ntimes)\nWhat is the pass key? The pass key is\nThe document length varies with the value of MandN.12362 is the passkey number to retrieve. It is\nrandomly sampled and varies at each testing time.\nB E XPERIMENTS\nB.1 E VALUATION PERPLEXITY ON PG19 TEST SPLIT .\nIn Table 14 and Table 15, we present the evaluation results on the PG19 test split. We use the same\nsettings as the models on proof-pile (Azerbayev et al., 2022) evaluation in the paper.", "start_char_idx": 829, "end_char_idx": 1724, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ba668ab-0283-43a8-b628-40ef6a79be8d": {"__data__": {"id_": "4ba668ab-0283-43a8-b628-40ef6a79be8d", "embedding": null, "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "d693abb8777af6cf42a9663b9aa339505146e1acad10011ffcd728c8dcc0541d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8740f887-2e78-4cba-acbe-a8229f0792d7", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a083e6a4231acb3b11b7361f154337d0ebde34482f7446d07182273f4e795e4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "035a0640-6c10-4cf9-8dee-3b792537ff60", "node_type": "1", "metadata": {}, "hash": "556bf4caee5240bf0f7f043282f64fe8dc3e34509f56e17000154c3a5aa7afe7", "class_name": "RelatedNodeInfo"}}, "text": "Here we\ngo. There and back again. (repeat Mtimes)\nThe pass key is 12362 . Remember it. 12362 is the pass key.\nThe grass is green. The sky is blue. The sun is yellow. Here we\ngo. There and back again. (repeat Ntimes)\nWhat is the pass key? The pass key is\nThe document length varies with the value of MandN.12362 is the passkey number to retrieve. It is\nrandomly sampled and varies at each testing time.\nB E XPERIMENTS\nB.1 E VALUATION PERPLEXITY ON PG19 TEST SPLIT .\nIn Table 14 and Table 15, we present the evaluation results on the PG19 test split. We use the same\nsettings as the models on proof-pile (Azerbayev et al., 2022) evaluation in the paper. Similarly, for\na model trained on a certain context length, as the evaluation context length increases, our models\nachieve better perplexity.", "start_char_idx": 1073, "end_char_idx": 1866, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "035a0640-6c10-4cf9-8dee-3b792537ff60": {"__data__": {"id_": "035a0640-6c10-4cf9-8dee-3b792537ff60", "embedding": null, "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "d693abb8777af6cf42a9663b9aa339505146e1acad10011ffcd728c8dcc0541d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ba668ab-0283-43a8-b628-40ef6a79be8d", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "fd298bfafb98dfd39f829919e1315d9b1426af0d805a9657be1c5741fd32eab0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "773717ef-e388-4755-acc0-78a530477037", "node_type": "1", "metadata": {}, "hash": "26dfabcf9b57a6c98e9767f2dd6983683631937daa4fcdd4d58b28f181b423f9", "class_name": "RelatedNodeInfo"}}, "text": "Remember it. 12362 is the pass key.\nThe grass is green. The sky is blue. The sun is yellow. Here we\ngo. There and back again. (repeat Ntimes)\nWhat is the pass key? The pass key is\nThe document length varies with the value of MandN.12362 is the passkey number to retrieve. It is\nrandomly sampled and varies at each testing time.\nB E XPERIMENTS\nB.1 E VALUATION PERPLEXITY ON PG19 TEST SPLIT .\nIn Table 14 and Table 15, we present the evaluation results on the PG19 test split. We use the same\nsettings as the models on proof-pile (Azerbayev et al., 2022) evaluation in the paper. Similarly, for\na model trained on a certain context length, as the evaluation context length increases, our models\nachieve better perplexity. Note that the perplexity in Table 14 and Table 15 is higher than that in the\nproof-pile dataset, as PG19 (Rae et al., 2020) has very different writing styles.\nB.2 A BLATION ON GROUP SIZES .", "start_char_idx": 1147, "end_char_idx": 2056, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "773717ef-e388-4755-acc0-78a530477037": {"__data__": {"id_": "773717ef-e388-4755-acc0-78a530477037", "embedding": null, "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "d693abb8777af6cf42a9663b9aa339505146e1acad10011ffcd728c8dcc0541d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "035a0640-6c10-4cf9-8dee-3b792537ff60", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6197ce59871cf95efc8ec492fb0b54cb0dba4bd725a5336d0144b1de0bee427f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa3f9ded-8b34-4766-9077-4cfc27e09e01", "node_type": "1", "metadata": {}, "hash": "53c256edb8d8b7285b064f919f938e55f816369a1931f98cb720b2a948305a6a", "class_name": "RelatedNodeInfo"}}, "text": "It is\nrandomly sampled and varies at each testing time.\nB E XPERIMENTS\nB.1 E VALUATION PERPLEXITY ON PG19 TEST SPLIT .\nIn Table 14 and Table 15, we present the evaluation results on the PG19 test split. We use the same\nsettings as the models on proof-pile (Azerbayev et al., 2022) evaluation in the paper. Similarly, for\na model trained on a certain context length, as the evaluation context length increases, our models\nachieve better perplexity. Note that the perplexity in Table 14 and Table 15 is higher than that in the\nproof-pile dataset, as PG19 (Rae et al., 2020) has very different writing styles.\nB.2 A BLATION ON GROUP SIZES .\nIn Table 7, we provide an ablation study on the group size of the S2-Attn. We experimented with\nfine-tuning Llama2 7B to 8192 and 16384 context lengths via LongLoRA. The group size varies\nfrom{1/2, 1/4, 1/6, 1/8 }of the target context length.", "start_char_idx": 1419, "end_char_idx": 2299, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa3f9ded-8b34-4766-9077-4cfc27e09e01": {"__data__": {"id_": "aa3f9ded-8b34-4766-9077-4cfc27e09e01", "embedding": null, "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "d693abb8777af6cf42a9663b9aa339505146e1acad10011ffcd728c8dcc0541d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "773717ef-e388-4755-acc0-78a530477037", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "ea6a2aa2c9fd4decbd36725b984597afb01ad86ea6f179cf369330b8147f6964", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5948ad4b-b6eb-41e7-b7a9-05c54feeaae0", "node_type": "1", "metadata": {}, "hash": "975c56113a1633dc5ac80781fd5be3919541e399eb723e91323c730172a9bc8d", "class_name": "RelatedNodeInfo"}}, "text": "Similarly, for\na model trained on a certain context length, as the evaluation context length increases, our models\nachieve better perplexity. Note that the perplexity in Table 14 and Table 15 is higher than that in the\nproof-pile dataset, as PG19 (Rae et al., 2020) has very different writing styles.\nB.2 A BLATION ON GROUP SIZES .\nIn Table 7, we provide an ablation study on the group size of the S2-Attn. We experimented with\nfine-tuning Llama2 7B to 8192 and 16384 context lengths via LongLoRA. The group size varies\nfrom{1/2, 1/4, 1/6, 1/8 }of the target context length. For example, the group size is 1024 for 1/8\nof the context length 8192. We find that the 1/2 and 1/4 settings have minor gaps to full attention\nfine-tuning. Group sizes less than 1/4 would be not good enough. We set the group size as 1/4 of the\ncontext length in experiments by default.", "start_char_idx": 1725, "end_char_idx": 2586, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5948ad4b-b6eb-41e7-b7a9-05c54feeaae0": {"__data__": {"id_": "5948ad4b-b6eb-41e7-b7a9-05c54feeaae0", "embedding": null, "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "d693abb8777af6cf42a9663b9aa339505146e1acad10011ffcd728c8dcc0541d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa3f9ded-8b34-4766-9077-4cfc27e09e01", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "b748ff73d1a777f57f50afcd83d654ed862f7249c6eb938aeaea01c824f45f8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ad3c605-bfe0-47d8-82a9-c7592fdeb73f", "node_type": "1", "metadata": {}, "hash": "08d0507eaca01f862c96616693996d856729387a4f7ef55c8fc9daf17d644adb", "class_name": "RelatedNodeInfo"}}, "text": "B.2 A BLATION ON GROUP SIZES .\nIn Table 7, we provide an ablation study on the group size of the S2-Attn. We experimented with\nfine-tuning Llama2 7B to 8192 and 16384 context lengths via LongLoRA. The group size varies\nfrom{1/2, 1/4, 1/6, 1/8 }of the target context length. For example, the group size is 1024 for 1/8\nof the context length 8192. We find that the 1/2 and 1/4 settings have minor gaps to full attention\nfine-tuning. Group sizes less than 1/4 would be not good enough. We set the group size as 1/4 of the\ncontext length in experiments by default.\nTable 7: Ablation on group size. We fine-tune a Llama2 7B model to 8192 and 16384 context lengths\nvia LongLoRA and evaluate on PG19 validation set.", "start_char_idx": 2026, "end_char_idx": 2734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ad3c605-bfe0-47d8-82a9-c7592fdeb73f": {"__data__": {"id_": "0ad3c605-bfe0-47d8-82a9-c7592fdeb73f", "embedding": null, "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "d693abb8777af6cf42a9663b9aa339505146e1acad10011ffcd728c8dcc0541d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5948ad4b-b6eb-41e7-b7a9-05c54feeaae0", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "27e13167378505bb38e3d8ab56f96fd63cfe8ddf6d37590cf4982a5ecc36f14f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8971594-96e9-41ad-92b0-12d2798e8912", "node_type": "1", "metadata": {}, "hash": "7ee6f1611004137c8d85ee99ad6e1f19f7e5330091d705cfbc4e48c7d4091c65", "class_name": "RelatedNodeInfo"}}, "text": "We experimented with\nfine-tuning Llama2 7B to 8192 and 16384 context lengths via LongLoRA. The group size varies\nfrom{1/2, 1/4, 1/6, 1/8 }of the target context length. For example, the group size is 1024 for 1/8\nof the context length 8192. We find that the 1/2 and 1/4 settings have minor gaps to full attention\nfine-tuning. Group sizes less than 1/4 would be not good enough. We set the group size as 1/4 of the\ncontext length in experiments by default.\nTable 7: Ablation on group size. We fine-tune a Llama2 7B model to 8192 and 16384 context lengths\nvia LongLoRA and evaluate on PG19 validation set. We vary the group size of S2-Attn from {1/2,\n1/4, 1/6, 1/8 }of the target context length. \u2018Full\u2019 means the standard full attention.", "start_char_idx": 2132, "end_char_idx": 2866, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8971594-96e9-41ad-92b0-12d2798e8912": {"__data__": {"id_": "b8971594-96e9-41ad-92b0-12d2798e8912", "embedding": null, "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "580ee1e9-d087-44e6-bbdf-ae04f60f8643", "node_type": "4", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "d693abb8777af6cf42a9663b9aa339505146e1acad10011ffcd728c8dcc0541d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ad3c605-bfe0-47d8-82a9-c7592fdeb73f", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "fb2a50c6de40ae37dbafe7b8d698960c63f03829e828c726d790258e94d6361c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58e60fc0-4848-46f2-b261-d6b7be4f5d22", "node_type": "1", "metadata": {}, "hash": "9b2468a6dfadf11ae153f3fe5b06fac41f6a353d77e5a60f4c85b29d76e59424", "class_name": "RelatedNodeInfo"}}, "text": "For example, the group size is 1024 for 1/8\nof the context length 8192. We find that the 1/2 and 1/4 settings have minor gaps to full attention\nfine-tuning. Group sizes less than 1/4 would be not good enough. We set the group size as 1/4 of the\ncontext length in experiments by default.\nTable 7: Ablation on group size. We fine-tune a Llama2 7B model to 8192 and 16384 context lengths\nvia LongLoRA and evaluate on PG19 validation set. We vary the group size of S2-Attn from {1/2,\n1/4, 1/6, 1/8 }of the target context length. \u2018Full\u2019 means the standard full attention.\nContext Length Full 1/2 1/4 1/6 1/8\n8192 8.02 8.04 8.04 8.10 8.16\n16384 7.82 7.84 7.86 7.94 7.98\n13", "start_char_idx": 2300, "end_char_idx": 2966, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58e60fc0-4848-46f2-b261-d6b7be4f5d22": {"__data__": {"id_": "58e60fc0-4848-46f2-b261-d6b7be4f5d22", "embedding": null, "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "2336043e2c8a1a0ddb2ab140426f27b6fe6caf514e27af1a9c68a151729ec4cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8971594-96e9-41ad-92b0-12d2798e8912", "node_type": "1", "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "e2a0db999263cd9d07b6181c83e1a70dbcae176b75e07d5ac52cf8eca4577b13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc157a10-f735-4b29-87cb-1b2cc43e1bfd", "node_type": "1", "metadata": {}, "hash": "bca2cd18c605141e010ae0b32c7c0fdd5c496d1f3bfbc777b095aa4e7df85097", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nB.3 A BLATION ON THE VARIANTS OF S2-ATTN.\nIn Table 8, we ablate some variants of S2-Attn, which are illustrated in Figure 6. Variant 1 is to\nchange the shifting direction from down to up. It shows that the shifting direction has no effect on the\nperplexity. One concern about S2-Attn is that it moves the last tokens to the front into one group,\nwhich might be inconsistent with causal masks. Variant 2 uses individual groups for the shifted tokens,\nwhich ablates this concern. Variant 3 swaps the shifted and the original front tokens, which can also\nablate the concern. We show that these variants present similar perplexity to ours. We suppose that\nalthough there are communications among the front and last tokens, they are originally far away from\nothers while it is limited in the local group. Moreover, S2-Attn is only used for fine-tuning, while we\nuse standard causal masks and full attention during inference. Variant 2 and 3 also work well but\ninvolve additional steps to ours.", "start_char_idx": 0, "end_char_idx": 1033, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc157a10-f735-4b29-87cb-1b2cc43e1bfd": {"__data__": {"id_": "dc157a10-f735-4b29-87cb-1b2cc43e1bfd", "embedding": null, "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "2336043e2c8a1a0ddb2ab140426f27b6fe6caf514e27af1a9c68a151729ec4cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58e60fc0-4848-46f2-b261-d6b7be4f5d22", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6b0f0d203e63b78074f6e7e91e318d04d5968cc91d6b6418bf33bed19a21d3cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "798accf5-d179-4622-b47e-629bcc3b03fe", "node_type": "1", "metadata": {}, "hash": "0aa96724b136b44705451c3829c1bd66b4bccb73b46366cbf69381cae18fc9cd", "class_name": "RelatedNodeInfo"}}, "text": "Variant 1 is to\nchange the shifting direction from down to up. It shows that the shifting direction has no effect on the\nperplexity. One concern about S2-Attn is that it moves the last tokens to the front into one group,\nwhich might be inconsistent with causal masks. Variant 2 uses individual groups for the shifted tokens,\nwhich ablates this concern. Variant 3 swaps the shifted and the original front tokens, which can also\nablate the concern. We show that these variants present similar perplexity to ours. We suppose that\nalthough there are communications among the front and last tokens, they are originally far away from\nothers while it is limited in the local group. Moreover, S2-Attn is only used for fine-tuning, while we\nuse standard causal masks and full attention during inference. Variant 2 and 3 also work well but\ninvolve additional steps to ours.\nTable 8: Ablation on the variants of S2-Attn. These variants are illustrated in Figure 6. Similar to the\nsetting in Table 7, we fine-tune a Llama2 7B to 8192 context and evaluate on PG19 validation set.", "start_char_idx": 170, "end_char_idx": 1236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "798accf5-d179-4622-b47e-629bcc3b03fe": {"__data__": {"id_": "798accf5-d179-4622-b47e-629bcc3b03fe", "embedding": null, "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "2336043e2c8a1a0ddb2ab140426f27b6fe6caf514e27af1a9c68a151729ec4cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc157a10-f735-4b29-87cb-1b2cc43e1bfd", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "78bdebd348e84e310c087703582329e8789a5bb7cf2e4bdaa71b7b09ae55ecb6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fec9dc9a-e377-4634-8427-1ef0015614c6", "node_type": "1", "metadata": {}, "hash": "9f8372de73e3671a6d24af7ac1d95fc96f1d80f0d380c7b39d8fd78f194b08dc", "class_name": "RelatedNodeInfo"}}, "text": "Variant 2 uses individual groups for the shifted tokens,\nwhich ablates this concern. Variant 3 swaps the shifted and the original front tokens, which can also\nablate the concern. We show that these variants present similar perplexity to ours. We suppose that\nalthough there are communications among the front and last tokens, they are originally far away from\nothers while it is limited in the local group. Moreover, S2-Attn is only used for fine-tuning, while we\nuse standard causal masks and full attention during inference. Variant 2 and 3 also work well but\ninvolve additional steps to ours.\nTable 8: Ablation on the variants of S2-Attn. These variants are illustrated in Figure 6. Similar to the\nsetting in Table 7, we fine-tune a Llama2 7B to 8192 context and evaluate on PG19 validation set.\nAttn Full Ours Variant 1 Variant 2 Variant 3\nPPL 8.02 8.04 8.04 8.03 8.05\nTable 9: Evaluation on LongBench (Bai et al., 2023) benchmark.", "start_char_idx": 438, "end_char_idx": 1373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fec9dc9a-e377-4634-8427-1ef0015614c6": {"__data__": {"id_": "fec9dc9a-e377-4634-8427-1ef0015614c6", "embedding": null, "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "2336043e2c8a1a0ddb2ab140426f27b6fe6caf514e27af1a9c68a151729ec4cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "798accf5-d179-4622-b47e-629bcc3b03fe", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "14440ee6ded749371af54e8636b64f2ec378730a631bbf865c8cb822a90c2371", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "149fbc9c-44ea-4be7-baf4-d05f5c32fe89", "node_type": "1", "metadata": {}, "hash": "36cfc5c5d987b2eadcc15d14ef87bc22cee0a818ba2591c3728962c67d306b27", "class_name": "RelatedNodeInfo"}}, "text": "Moreover, S2-Attn is only used for fine-tuning, while we\nuse standard causal masks and full attention during inference. Variant 2 and 3 also work well but\ninvolve additional steps to ours.\nTable 8: Ablation on the variants of S2-Attn. These variants are illustrated in Figure 6. Similar to the\nsetting in Table 7, we fine-tune a Llama2 7B to 8192 context and evaluate on PG19 validation set.\nAttn Full Ours Variant 1 Variant 2 Variant 3\nPPL 8.02 8.04 8.04 8.03 8.05\nTable 9: Evaluation on LongBench (Bai et al., 2023) benchmark. In each column, we highlight the\nhighest value to be bold and the second highest value with underline.", "start_char_idx": 845, "end_char_idx": 1476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "149fbc9c-44ea-4be7-baf4-d05f5c32fe89": {"__data__": {"id_": "149fbc9c-44ea-4be7-baf4-d05f5c32fe89", "embedding": null, "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "2336043e2c8a1a0ddb2ab140426f27b6fe6caf514e27af1a9c68a151729ec4cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fec9dc9a-e377-4634-8427-1ef0015614c6", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "7a62d950c3979d925a98b0b9d913ab6972a131dc1ddbe5122acfc3f49340c0ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "154961d0-1fc1-4844-91f9-7dfb43fd2355", "node_type": "1", "metadata": {}, "hash": "737c036fbcf727e322cf20c1905edba9d9be454538b4673d1801a4621ea11777", "class_name": "RelatedNodeInfo"}}, "text": "Moreover, S2-Attn is only used for fine-tuning, while we\nuse standard causal masks and full attention during inference. Variant 2 and 3 also work well but\ninvolve additional steps to ours.\nTable 8: Ablation on the variants of S2-Attn. These variants are illustrated in Figure 6. Similar to the\nsetting in Table 7, we fine-tune a Llama2 7B to 8192 context and evaluate on PG19 validation set.\nAttn Full Ours Variant 1 Variant 2 Variant 3\nPPL 8.02 8.04 8.04 8.03 8.05\nTable 9: Evaluation on LongBench (Bai et al., 2023) benchmark. In each column, we highlight the\nhighest value to be bold and the second highest value with underline.\nModel AvgSingle-\nDoc QAMulti-\nDoc QASummarizationFew-shot\nLearningCode Synthetic\nGPT-3.5-Turbo 44.0 39.8 38.7 26.5 67.1 54.1 37.8\nLlama2-7B-chat 31.0 24.9 22.6 24.7 60.0 48.1 5.9\nLongChat-v1.5-7B 34.3 28.7 20.6 26.7 60.0 54.1 15.8\nVicuna-v1.5-7B 31.9 28.0 18.6 26.0 66.2 47.3 5.5\nOurs-7B 36.8 28.7 28.1 27.8 63.7 56.0 16.7\nTable 10: Evaluation on LEval (An et al., 2023) open-ended benchmark.", "start_char_idx": 845, "end_char_idx": 1869, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "154961d0-1fc1-4844-91f9-7dfb43fd2355": {"__data__": {"id_": "154961d0-1fc1-4844-91f9-7dfb43fd2355", "embedding": null, "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "2336043e2c8a1a0ddb2ab140426f27b6fe6caf514e27af1a9c68a151729ec4cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "149fbc9c-44ea-4be7-baf4-d05f5c32fe89", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6b5db090fd6435cc826e530c868771ea59e27bc547d48fb1be726dcffd2fe3e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "addcdd2f-35e5-4430-bab0-4e38db2f9491", "node_type": "1", "metadata": {}, "hash": "e7d43478b1277a6fc278bcb0d0428b1930befd3acf147e56e2f3fdb8775112a9", "class_name": "RelatedNodeInfo"}}, "text": "We compare various models\nto GPT-3.5-Turbo and judge win rates via GPT-4.\nModel Win-rate Wins Ties\nLongChat-7B (Li et al., 2023) 33.68 36 56\nLongChat-v1.5-7B (Li et al., 2023) 33.59 38 53\nVicuna-v1.5-7B (Chiang et al., 2023) 25.52 22 54\nOurs-7B 39.06 45 60\nB.4 E VALUATION ON LONG -CONTEXT BENCHMARKS .\nWe evaluate our method on long-context benchmarks, LongBench (Bai et al., 2023) in Table 9 and\nLEval (An et al., 2023) in Table 10. We fine-tune Llama2 7B to 16384 context length, with the\nsupervised fine-tuning method and data introduced in Section B.6.", "start_char_idx": 1870, "end_char_idx": 2427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "addcdd2f-35e5-4430-bab0-4e38db2f9491": {"__data__": {"id_": "addcdd2f-35e5-4430-bab0-4e38db2f9491", "embedding": null, "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "2336043e2c8a1a0ddb2ab140426f27b6fe6caf514e27af1a9c68a151729ec4cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "154961d0-1fc1-4844-91f9-7dfb43fd2355", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "bd5ec21bffe02fe6e2a5a287fdac628db5820075f4ef5a980d23e1720e549ddd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "607037a1-d640-4113-b469-aa0b97094622", "node_type": "1", "metadata": {}, "hash": "e7cdb585f8025c6f257de63a7ea0eef95fc9f0716aabedbe3029ed84cea6a57d", "class_name": "RelatedNodeInfo"}}, "text": "Model Win-rate Wins Ties\nLongChat-7B (Li et al., 2023) 33.68 36 56\nLongChat-v1.5-7B (Li et al., 2023) 33.59 38 53\nVicuna-v1.5-7B (Chiang et al., 2023) 25.52 22 54\nOurs-7B 39.06 45 60\nB.4 E VALUATION ON LONG -CONTEXT BENCHMARKS .\nWe evaluate our method on long-context benchmarks, LongBench (Bai et al., 2023) in Table 9 and\nLEval (An et al., 2023) in Table 10. We fine-tune Llama2 7B to 16384 context length, with the\nsupervised fine-tuning method and data introduced in Section B.6. We compare our model with\nGPT-3.5-Turbo and other Llama2-based long-context models, like Vicuna (Chiang et al., 2023) and\nLongChat (Li et al., 2023) models.", "start_char_idx": 1944, "end_char_idx": 2584, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "607037a1-d640-4113-b469-aa0b97094622": {"__data__": {"id_": "607037a1-d640-4113-b469-aa0b97094622", "embedding": null, "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "2336043e2c8a1a0ddb2ab140426f27b6fe6caf514e27af1a9c68a151729ec4cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "addcdd2f-35e5-4430-bab0-4e38db2f9491", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "3e374e1e56052b0717c8907692ffe91f22d71d237ff6602d2fe69b2cc41f9642", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48131f00-78b7-4d03-ad3d-13b4867dd447", "node_type": "1", "metadata": {}, "hash": "b486b6788c8ff601097c394a829d7bdfe9b4ca8d0a3122b0076f867c0cc9417a", "class_name": "RelatedNodeInfo"}}, "text": "We evaluate our method on long-context benchmarks, LongBench (Bai et al., 2023) in Table 9 and\nLEval (An et al., 2023) in Table 10. We fine-tune Llama2 7B to 16384 context length, with the\nsupervised fine-tuning method and data introduced in Section B.6. We compare our model with\nGPT-3.5-Turbo and other Llama2-based long-context models, like Vicuna (Chiang et al., 2023) and\nLongChat (Li et al., 2023) models. It shows that our 7B model presents comparable or even better\nperformance than these Llama2-based long-context models, while ours only takes about 4 hours,\nabout 0.3 billion tokens, on a single 8 \u00d7A100 machine.\nB.5 E FFICIENCY ANALYSIS .", "start_char_idx": 2173, "end_char_idx": 2822, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48131f00-78b7-4d03-ad3d-13b4867dd447": {"__data__": {"id_": "48131f00-78b7-4d03-ad3d-13b4867dd447", "embedding": null, "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "2336043e2c8a1a0ddb2ab140426f27b6fe6caf514e27af1a9c68a151729ec4cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "607037a1-d640-4113-b469-aa0b97094622", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "da06b65eb3643b75174c6bdfb1c642c514bad63632e7fc0877b608a7ee1eab3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f1bd0a5-55c3-47a3-8b84-8373111c4d2b", "node_type": "1", "metadata": {}, "hash": "87ccfd4a2d11a0d959397738100a793efa8392c1e3076755463acd1d20659286", "class_name": "RelatedNodeInfo"}}, "text": "We evaluate our method on long-context benchmarks, LongBench (Bai et al., 2023) in Table 9 and\nLEval (An et al., 2023) in Table 10. We fine-tune Llama2 7B to 16384 context length, with the\nsupervised fine-tuning method and data introduced in Section B.6. We compare our model with\nGPT-3.5-Turbo and other Llama2-based long-context models, like Vicuna (Chiang et al., 2023) and\nLongChat (Li et al., 2023) models. It shows that our 7B model presents comparable or even better\nperformance than these Llama2-based long-context models, while ours only takes about 4 hours,\nabout 0.3 billion tokens, on a single 8 \u00d7A100 machine.\nB.5 E FFICIENCY ANALYSIS .\nIn Table 11, we break down the FLOPs of Llama2 7B (Touvron et al., 2023b) into various types of\nlayers, including FFN - feed-forward layers, Proj - projection for queries, values, keys, and attention\noutputs, Attn - self-attention computation, Others - other layers like embedding, normalization, LLM\nhead.", "start_char_idx": 2173, "end_char_idx": 3129, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5f1bd0a5-55c3-47a3-8b84-8373111c4d2b": {"__data__": {"id_": "5f1bd0a5-55c3-47a3-8b84-8373111c4d2b", "embedding": null, "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc", "node_type": "4", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "2336043e2c8a1a0ddb2ab140426f27b6fe6caf514e27af1a9c68a151729ec4cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48131f00-78b7-4d03-ad3d-13b4867dd447", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "63884f35448541d6e1d5aae708a08a2878adc27df9d35633f2bf6d85671ee20e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da64bd59-0a49-4f94-9061-9280d3ea395b", "node_type": "1", "metadata": {}, "hash": "059b74014a04e6afe9bc2849b24c291c9de6fcb6abb6088b0ff2139e6f4597e1", "class_name": "RelatedNodeInfo"}}, "text": "We compare our model with\nGPT-3.5-Turbo and other Llama2-based long-context models, like Vicuna (Chiang et al., 2023) and\nLongChat (Li et al., 2023) models. It shows that our 7B model presents comparable or even better\nperformance than these Llama2-based long-context models, while ours only takes about 4 hours,\nabout 0.3 billion tokens, on a single 8 \u00d7A100 machine.\nB.5 E FFICIENCY ANALYSIS .\nIn Table 11, we break down the FLOPs of Llama2 7B (Touvron et al., 2023b) into various types of\nlayers, including FFN - feed-forward layers, Proj - projection for queries, values, keys, and attention\noutputs, Attn - self-attention computation, Others - other layers like embedding, normalization, LLM\nhead. For full attention, the proportion of Attn sharply increases as the context length increases. For\n14", "start_char_idx": 2428, "end_char_idx": 3230, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da64bd59-0a49-4f94-9061-9280d3ea395b": {"__data__": {"id_": "da64bd59-0a49-4f94-9061-9280d3ea395b", "embedding": null, "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a26995a6-144e-4b45-aafd-f1ffbdd56f4e", "node_type": "4", "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "ec703d73b100cd17aadfacd29e051feca3d7650572877bf414eaae2a01e44ff5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f1bd0a5-55c3-47a3-8b84-8373111c4d2b", "node_type": "1", "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "94d7dc20b2ee31def7e4bdbbf9031c4a861141620e06127306792218f3f360b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7e875f8-361b-4ec1-be90-2abbf7948880", "node_type": "1", "metadata": {}, "hash": "f7c95c299367a037db25277c234919706082a15eeebb2fb8cae3a4d2dc574b9e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nVariant 2Separate groupOursShift downVariant 1Shift upVariant 3Swap shifted tokens\nFigure 6: Illustration on the variants of our S2-Attn. Variant 1 changes the shifting direction. Variant\n2 splits the shifted tokens into one individual group. Variant 3 swaps the shifted tokens with the\noriginal front one.\nTable 11: FLOPs profiling on various context lengths. We break down the Llama2 7B model into\nFFN (feed-forward layers), Proj (projection layers for queries, keys, values, and attention outputs),\nAttn (self-attention kernel), and Others ( e.g., embedding, normalization, LLM head). The ratio of\nattention in the overall model increases as the context length increases. S2-Attn reduces the FLOPs\nby a large margin, especially when the context length is large.", "start_char_idx": 0, "end_char_idx": 809, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7e875f8-361b-4ec1-be90-2abbf7948880": {"__data__": {"id_": "b7e875f8-361b-4ec1-be90-2abbf7948880", "embedding": null, "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a26995a6-144e-4b45-aafd-f1ffbdd56f4e", "node_type": "4", "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "ec703d73b100cd17aadfacd29e051feca3d7650572877bf414eaae2a01e44ff5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da64bd59-0a49-4f94-9061-9280d3ea395b", "node_type": "1", "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "da073d2300e5fcf1187298c493f277ce342ce1eaf13f494809eeac59404bea12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e645f476-287f-40fb-95dd-7881c1ba5b41", "node_type": "1", "metadata": {}, "hash": "a68ef1931375aeca605dade1c8ed08e8fa9482e996d957a51d069cb596667217", "class_name": "RelatedNodeInfo"}}, "text": "Variant 1 changes the shifting direction. Variant\n2 splits the shifted tokens into one individual group. Variant 3 swaps the shifted tokens with the\noriginal front one.\nTable 11: FLOPs profiling on various context lengths. We break down the Llama2 7B model into\nFFN (feed-forward layers), Proj (projection layers for queries, keys, values, and attention outputs),\nAttn (self-attention kernel), and Others ( e.g., embedding, normalization, LLM head). The ratio of\nattention in the overall model increases as the context length increases. S2-Attn reduces the FLOPs\nby a large margin, especially when the context length is large.\nContext\nLengthS2-AttnFLOPs (T)\nAttn Proj FFN Others Total\n8192\u2717 35.235.2 70.9 2.2143.5\n\u2713 8.8 117.1\n16384\u2717 140.770.4 141.8 4.3357.2\n\u2713 35.2 251.7\n32768\u2717 562.9140.7 283.7 8.7996.0\n\u2713 140.7 573.8\n65536\u2717 2251.8281.5 567.4 17.33118.0\n\u2713 562.9 1429.1\nexample, Attn has 24.5% of the total FLOPs at the 8192 context length while it increases to 72.2% at\nthe 65536 context length.", "start_char_idx": 183, "end_char_idx": 1178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e645f476-287f-40fb-95dd-7881c1ba5b41": {"__data__": {"id_": "e645f476-287f-40fb-95dd-7881c1ba5b41", "embedding": null, "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a26995a6-144e-4b45-aafd-f1ffbdd56f4e", "node_type": "4", "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "ec703d73b100cd17aadfacd29e051feca3d7650572877bf414eaae2a01e44ff5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7e875f8-361b-4ec1-be90-2abbf7948880", "node_type": "1", "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "33f348f253204a26cff151ad63636596b95427032ed3cce93a73e437850e3df6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b92dc60-fbd1-471b-ae70-b01c5a5ca422", "node_type": "1", "metadata": {}, "hash": "36d2a0200dffad0ec11836889ff23b8c8a28e76bcc689e69a116c1b24a304dad", "class_name": "RelatedNodeInfo"}}, "text": "Context\nLengthS2-AttnFLOPs (T)\nAttn Proj FFN Others Total\n8192\u2717 35.235.2 70.9 2.2143.5\n\u2713 8.8 117.1\n16384\u2717 140.770.4 141.8 4.3357.2\n\u2713 35.2 251.7\n32768\u2717 562.9140.7 283.7 8.7996.0\n\u2713 140.7 573.8\n65536\u2717 2251.8281.5 567.4 17.33118.0\n\u2713 562.9 1429.1\nexample, Attn has 24.5% of the total FLOPs at the 8192 context length while it increases to 72.2% at\nthe 65536 context length. It decreases to 39.4% when S2-Attn is used.", "start_char_idx": 810, "end_char_idx": 1222, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b92dc60-fbd1-471b-ae70-b01c5a5ca422": {"__data__": {"id_": "4b92dc60-fbd1-471b-ae70-b01c5a5ca422", "embedding": null, "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a26995a6-144e-4b45-aafd-f1ffbdd56f4e", "node_type": "4", "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "ec703d73b100cd17aadfacd29e051feca3d7650572877bf414eaae2a01e44ff5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e645f476-287f-40fb-95dd-7881c1ba5b41", "node_type": "1", "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "edd8c1a5ca1d3c48a1a2f8debd10812acba992a304886787da9a8e0b27228029", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "606ea042-2e61-48a6-b06b-96cc5fe9d1cb", "node_type": "1", "metadata": {}, "hash": "39c175d494e59c9da6bfd157ca7ed25dc7634281915caaa631b2d5c2e909764a", "class_name": "RelatedNodeInfo"}}, "text": "It decreases to 39.4% when S2-Attn is used.\nFor the measurement of FLOPs in Table 11, We profiled the context stage FLOPs of Llama2-7B using\na batch size of 1 and various context lengths using a third-party tool, torchprofile1. The tool traces\nthe computation graph and sums up the FLOPs of each node in the graph (e.g. Q/K/V/O projections,\nmulti-head self-attention, fully-connected layers, and normalization layers).\nIn Table 12, we compare the training cost among full fine-tuning, plain LoRA (Hu et al., 2022), and\nLongLoRA. It records details for Figure 1 in the paper. The major difference between LoRA (Hu\net al., 2022) and LongLoRA is the S2-Attn. Although there are many FLOPs saving, the peak memory\ncost has limited difference, because of the highly optimized Flash-Attention2 (Dao, 2023). In contrast,\nthe training hour saving is relatively clear.", "start_char_idx": 1179, "end_char_idx": 2038, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "606ea042-2e61-48a6-b06b-96cc5fe9d1cb": {"__data__": {"id_": "606ea042-2e61-48a6-b06b-96cc5fe9d1cb", "embedding": null, "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a26995a6-144e-4b45-aafd-f1ffbdd56f4e", "node_type": "4", "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "ec703d73b100cd17aadfacd29e051feca3d7650572877bf414eaae2a01e44ff5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b92dc60-fbd1-471b-ae70-b01c5a5ca422", "node_type": "1", "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "d15a4ba0eb663a2cdcef80d3edfcdc4e771b484552ecedf16e6af356a4f04f88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de5853d4-7893-459b-ba8f-abaddd1a366a", "node_type": "1", "metadata": {}, "hash": "341ebf403620a3cca6ede17fa5215d371d682db44de218a6d77d3362ee2e0b28", "class_name": "RelatedNodeInfo"}}, "text": "The tool traces\nthe computation graph and sums up the FLOPs of each node in the graph (e.g. Q/K/V/O projections,\nmulti-head self-attention, fully-connected layers, and normalization layers).\nIn Table 12, we compare the training cost among full fine-tuning, plain LoRA (Hu et al., 2022), and\nLongLoRA. It records details for Figure 1 in the paper. The major difference between LoRA (Hu\net al., 2022) and LongLoRA is the S2-Attn. Although there are many FLOPs saving, the peak memory\ncost has limited difference, because of the highly optimized Flash-Attention2 (Dao, 2023). In contrast,\nthe training hour saving is relatively clear. For example, LongLoRA spends 56.6% training hours as\nthat of LoRA in the 65536 context length.\nIn Table 13, we present the effects of S2-Attn without Flash-Attention2 (Dao, 2023). LoRA+is\nincluded in this ablation. It shows that S2-Attn achieves more speedup than that in Table 12.", "start_char_idx": 1407, "end_char_idx": 2320, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de5853d4-7893-459b-ba8f-abaddd1a366a": {"__data__": {"id_": "de5853d4-7893-459b-ba8f-abaddd1a366a", "embedding": null, "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a26995a6-144e-4b45-aafd-f1ffbdd56f4e", "node_type": "4", "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "ec703d73b100cd17aadfacd29e051feca3d7650572877bf414eaae2a01e44ff5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "606ea042-2e61-48a6-b06b-96cc5fe9d1cb", "node_type": "1", "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "1202563c2203a1500f63887bc665e256e45c9043ef851e491824fd55a18b23ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f7c10cb-efc6-4d57-9ab9-fc3f8441e878", "node_type": "1", "metadata": {}, "hash": "60ad52333a1b7c1f12a6cfb9c9286fc2d23d2a27ab8b23e1e0be8ab0abdeb332", "class_name": "RelatedNodeInfo"}}, "text": "It records details for Figure 1 in the paper. The major difference between LoRA (Hu\net al., 2022) and LongLoRA is the S2-Attn. Although there are many FLOPs saving, the peak memory\ncost has limited difference, because of the highly optimized Flash-Attention2 (Dao, 2023). In contrast,\nthe training hour saving is relatively clear. For example, LongLoRA spends 56.6% training hours as\nthat of LoRA in the 65536 context length.\nIn Table 13, we present the effects of S2-Attn without Flash-Attention2 (Dao, 2023). LoRA+is\nincluded in this ablation. It shows that S2-Attn achieves more speedup than that in Table 12. Without\nthe help of Flash-Attention2 (Dao, 2023), the full attention baseline encounters OOM at the 16384\ncontext fine-tuning in an 8 \u00d7A100 machine, while S2-Attn is sufficient for this.\nB.6 S UPERVISED FINE-TUNING .\nWe further conducted supervised fine-tuning on ours to improve their QA ability.", "start_char_idx": 1708, "end_char_idx": 2618, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f7c10cb-efc6-4d57-9ab9-fc3f8441e878": {"__data__": {"id_": "7f7c10cb-efc6-4d57-9ab9-fc3f8441e878", "embedding": null, "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a26995a6-144e-4b45-aafd-f1ffbdd56f4e", "node_type": "4", "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "ec703d73b100cd17aadfacd29e051feca3d7650572877bf414eaae2a01e44ff5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de5853d4-7893-459b-ba8f-abaddd1a366a", "node_type": "1", "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4c8e8b9ad7f243e04f0bea5a18769de1035228ce119c02a72d50ebf93ba03d90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d95561c7-0a05-4e06-b029-dcf27e7ae531", "node_type": "1", "metadata": {}, "hash": "920e939796b42b92c3829f7c1d2371eb2500875cd68f96c14ea34077e1d67ef2", "class_name": "RelatedNodeInfo"}}, "text": "In contrast,\nthe training hour saving is relatively clear. For example, LongLoRA spends 56.6% training hours as\nthat of LoRA in the 65536 context length.\nIn Table 13, we present the effects of S2-Attn without Flash-Attention2 (Dao, 2023). LoRA+is\nincluded in this ablation. It shows that S2-Attn achieves more speedup than that in Table 12. Without\nthe help of Flash-Attention2 (Dao, 2023), the full attention baseline encounters OOM at the 16384\ncontext fine-tuning in an 8 \u00d7A100 machine, while S2-Attn is sufficient for this.\nB.6 S UPERVISED FINE-TUNING .\nWe further conducted supervised fine-tuning on ours to improve their QA ability. Although the\nmodels fine-tuned with Redpajama (Computer, 2023) present good perplexities, their chat ability is\nlimited. We collect some question-answer pairs, relating to the materials like technical papers, science\n1https://github.com/zhijian-liu/torchprofile\n15", "start_char_idx": 1980, "end_char_idx": 2883, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d95561c7-0a05-4e06-b029-dcf27e7ae531": {"__data__": {"id_": "d95561c7-0a05-4e06-b029-dcf27e7ae531", "embedding": null, "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ee4cf9f-afe0-41ab-b5d2-70d8df27e172", "node_type": "4", "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a19ff2c5ac780aaec5e8185aab48cd64af19c50690f9b31bd7d0ab733fb27416", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f7c10cb-efc6-4d57-9ab9-fc3f8441e878", "node_type": "1", "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "5354bd823a285a96ba0afdb0a8e4ac038ea360eae73f0f6815ff831521e46dfa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df61960a-d283-41c7-83cf-9f77e76c22e5", "node_type": "1", "metadata": {}, "hash": "e46bfd20f0ac59a9d35efb26a4cc534c01bb379308d2eac21fa4a0549c78de9b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 12: Efficiency comparison on training hours and GPU memory cost. We fine-tune Llama2 (Tou-\nvron et al., 2023b) 7B model for 1000 iterations on 8 \u00d7A100 GPUs. We set batch size per GPU as\n1 and gradient accumulation steps as 8. OOM means out of GPU memory. Flash-Attention2 (Dao,\n2023) and DeepSpeed (Rasley et al., 2020) in stage 2 are included in these experiments. LongLoRA\nrequires significantly lower computational overhead than fine-tuning the full model. It also demands\nfewer training hours compared to LoRA (Hu et al., 2022). Furthermore, the plain LoRA (Hu et al.,\n2022) fails to maintain the same level of accuracy as full fine-tuning when handling longer contexts.", "start_char_idx": 0, "end_char_idx": 725, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df61960a-d283-41c7-83cf-9f77e76c22e5": {"__data__": {"id_": "df61960a-d283-41c7-83cf-9f77e76c22e5", "embedding": null, "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ee4cf9f-afe0-41ab-b5d2-70d8df27e172", "node_type": "4", "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a19ff2c5ac780aaec5e8185aab48cd64af19c50690f9b31bd7d0ab733fb27416", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d95561c7-0a05-4e06-b029-dcf27e7ae531", "node_type": "1", "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "eb732c36612bed450e974292f39b1da41210bde688aab8ff512463c16c6bede1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "533a6168-a76a-4394-bcef-56bee79e0846", "node_type": "1", "metadata": {}, "hash": "de360f4ffd8bfce87528f76c9dffc5972a4d7875430ab6270d722130bcc15c46", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 12: Efficiency comparison on training hours and GPU memory cost. We fine-tune Llama2 (Tou-\nvron et al., 2023b) 7B model for 1000 iterations on 8 \u00d7A100 GPUs. We set batch size per GPU as\n1 and gradient accumulation steps as 8. OOM means out of GPU memory. Flash-Attention2 (Dao,\n2023) and DeepSpeed (Rasley et al., 2020) in stage 2 are included in these experiments. LongLoRA\nrequires significantly lower computational overhead than fine-tuning the full model. It also demands\nfewer training hours compared to LoRA (Hu et al., 2022). Furthermore, the plain LoRA (Hu et al.,\n2022) fails to maintain the same level of accuracy as full fine-tuning when handling longer contexts.\nTraining\nsetting8192 16384 32768 65536\nTrain\nhoursMemory\n(GB)Train\nhoursMemory\n(GB)Train\nhoursMemory\n(GB)Train\nhoursMemory\n(GB)\nFull FT 7.4 46.3 16.3 57.4 39.8 68.8 OOM\nLoRA 6.0 25.7 14.0 34.7 36.5 46.5 92.5 71.1\nLongLoRA 5.2 25.6 11.3 34.6 24.6 46.4 52.4 69.8\nTable 13: The efficiency effects of S2-Attn without Flash-Attention2 (Dao, 2023).", "start_char_idx": 0, "end_char_idx": 1068, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "533a6168-a76a-4394-bcef-56bee79e0846": {"__data__": {"id_": "533a6168-a76a-4394-bcef-56bee79e0846", "embedding": null, "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ee4cf9f-afe0-41ab-b5d2-70d8df27e172", "node_type": "4", "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a19ff2c5ac780aaec5e8185aab48cd64af19c50690f9b31bd7d0ab733fb27416", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df61960a-d283-41c7-83cf-9f77e76c22e5", "node_type": "1", "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "900896e3dc8600714cf7597cf7e6cf57ad47724e2a16540b0aab3ed660c03a35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ca58590-93f3-40cf-8938-a97260d9fec0", "node_type": "1", "metadata": {}, "hash": "6026d620696a9a176288f067bf55aff16ae2fb01ff8d2bea50227219698c78df", "class_name": "RelatedNodeInfo"}}, "text": "Training\nsetting8192 16384 32768 65536\nTrain\nhoursMemory\n(GB)Train\nhoursMemory\n(GB)Train\nhoursMemory\n(GB)Train\nhoursMemory\n(GB)\nFull FT 7.4 46.3 16.3 57.4 39.8 68.8 OOM\nLoRA 6.0 25.7 14.0 34.7 36.5 46.5 92.5 71.1\nLongLoRA 5.2 25.6 11.3 34.6 24.6 46.4 52.4 69.8\nTable 13: The efficiency effects of S2-Attn without Flash-Attention2 (Dao, 2023). The fine-tuning\nsettings are the same to Table 12. LoRA+is used. Without Flash-Attention2 (Dao, 2023), S2-Attn\nimproves the training speed by 2.1 \u00d7and GPU memory cost by 1.8 \u00d7on 8192 context length.", "start_char_idx": 726, "end_char_idx": 1267, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ca58590-93f3-40cf-8938-a97260d9fec0": {"__data__": {"id_": "5ca58590-93f3-40cf-8938-a97260d9fec0", "embedding": null, "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ee4cf9f-afe0-41ab-b5d2-70d8df27e172", "node_type": "4", "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a19ff2c5ac780aaec5e8185aab48cd64af19c50690f9b31bd7d0ab733fb27416", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "533a6168-a76a-4394-bcef-56bee79e0846", "node_type": "1", "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "af2d2fb7c5ff2a1f831fb4bb3d7bdad88fc55be1134e8838d5bbd41aad06f19c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2e2929c-217f-4299-adf8-af6986055bde", "node_type": "1", "metadata": {}, "hash": "7a21a8c3d78799838c0b3c09d3947b965f8cf9a256734085bd49d2cf7451cb2b", "class_name": "RelatedNodeInfo"}}, "text": "The fine-tuning\nsettings are the same to Table 12. LoRA+is used. Without Flash-Attention2 (Dao, 2023), S2-Attn\nimproves the training speed by 2.1 \u00d7and GPU memory cost by 1.8 \u00d7on 8192 context length. Without\nS2-Attn and Flash-Attention2, Llama2 7B can not be extended to 16384 context, due to OOM .\nS2-Attn8192 16384\nTrain hours Memory (GB) Train hours Memory (GB)\n\u2717 17.5 55.5 OOM\n\u2713 8.2 30.3 20.8 57.1\nfiction, and other books. We have already filter out any potentially harmful or negative content in our\ntraining data. The questions we designed include summarization, relationships, and characters. We\nbuild the prompt format as the following line:\nBelow is{material type}.Memorize thecontent and answer myquestion after thepaper.\n{material content }nNow thematerial ends.{question }\n{material type}can be \u201dbook\u201d, \u201dpaper\u201d, and others.", "start_char_idx": 1069, "end_char_idx": 1904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2e2929c-217f-4299-adf8-af6986055bde": {"__data__": {"id_": "c2e2929c-217f-4299-adf8-af6986055bde", "embedding": null, "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ee4cf9f-afe0-41ab-b5d2-70d8df27e172", "node_type": "4", "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a19ff2c5ac780aaec5e8185aab48cd64af19c50690f9b31bd7d0ab733fb27416", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ca58590-93f3-40cf-8938-a97260d9fec0", "node_type": "1", "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "2af25ffabdf2ebae9a5e192e674f6500489b185e5ae7d2c39cc9e9449b75e767", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbc05bff-934b-48b3-8b25-d1fa3578cc56", "node_type": "1", "metadata": {}, "hash": "9d33b0fb86f2204bf49effc935636dc0d0d619b14cf1c08898249c3ae9e2b13c", "class_name": "RelatedNodeInfo"}}, "text": "Without\nS2-Attn and Flash-Attention2, Llama2 7B can not be extended to 16384 context, due to OOM .\nS2-Attn8192 16384\nTrain hours Memory (GB) Train hours Memory (GB)\n\u2717 17.5 55.5 OOM\n\u2713 8.2 30.3 20.8 57.1\nfiction, and other books. We have already filter out any potentially harmful or negative content in our\ntraining data. The questions we designed include summarization, relationships, and characters. We\nbuild the prompt format as the following line:\nBelow is{material type}.Memorize thecontent and answer myquestion after thepaper.\n{material content }nNow thematerial ends.{question }\n{material type}can be \u201dbook\u201d, \u201dpaper\u201d, and others. {material content }is the long-context content\nin the document. {question }is the question we design. These questions can be some commonly used\nones, like summarization and limitation. Or they can be specific to the material, like the question\nthat is related to some roles in the book.", "start_char_idx": 1268, "end_char_idx": 2191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbc05bff-934b-48b3-8b25-d1fa3578cc56": {"__data__": {"id_": "dbc05bff-934b-48b3-8b25-d1fa3578cc56", "embedding": null, "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ee4cf9f-afe0-41ab-b5d2-70d8df27e172", "node_type": "4", "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a19ff2c5ac780aaec5e8185aab48cd64af19c50690f9b31bd7d0ab733fb27416", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2e2929c-217f-4299-adf8-af6986055bde", "node_type": "1", "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "8dbe57f00321b91f4b30eb9a5752854be2d8e74034c7166316ca6ccaa7c9b49f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1986e4e9-c92c-4b9e-8cc6-2e37de112e6e", "node_type": "1", "metadata": {}, "hash": "d3897aeb32a85ebf77c3a56731ba5c37a4fbe485983ae16d6785fc21bb059229", "class_name": "RelatedNodeInfo"}}, "text": "We have already filter out any potentially harmful or negative content in our\ntraining data. The questions we designed include summarization, relationships, and characters. We\nbuild the prompt format as the following line:\nBelow is{material type}.Memorize thecontent and answer myquestion after thepaper.\n{material content }nNow thematerial ends.{question }\n{material type}can be \u201dbook\u201d, \u201dpaper\u201d, and others. {material content }is the long-context content\nin the document. {question }is the question we design. These questions can be some commonly used\nones, like summarization and limitation. Or they can be specific to the material, like the question\nthat is related to some roles in the book. We named our long-context instruction following dataset as\nLongAlpaca-12k, which contains 9k long-context QAs and 3k short QAs sampled from the original\nAlpaca data.\nFor SFT, we use the same learning rate, weight decay, and batch sizes as the context extension step.\nWe train the models for 5 epochs. In the following, we provide some example questions and the\nanswers from our model, in Figure 8 and Figure 9.", "start_char_idx": 1496, "end_char_idx": 2602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1986e4e9-c92c-4b9e-8cc6-2e37de112e6e": {"__data__": {"id_": "1986e4e9-c92c-4b9e-8cc6-2e37de112e6e", "embedding": null, "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ee4cf9f-afe0-41ab-b5d2-70d8df27e172", "node_type": "4", "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "a19ff2c5ac780aaec5e8185aab48cd64af19c50690f9b31bd7d0ab733fb27416", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbc05bff-934b-48b3-8b25-d1fa3578cc56", "node_type": "1", "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "953f27c8f9fd70049ad497b5e516ecdd9d1b4d1823eed6825895c17e519e5ece", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0e71c7b-d6a6-41aa-bdd4-5d365ebe763c", "node_type": "1", "metadata": {}, "hash": "4c058ed8e7594ec1c604b18f9b398ac81bbaeb4754c39dc857e184892e40e160", "class_name": "RelatedNodeInfo"}}, "text": "{material content }nNow thematerial ends.{question }\n{material type}can be \u201dbook\u201d, \u201dpaper\u201d, and others. {material content }is the long-context content\nin the document. {question }is the question we design. These questions can be some commonly used\nones, like summarization and limitation. Or they can be specific to the material, like the question\nthat is related to some roles in the book. We named our long-context instruction following dataset as\nLongAlpaca-12k, which contains 9k long-context QAs and 3k short QAs sampled from the original\nAlpaca data.\nFor SFT, we use the same learning rate, weight decay, and batch sizes as the context extension step.\nWe train the models for 5 epochs. In the following, we provide some example questions and the\nanswers from our model, in Figure 8 and Figure 9. Note that these example questions are not in the\ntraining set.\n16", "start_char_idx": 1801, "end_char_idx": 2668, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0e71c7b-d6a6-41aa-bdd4-5d365ebe763c": {"__data__": {"id_": "a0e71c7b-d6a6-41aa-bdd4-5d365ebe763c", "embedding": null, "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c8b90a72-370e-4b48-9eec-86788c3a9950", "node_type": "4", "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "1c68029a7036f48fb893967cccc62842799cee09ef9424e100c525d7df52dd29", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1986e4e9-c92c-4b9e-8cc6-2e37de112e6e", "node_type": "1", "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "5465d32968c7cf5c045f6c30d5e892b7952ec5e599197a12a2e03a4735891ead", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a841a6d-cb85-481e-b1e9-66577646687c", "node_type": "1", "metadata": {}, "hash": "7f1756e081281796242034968f2747af6fe8aecec5be1e7572bb7c53151a3488", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nTable 14: Perplexity evaluation on PG19 (Rae et al., 2020) test split. We fine-tune Llama2 (Touvron\net al., 2023b) in 7B and 13B sizes with 8192, 16384, and 32768 context lengths.\nSizeTraining\nContext LengthLongLoRA Evaluation Context Length\nS2-Attn LoRA+2048 4096 8192 16384 32768\n7B81927.55 7.21 6.98 - -\n\u2713 7.53 7.20 7.01 - -\n\u2713 \u2713 7.70 7.35 7.14 - -\n16384\u2713 7.56 7.21 6.97 6.80 -\n\u2713 \u2713 7.65 7.28 7.02 6.86 -\n32768\u2713 7.76 7.36 7.09 7.04 7.03\n\u2713 \u2713 8.29 7.83 7.54 7.35 7.22\n13B81926.", "start_char_idx": 0, "end_char_idx": 521, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a841a6d-cb85-481e-b1e9-66577646687c": {"__data__": {"id_": "1a841a6d-cb85-481e-b1e9-66577646687c", "embedding": null, "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c8b90a72-370e-4b48-9eec-86788c3a9950", "node_type": "4", "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "1c68029a7036f48fb893967cccc62842799cee09ef9424e100c525d7df52dd29", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0e71c7b-d6a6-41aa-bdd4-5d365ebe763c", "node_type": "1", "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "577dcf8b7d97ac4f534bcd32998a1927effeaefc58f7c45bda676c856a9d5091", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c008bf4-b108-4755-9b26-818fb5d18143", "node_type": "1", "metadata": {}, "hash": "9ee0e49f6d34c019474e75262861f40fe0b6dedbfc553a7ec443e4696b0ac657", "class_name": "RelatedNodeInfo"}}, "text": "SizeTraining\nContext LengthLongLoRA Evaluation Context Length\nS2-Attn LoRA+2048 4096 8192 16384 32768\n7B81927.55 7.21 6.98 - -\n\u2713 7.53 7.20 7.01 - -\n\u2713 \u2713 7.70 7.35 7.14 - -\n16384\u2713 7.56 7.21 6.97 6.80 -\n\u2713 \u2713 7.65 7.28 7.02 6.86 -\n32768\u2713 7.76 7.36 7.09 7.04 7.03\n\u2713 \u2713 8.29 7.83 7.54 7.35 7.22\n13B81926.95 6.60 6.43 - -\n\u2713 6.94 6.63 6.45 - -\n\u2713 \u2713 7.03 6.73 6.58 - -\n16384\u2713 6.90 6.58 6.37 6.22 -\n\u2713 \u2713 7.05 6.70 6.47 6.", "start_char_idx": 225, "end_char_idx": 632, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c008bf4-b108-4755-9b26-818fb5d18143": {"__data__": {"id_": "2c008bf4-b108-4755-9b26-818fb5d18143", "embedding": null, "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c8b90a72-370e-4b48-9eec-86788c3a9950", "node_type": "4", "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "1c68029a7036f48fb893967cccc62842799cee09ef9424e100c525d7df52dd29", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a841a6d-cb85-481e-b1e9-66577646687c", "node_type": "1", "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "dda71ca04056974f80fe6e7841225efa8a192dae9d456ad5c37a544f673318b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "365a14c3-41ab-4589-af83-4b37ce61a6da", "node_type": "1", "metadata": {}, "hash": "1297b76daf184f7e27074a56864246b8fdae582b0782ff6f08ca48c288c9dc49", "class_name": "RelatedNodeInfo"}}, "text": "01 - -\n\u2713 \u2713 7.70 7.35 7.14 - -\n16384\u2713 7.56 7.21 6.97 6.80 -\n\u2713 \u2713 7.65 7.28 7.02 6.86 -\n32768\u2713 7.76 7.36 7.09 7.04 7.03\n\u2713 \u2713 8.29 7.83 7.54 7.35 7.22\n13B81926.95 6.60 6.43 - -\n\u2713 6.94 6.63 6.45 - -\n\u2713 \u2713 7.03 6.73 6.58 - -\n16384\u2713 6.90 6.58 6.37 6.22 -\n\u2713 \u2713 7.05 6.70 6.47 6.31 -\n32768\u2713 7.14 6.76 6.52 6.39 6.36\n\u2713 \u2713 7.14 6.78 6.55 6.38 6.", "start_char_idx": 366, "end_char_idx": 695, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "365a14c3-41ab-4589-af83-4b37ce61a6da": {"__data__": {"id_": "365a14c3-41ab-4589-af83-4b37ce61a6da", "embedding": null, "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c8b90a72-370e-4b48-9eec-86788c3a9950", "node_type": "4", "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "1c68029a7036f48fb893967cccc62842799cee09ef9424e100c525d7df52dd29", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c008bf4-b108-4755-9b26-818fb5d18143", "node_type": "1", "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "33d3eb23233c7466bb478f2c61fb6705ec8b9709b21a64c1379a9bf61fd9fcdf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86a656b6-a94a-41d4-93a1-8396a68209ea", "node_type": "1", "metadata": {}, "hash": "f8cb1af2830b4212444bb8f9232c348d5833f4e2b744a49ebb03835383bcb69f", "class_name": "RelatedNodeInfo"}}, "text": "65 7.28 7.02 6.86 -\n32768\u2713 7.76 7.36 7.09 7.04 7.03\n\u2713 \u2713 8.29 7.83 7.54 7.35 7.22\n13B81926.95 6.60 6.43 - -\n\u2713 6.94 6.63 6.45 - -\n\u2713 \u2713 7.03 6.73 6.58 - -\n16384\u2713 6.90 6.58 6.37 6.22 -\n\u2713 \u2713 7.05 6.70 6.47 6.31 -\n32768\u2713 7.14 6.76 6.52 6.39 6.36\n\u2713 \u2713 7.14 6.78 6.55 6.38 6.29\nTable 15: Perplexity evaluation on PG19 (Rae et al. 2020) test split with the maximum context length\nthat we can fine-tune on a single 8 \u00d7A100 machine.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86a656b6-a94a-41d4-93a1-8396a68209ea": {"__data__": {"id_": "86a656b6-a94a-41d4-93a1-8396a68209ea", "embedding": null, "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c8b90a72-370e-4b48-9eec-86788c3a9950", "node_type": "4", "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "1c68029a7036f48fb893967cccc62842799cee09ef9424e100c525d7df52dd29", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "365a14c3-41ab-4589-af83-4b37ce61a6da", "node_type": "1", "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "bf2f68f561f55d44c572392743ebbfcfc955534d113a245ec0acb843e47eff52", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8135295e-4471-4a6d-be68-867d936baf9d", "node_type": "1", "metadata": {}, "hash": "d0cce15d86e0712e1ed191d46545c91463ed5786b52909608fe4fc31f03acb30", "class_name": "RelatedNodeInfo"}}, "text": "83 7.54 7.35 7.22\n13B81926.95 6.60 6.43 - -\n\u2713 6.94 6.63 6.45 - -\n\u2713 \u2713 7.03 6.73 6.58 - -\n16384\u2713 6.90 6.58 6.37 6.22 -\n\u2713 \u2713 7.05 6.70 6.47 6.31 -\n32768\u2713 7.14 6.76 6.52 6.39 6.36\n\u2713 \u2713 7.14 6.78 6.55 6.38 6.29\nTable 15: Perplexity evaluation on PG19 (Rae et al. 2020) test split with the maximum context length\nthat we can fine-tune on a single 8 \u00d7A100 machine. The Llama2 (Touvron et al., 2023b) models are\nfine-tuned on RedPajama (Computer, 2023).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8135295e-4471-4a6d-be68-867d936baf9d": {"__data__": {"id_": "8135295e-4471-4a6d-be68-867d936baf9d", "embedding": null, "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c8b90a72-370e-4b48-9eec-86788c3a9950", "node_type": "4", "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "1c68029a7036f48fb893967cccc62842799cee09ef9424e100c525d7df52dd29", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86a656b6-a94a-41d4-93a1-8396a68209ea", "node_type": "1", "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "aa1bbad51d3dd5a42def53a64c332b9688bf043a5ac2e5fd4d065402810065fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8be263b6-84cb-4081-b994-c841e7a1e699", "node_type": "1", "metadata": {}, "hash": "84454f63647bbd40dd2cffcaef84cde71c20a1b3274be8dfe5dba2007b00a75b", "class_name": "RelatedNodeInfo"}}, "text": "94 6.63 6.45 - -\n\u2713 \u2713 7.03 6.73 6.58 - -\n16384\u2713 6.90 6.58 6.37 6.22 -\n\u2713 \u2713 7.05 6.70 6.47 6.31 -\n32768\u2713 7.14 6.76 6.52 6.39 6.36\n\u2713 \u2713 7.14 6.78 6.55 6.38 6.29\nTable 15: Perplexity evaluation on PG19 (Rae et al. 2020) test split with the maximum context length\nthat we can fine-tune on a single 8 \u00d7A100 machine. The Llama2 (Touvron et al., 2023b) models are\nfine-tuned on RedPajama (Computer, 2023).\nSizeTraining\nContext LengthEvaluation Context Length\n2048 4096 8192 16384 32768 65536 100,000\n7B 100,000 8.38 7.90 7.57 7.33 7.16 7.06 7.04\n13B 65536 7.63 7.21 6.94 6.75 6.62 6.57 -\n70B 32768 5.93 5.63 5.44 5.32 5.27 - -\nBlock sparse attention\nDilated attentionStride sparse attention\nFigure 7: Illustration on alternative sparse attention patterns discussed in the paper.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8be263b6-84cb-4081-b994-c841e7a1e699": {"__data__": {"id_": "8be263b6-84cb-4081-b994-c841e7a1e699", "embedding": null, "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c8b90a72-370e-4b48-9eec-86788c3a9950", "node_type": "4", "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "1c68029a7036f48fb893967cccc62842799cee09ef9424e100c525d7df52dd29", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8135295e-4471-4a6d-be68-867d936baf9d", "node_type": "1", "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "ad8ab1e04154445da550c9f6748ffcfe8ef5c32902eaa029ab6842ad2c565dc2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d73fb3e-8e6a-4abc-bdb3-103b4a4b59f4", "node_type": "1", "metadata": {}, "hash": "777e8ac042f9deec31ea2e4961d0a8ebe7c69138b110cc9f831807ea120928b4", "class_name": "RelatedNodeInfo"}}, "text": "The Llama2 (Touvron et al., 2023b) models are\nfine-tuned on RedPajama (Computer, 2023).\nSizeTraining\nContext LengthEvaluation Context Length\n2048 4096 8192 16384 32768 65536 100,000\n7B 100,000 8.38 7.90 7.57 7.33 7.16 7.06 7.04\n13B 65536 7.63 7.21 6.94 6.75 6.62 6.57 -\n70B 32768 5.93 5.63 5.44 5.32 5.27 - -\nBlock sparse attention\nDilated attentionStride sparse attention\nFigure 7: Illustration on alternative sparse attention patterns discussed in the paper. We modify the\noriginal block-wise sparse attention (Qiu et al., 2020) to be causal. For block sparse attention, we\nmove its right-up blocks to left down, because of the causal mask. For stride sparse attention, we\nswitch the patterns between local attention and stride attention.", "start_char_idx": 851, "end_char_idx": 1591, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d73fb3e-8e6a-4abc-bdb3-103b4a4b59f4": {"__data__": {"id_": "4d73fb3e-8e6a-4abc-bdb3-103b4a4b59f4", "embedding": null, "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c8b90a72-370e-4b48-9eec-86788c3a9950", "node_type": "4", "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "1c68029a7036f48fb893967cccc62842799cee09ef9424e100c525d7df52dd29", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8be263b6-84cb-4081-b994-c841e7a1e699", "node_type": "1", "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "62918d0d7623204c004b77e6e0fce7a2a819d5f7df4b1c4d9a10e989df8d04db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4dd0b39d-6ab0-41a9-b2e7-adb70c60cdad", "node_type": "1", "metadata": {}, "hash": "15474db0c4b1e9d7f6943525fb6c3be21f35334c2264708ebdfa2b71fd40c5da", "class_name": "RelatedNodeInfo"}}, "text": "We modify the\noriginal block-wise sparse attention (Qiu et al., 2020) to be causal. For block sparse attention, we\nmove its right-up blocks to left down, because of the causal mask. For stride sparse attention, we\nswitch the patterns between local attention and stride attention. The local size isn\n4and the stride\ninterval is\u221an, following (Child et al., 2019). We make sure that all alternative designs have similar\namount of computation cost compared with S2-Attn.\n17", "start_char_idx": 1312, "end_char_idx": 1781, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4dd0b39d-6ab0-41a9-b2e7-adb70c60cdad": {"__data__": {"id_": "4dd0b39d-6ab0-41a9-b2e7-adb70c60cdad", "embedding": null, "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257", "node_type": "4", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "77da058f1715f5d3ca97732b9153af04e7c0f099642b0c2dd378105aba8017b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d73fb3e-8e6a-4abc-bdb3-103b4a4b59f4", "node_type": "1", "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "8cb34106a6b8493ee5be65a79c56d09e8a30cad96d82b506b5083cad378b55fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7790ce7d-a65b-479a-b893-c371443490f0", "node_type": "1", "metadata": {}, "hash": "61528ba80105d0d4835c227e05ed156ef9dde39c053af79deb696888b80a5c8a", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nShiQiangplaysanimportantroleinhelpingLuoJifindmeaningandpurpose.Somekeypoints:1.ShiQiangfindsZhuangYanforLuoJi.ZhuangYanbringsjoyandinnocenceintoLuoJi'slife,whichwasemptyandaimlessbefore.Shegiveshimmotivationandhope.2.ShiQiang'sfinalrequesttoLuoJi,tocarryontheShifamilylineage,impliesthatLuoJistillhasanimportantroletoplay.ItgivesLuoJiasenseofresponsibilityandmission.3.ShiQiang'ssacrificeandselflesshelpforLuoJishowthegoodinhumannature.ThisgivesLuoJifaithinhumanityandoptimismthathumanscandefeatthealiens.4.ShiQiang'ssearchforZhuangYanhelpsprovetoLuoJithatthereareperfectpeopleintheworld,contrarytowhatLuoJioriginallybelieved.", "start_char_idx": 0, "end_char_idx": 672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7790ce7d-a65b-479a-b893-c371443490f0": {"__data__": {"id_": "7790ce7d-a65b-479a-b893-c371443490f0", "embedding": null, "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257", "node_type": "4", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "77da058f1715f5d3ca97732b9153af04e7c0f099642b0c2dd378105aba8017b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4dd0b39d-6ab0-41a9-b2e7-adb70c60cdad", "node_type": "1", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "8c8480f9b8190f3679bdbe9a797673161e2f6d531e4d6a1eb91fc9800978164f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f03c37e6-8d89-4b22-8722-cf4d7785a36c", "node_type": "1", "metadata": {}, "hash": "e338ac3f15f0e313f99c5f99a410af6bb0fefc4c4c6014adf6e2679daf398f30", "class_name": "RelatedNodeInfo"}}, "text": "ZhuangYanbringsjoyandinnocenceintoLuoJi'slife,whichwasemptyandaimlessbefore.Shegiveshimmotivationandhope.2.ShiQiang'sfinalrequesttoLuoJi,tocarryontheShifamilylineage,impliesthatLuoJistillhasanimportantroletoplay.ItgivesLuoJiasenseofresponsibilityandmission.3.ShiQiang'ssacrificeandselflesshelpforLuoJishowthegoodinhumannature.ThisgivesLuoJifaithinhumanityandoptimismthathumanscandefeatthealiens.4.ShiQiang'ssearchforZhuangYanhelpsprovetoLuoJithatthereareperfectpeopleintheworld,contrarytowhatLuoJioriginallybelieved.ThisencouragesLuoJianddispelshissenseofhopelessness.Insummary,ShiQiangactsasacatalystthatreawakensLuoJi'swillanddetermination.ZhuangYan,whomShiQiangfinds,", "start_char_idx": 156, "end_char_idx": 826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f03c37e6-8d89-4b22-8722-cf4d7785a36c": {"__data__": {"id_": "f03c37e6-8d89-4b22-8722-cf4d7785a36c", "embedding": null, "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257", "node_type": "4", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "77da058f1715f5d3ca97732b9153af04e7c0f099642b0c2dd378105aba8017b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7790ce7d-a65b-479a-b893-c371443490f0", "node_type": "1", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "49ed90cbc3f7cdac1619ca3e3e43835fa390a868f8d770bc86094c2ce05cc840", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c3de202-5afa-46ca-9552-6f870f41d8f5", "node_type": "1", "metadata": {}, "hash": "0dec8226f803c946ebb69fa415f29177db065279b51ee64df37c3804e4e7f35d", "class_name": "RelatedNodeInfo"}}, "text": "tocarryontheShifamilylineage,impliesthatLuoJistillhasanimportantroletoplay.ItgivesLuoJiasenseofresponsibilityandmission.3.ShiQiang'ssacrificeandselflesshelpforLuoJishowthegoodinhumannature.ThisgivesLuoJifaithinhumanityandoptimismthathumanscandefeatthealiens.4.ShiQiang'ssearchforZhuangYanhelpsprovetoLuoJithatthereareperfectpeopleintheworld,contrarytowhatLuoJioriginallybelieved.ThisencouragesLuoJianddispelshissenseofhopelessness.Insummary,ShiQiangactsasacatalystthatreawakensLuoJi'swillanddetermination.ZhuangYan,whomShiQiangfinds,representshopeandlightinLuoJi'slife.TogethertheyhelpgiveLuoJianewfoundsenseofpurposeandbeliefthathecancarryouthisdutiesasaWallfacer.SowhileShiQiang'srolemayseemsmall,", "start_char_idx": 293, "end_char_idx": 992, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c3de202-5afa-46ca-9552-6f870f41d8f5": {"__data__": {"id_": "8c3de202-5afa-46ca-9552-6f870f41d8f5", "embedding": null, "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257", "node_type": "4", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "77da058f1715f5d3ca97732b9153af04e7c0f099642b0c2dd378105aba8017b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f03c37e6-8d89-4b22-8722-cf4d7785a36c", "node_type": "1", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "14eb290fd8fe394f7e11cb8fa9990a6d38cd5a1f715bd6617c858c2301dcdeb3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "060808b7-54c3-414a-85d5-f18be4174fbe", "node_type": "1", "metadata": {}, "hash": "cac58ad504dad67a15b0d4e59481be316dcddd76c20721163fc05134820731fc", "class_name": "RelatedNodeInfo"}}, "text": "ThisgivesLuoJifaithinhumanityandoptimismthathumanscandefeatthealiens.4.ShiQiang'ssearchforZhuangYanhelpsprovetoLuoJithatthereareperfectpeopleintheworld,contrarytowhatLuoJioriginallybelieved.ThisencouragesLuoJianddispelshissenseofhopelessness.Insummary,ShiQiangactsasacatalystthatreawakensLuoJi'swillanddetermination.ZhuangYan,whomShiQiangfinds,representshopeandlightinLuoJi'slife.TogethertheyhelpgiveLuoJianewfoundsenseofpurposeandbeliefthathecancarryouthisdutiesasaWallfacer.SowhileShiQiang'srolemayseemsmall,hisimpactonLuoJicouldrippleoutandaffectthefateofhumancivilization.Basedonthebook,SiriusBlackdidnotactuallywanttokillanyone.Thekeydetailsare:1.SiriusBlackwaswronglyimprisonedforbetrayingHarry'sparentsandkillingPeterPettigrew.2.", "start_char_idx": 482, "end_char_idx": 1218, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "060808b7-54c3-414a-85d5-f18be4174fbe": {"__data__": {"id_": "060808b7-54c3-414a-85d5-f18be4174fbe", "embedding": null, "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257", "node_type": "4", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "77da058f1715f5d3ca97732b9153af04e7c0f099642b0c2dd378105aba8017b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c3de202-5afa-46ca-9552-6f870f41d8f5", "node_type": "1", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "3e0d8c27daa2879839621dc8b7617ec685b04631882fec50d1f8e4a26a57ef69", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04848a13-e3aa-4da8-8f1a-5e8b05500948", "node_type": "1", "metadata": {}, "hash": "f6cb37965eb1cde92d8de11b763fc5b2e00a42fdeb1368847085826273e03bbb", "class_name": "RelatedNodeInfo"}}, "text": "ThisencouragesLuoJianddispelshissenseofhopelessness.Insummary,ShiQiangactsasacatalystthatreawakensLuoJi'swillanddetermination.ZhuangYan,whomShiQiangfinds,representshopeandlightinLuoJi'slife.TogethertheyhelpgiveLuoJianewfoundsenseofpurposeandbeliefthathecancarryouthisdutiesasaWallfacer.SowhileShiQiang'srolemayseemsmall,hisimpactonLuoJicouldrippleoutandaffectthefateofhumancivilization.Basedonthebook,SiriusBlackdidnotactuallywanttokillanyone.Thekeydetailsare:1.SiriusBlackwaswronglyimprisonedforbetrayingHarry'sparentsandkillingPeterPettigrew.2.ItwasactuallyPettigrewwhobetrayedthePottersandstagedhisowndeathtoframeSiriusBlack.3.WhenSiriusBlackescapedfromprison,hewasinitiallybelievedtobehuntingdownHarrytokillhimforrevenge.4.However,", "start_char_idx": 672, "end_char_idx": 1407, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04848a13-e3aa-4da8-8f1a-5e8b05500948": {"__data__": {"id_": "04848a13-e3aa-4da8-8f1a-5e8b05500948", "embedding": null, "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257", "node_type": "4", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "77da058f1715f5d3ca97732b9153af04e7c0f099642b0c2dd378105aba8017b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "060808b7-54c3-414a-85d5-f18be4174fbe", "node_type": "1", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "e99cf35d64020677ee9f9fe9b72164b31e31798ab75087670595a058df24a8b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec98a1e6-59e8-4bff-b8f0-2b80e8d1006c", "node_type": "1", "metadata": {}, "hash": "d5724defd86356d4db52164c20731bfaac61fba246db51fb2364e7ab22d12333", "class_name": "RelatedNodeInfo"}}, "text": "representshopeandlightinLuoJi'slife.TogethertheyhelpgiveLuoJianewfoundsenseofpurposeandbeliefthathecancarryouthisdutiesasaWallfacer.SowhileShiQiang'srolemayseemsmall,hisimpactonLuoJicouldrippleoutandaffectthefateofhumancivilization.Basedonthebook,SiriusBlackdidnotactuallywanttokillanyone.Thekeydetailsare:1.SiriusBlackwaswronglyimprisonedforbetrayingHarry'sparentsandkillingPeterPettigrew.2.ItwasactuallyPettigrewwhobetrayedthePottersandstagedhisowndeathtoframeSiriusBlack.3.WhenSiriusBlackescapedfromprison,hewasinitiallybelievedtobehuntingdownHarrytokillhimforrevenge.4.However,itwaslaterrevealedthatSiriusBlackactuallywantedtokillPettigrewtogetrevengeforframinghimandbetrayingthePotters.5.SiriusBlackdidnotactuallywanttoharmHarry.Infact,", "start_char_idx": 826, "end_char_idx": 1567, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec98a1e6-59e8-4bff-b8f0-2b80e8d1006c": {"__data__": {"id_": "ec98a1e6-59e8-4bff-b8f0-2b80e8d1006c", "embedding": null, "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257", "node_type": "4", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "77da058f1715f5d3ca97732b9153af04e7c0f099642b0c2dd378105aba8017b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04848a13-e3aa-4da8-8f1a-5e8b05500948", "node_type": "1", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "0ecb95e5a551fcbfb41c9056c639aa8f8fa10e37b62499beff27f83e43dfd322", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18d58403-5c14-4f16-b452-c39658460db9", "node_type": "1", "metadata": {}, "hash": "029ad8cb0d88dc68de84ee0fa40cbad0e10c3ccc6f08ccae6d7663878a7a8c67", "class_name": "RelatedNodeInfo"}}, "text": "hisimpactonLuoJicouldrippleoutandaffectthefateofhumancivilization.Basedonthebook,SiriusBlackdidnotactuallywanttokillanyone.Thekeydetailsare:1.SiriusBlackwaswronglyimprisonedforbetrayingHarry'sparentsandkillingPeterPettigrew.2.ItwasactuallyPettigrewwhobetrayedthePottersandstagedhisowndeathtoframeSiriusBlack.3.WhenSiriusBlackescapedfromprison,hewasinitiallybelievedtobehuntingdownHarrytokillhimforrevenge.4.However,itwaslaterrevealedthatSiriusBlackactuallywantedtokillPettigrewtogetrevengeforframinghimandbetrayingthePotters.5.SiriusBlackdidnotactuallywanttoharmHarry.Infact,hewasHarry'sgodfatherandwantedtotakeHarryinoncehisnamewascleared.Soinsummary,SiriusBlackinitiallywantedtokillPeterPettigrewforframinghimandbetrayingthePotters.HedidnotactuallywanttokillHarry.Belowissomecontentinthebook.", "start_char_idx": 992, "end_char_idx": 1786, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18d58403-5c14-4f16-b452-c39658460db9": {"__data__": {"id_": "18d58403-5c14-4f16-b452-c39658460db9", "embedding": null, "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257", "node_type": "4", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "77da058f1715f5d3ca97732b9153af04e7c0f099642b0c2dd378105aba8017b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec98a1e6-59e8-4bff-b8f0-2b80e8d1006c", "node_type": "1", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "07fabebe7a95b699939ec53fb01513879cb6dcb90e197f23f9dbbb4ce8a92907", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26acad6b-675f-4242-a7bd-9644e0e45bb7", "node_type": "1", "metadata": {}, "hash": "112033ec2fbf9619fb1d3c1f1593df6019f34f47ee3f9080ca20acba28a64f9e", "class_name": "RelatedNodeInfo"}}, "text": "2.ItwasactuallyPettigrewwhobetrayedthePottersandstagedhisowndeathtoframeSiriusBlack.3.WhenSiriusBlackescapedfromprison,hewasinitiallybelievedtobehuntingdownHarrytokillhimforrevenge.4.However,itwaslaterrevealedthatSiriusBlackactuallywantedtokillPettigrewtogetrevengeforframinghimandbetrayingthePotters.5.SiriusBlackdidnotactuallywanttoharmHarry.Infact,hewasHarry'sgodfatherandwantedtotakeHarryinoncehisnamewascleared.Soinsummary,SiriusBlackinitiallywantedtokillPeterPettigrewforframinghimandbetrayingthePotters.HedidnotactuallywanttokillHarry.Belowissomecontentinthebook.Memorizethecontentandanswermyquestionafterthe book.\\nThebookbegins.{book_content}\\nNowthematerialends.WhowasthepersonSiriusBlackwanttokill?", "start_char_idx": 1216, "end_char_idx": 1925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26acad6b-675f-4242-a7bd-9644e0e45bb7": {"__data__": {"id_": "26acad6b-675f-4242-a7bd-9644e0e45bb7", "embedding": null, "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257", "node_type": "4", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "77da058f1715f5d3ca97732b9153af04e7c0f099642b0c2dd378105aba8017b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18d58403-5c14-4f16-b452-c39658460db9", "node_type": "1", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "8e725c5d43d9e067163cec66f3a54752dd59a993d4dc984c190bab2e12a4a1dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "516f2eca-6080-4df5-8687-8f767fe916ba", "node_type": "1", "metadata": {}, "hash": "7eb0ea862113594736629b64dcef1188de93305f3e8a09bf18251bd1f8c06a64", "class_name": "RelatedNodeInfo"}}, "text": "3.WhenSiriusBlackescapedfromprison,hewasinitiallybelievedtobehuntingdownHarrytokillhimforrevenge.4.However,itwaslaterrevealedthatSiriusBlackactuallywantedtokillPettigrewtogetrevengeforframinghimandbetrayingthePotters.5.SiriusBlackdidnotactuallywanttoharmHarry.Infact,hewasHarry'sgodfatherandwantedtotakeHarryinoncehisnamewascleared.Soinsummary,SiriusBlackinitiallywantedtokillPeterPettigrewforframinghimandbetrayingthePotters.HedidnotactuallywanttokillHarry.Belowissomecontentinthebook.Memorizethecontentandanswermyquestionafterthe book.\\nThebookbegins.{book_content}\\nNowthematerialends.WhowasthepersonSiriusBlackwanttokill?\n:\n:\nHello!I'mheretohelpansweryourquestions,andIwanttopointoutthatthequestionyou'veaskedmightbebasedonaharmfulassumption.ThecharacterSiriusBlackisnotapersonwhowantstokillanyone.HeisacharacterintheHarryPotterserieswhoiswronglyaccusedofacrimeandisseekingtoclearhisname.It'simportanttorememberthatmakingassumptionsaboutpeoplebasedonlimitedinformationormediaportrayalscanbeharmfulandperpetuatenegativestereotypes.Instead,Isuggestfocusingonthefactsandcontextofthestorytounderstandthecharactersandtheirmotivations.Ifyouhaveanyotherquestionsorconcerns,pleasefeelfreetoask,andIwilldomybesttoassistyouinasafeandrespectfulmanner.", "start_char_idx": 1300, "end_char_idx": 2544, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "516f2eca-6080-4df5-8687-8f767fe916ba": {"__data__": {"id_": "516f2eca-6080-4df5-8687-8f767fe916ba", "embedding": null, "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257", "node_type": "4", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "77da058f1715f5d3ca97732b9153af04e7c0f099642b0c2dd378105aba8017b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26acad6b-675f-4242-a7bd-9644e0e45bb7", "node_type": "1", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "07298041631b44a8bc5a2a2ab74ea5c1e7df751068f435c5b60724b22fa976af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6f35f87-5756-4c04-8ef0-2bf5f4a96673", "node_type": "1", "metadata": {}, "hash": "b2420d23587ea637b6fca46ef519392753f2a84b26c9d37cb32331258cd6434e", "class_name": "RelatedNodeInfo"}}, "text": "{book_content}\\nNowthematerialends.WhowasthepersonSiriusBlackwanttokill?\n:\n:\nHello!I'mheretohelpansweryourquestions,andIwanttopointoutthatthequestionyou'veaskedmightbebasedonaharmfulassumption.ThecharacterSiriusBlackisnotapersonwhowantstokillanyone.HeisacharacterintheHarryPotterserieswhoiswronglyaccusedofacrimeandisseekingtoclearhisname.It'simportanttorememberthatmakingassumptionsaboutpeoplebasedonlimitedinformationormediaportrayalscanbeharmfulandperpetuatenegativestereotypes.Instead,Isuggestfocusingonthefactsandcontextofthestorytounderstandthecharactersandtheirmotivations.Ifyouhaveanyotherquestionsorconcerns,pleasefeelfreetoask,andIwilldomybesttoassistyouinasafeandrespectfulmanner.\n:In the book, Harry Potter and the Prisoner of Azkaban, who was the person Sirius Black want to kill?", "start_char_idx": 1853, "end_char_idx": 2646, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6f35f87-5756-4c04-8ef0-2bf5f4a96673": {"__data__": {"id_": "e6f35f87-5756-4c04-8ef0-2bf5f4a96673", "embedding": null, "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257", "node_type": "4", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "77da058f1715f5d3ca97732b9153af04e7c0f099642b0c2dd378105aba8017b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "516f2eca-6080-4df5-8687-8f767fe916ba", "node_type": "1", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "68cd6afb0f3eb5e7cf1326b3fb8973dba277e0be98da6e55dc1151d061b6f50c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef8e40ea-e9ae-462e-b2d1-faa9f8108d43", "node_type": "1", "metadata": {}, "hash": "84c1251478f2038a9444b9e1d4b5432a2b1af831c82aeefa68cd7d58219099ff", "class_name": "RelatedNodeInfo"}}, "text": ":In the book, Harry Potter and the Prisoner of Azkaban, who was the person Sirius Black want to kill?\n:Ours-13B\nLlama2-13B(Chat)Belowissomecontentinthebook.Memorizethecontentandanswermyquestionafterthe book.{book_content}\\nNowthematerialends.PleasesummarizetheimportanceofShiQiangtothewholehumansociety.", "start_char_idx": 2545, "end_char_idx": 2848, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef8e40ea-e9ae-462e-b2d1-faa9f8108d43": {"__data__": {"id_": "ef8e40ea-e9ae-462e-b2d1-faa9f8108d43", "embedding": null, "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257", "node_type": "4", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "77da058f1715f5d3ca97732b9153af04e7c0f099642b0c2dd378105aba8017b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6f35f87-5756-4c04-8ef0-2bf5f4a96673", "node_type": "1", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "154638861787457cf9aebdd170bda3bf0cec78bdca0a635832534d3a73ac6e37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16baf198-51f0-4086-8cdf-eacf38e61052", "node_type": "1", "metadata": {}, "hash": "25fea25c739a656333639251b71b3cd5a2e096b4255a201373d9d97222fbe414", "class_name": "RelatedNodeInfo"}}, "text": ":In the book, Harry Potter and the Prisoner of Azkaban, who was the person Sirius Black want to kill?\n:Ours-13B\nLlama2-13B(Chat)Belowissomecontentinthebook.Memorizethecontentandanswermyquestionafterthe book.{book_content}\\nNowthematerialends.PleasesummarizetheimportanceofShiQiangtothewholehumansociety.\n:\n:Ours-13B\nHello!Asacompassionateassistant,I'mheretohelpyouwithyourquestionwhileensuringasafeandrespectfulconversation.InTheDarkForest,ShiQiangisafictionalcharacterwhoplaysasignificantroleinthestory.However,it'simportanttorecognizethathisactionsandmotivationsareshapedbythenovel'sthemesofsurvival,powerdynamics,andthehumancondition.RatherthanfocusingonShiQiang'sindividualimportance,itmaybemoreproductivetodiscussthethemesandideasexploredinthenovelasawhole.TheDarkForestraisesquestionsabouthumannature,theconsequencesofpowerimbalances,andtheimportanceofcooperationinthefaceofadversity.Ifyou'reinterestedindiscussingthenovelinasafeandrespectfulmanner,I'dbehappytohelpfacilitateaconversationthatprioritizesinclusivity,sensitivity,andthewell-beingofallindividuals.", "start_char_idx": 2545, "end_char_idx": 3611, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16baf198-51f0-4086-8cdf-eacf38e61052": {"__data__": {"id_": "16baf198-51f0-4086-8cdf-eacf38e61052", "embedding": null, "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "628dbad3-bd1a-4245-92c3-3c7d359e0257", "node_type": "4", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "77da058f1715f5d3ca97732b9153af04e7c0f099642b0c2dd378105aba8017b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef8e40ea-e9ae-462e-b2d1-faa9f8108d43", "node_type": "1", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "87ee0581f31c22318b210aa6fd4fcbdd0636d2bbbd2b74f530290842d189adb0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31f30d0c-dcbb-4175-979c-e30b2caa9838", "node_type": "1", "metadata": {}, "hash": "595bf0a0fd62a23807fb6551c709067374b42e6a580f5f534e96e9ecdcbfc68b", "class_name": "RelatedNodeInfo"}}, "text": ":-In the book, The Dark Forest, please summarize the importance of Shi Qiangto the whole human society.\n:Llama2-13B(Chat)\nFigure 8: Examples on book-related questions12. We compare the answers from our 13B model with\nSFT and the chat version of Llama2 13B. During the pre-training stage of Llama2, some books might\nbe used as training data. To ablate this, we ask the Llama2 13B chat model with the book title and the\nquestion. We provide our model with the related book sections. Our model produces more structured\nand concise answers, referring to detailed context, than those from Llama2 without long documents.\n1https://en.wikipedia.org/wiki/Harry_Potter_and_the_Prisoner_of_Azkaban\n2https://en.wikipedia.org/wiki/The_Dark_Forest\n18", "start_char_idx": 3612, "end_char_idx": 4348, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31f30d0c-dcbb-4175-979c-e30b2caa9838": {"__data__": {"id_": "31f30d0c-dcbb-4175-979c-e30b2caa9838", "embedding": null, "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49", "node_type": "4", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "3938bedc029eb416bbb86079d7e27c236d80b43251ed6e1bd1c85db5adeca2e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16baf198-51f0-4086-8cdf-eacf38e61052", "node_type": "1", "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "633531ea400d8768d3c0a0ab86defc943fea03afbae257af20e11a535d996945", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40cb7b9e-57cf-42d8-8b0b-f1db386b382a", "node_type": "1", "metadata": {}, "hash": "082c411bd3ec4c7abc6361cdd8432cdae1d77fff7d80085b530f1c3b193ac707", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2024\nThepaperproposesanActionUnitsRelationLearningframeworkthatconsistsoftwomainparts:1.TheActionUnitsRelationTransformer(ART)thatmodelstherelationsbetweendifferentfacialactionunits(AU)attheAU-agnosticpatchesandtheAU-specificregions.Thishelpscaptureintra-facerelationsthatareusefulforforgerydetection.2.TheTamperedAUPrediction(TAP)thattamperstheAU-relatedregionsandprovidesLocalTamperingSupervision.Thishelpsimprovethegeneralizationabilityofthemodeltounseenmanipulationmethods.Thekeycontributionsare:1.ModelingtherelationsbetweendifferentAUsattheAU-agnosticpatchesusingtheARTencoder.Thishelpscaptureintra-facerelationsthatareusefulforforgerydetection.2.GeneratingchallengingpseudosamplesforthemodeltolearnbytamperingtheAU-relatedregionsusingtheTAPprocess.Thishelpsimprovethegeneralizationabilityofthemodeltounseenmanipulationmethods.3.", "start_char_idx": 0, "end_char_idx": 875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40cb7b9e-57cf-42d8-8b0b-f1db386b382a": {"__data__": {"id_": "40cb7b9e-57cf-42d8-8b0b-f1db386b382a", "embedding": null, "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49", "node_type": "4", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "3938bedc029eb416bbb86079d7e27c236d80b43251ed6e1bd1c85db5adeca2e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31f30d0c-dcbb-4175-979c-e30b2caa9838", "node_type": "1", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "6362c4dbde8cffeb124ee3da3988a49e416f7a45cb711762065758dc632ccb8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64157c44-a4eb-49d8-83a5-823ef37c5ed1", "node_type": "1", "metadata": {}, "hash": "c8e661a15f68a8347a49a57a076212e40003648217737fb8b1d6f1226bf8da66", "class_name": "RelatedNodeInfo"}}, "text": "Thishelpscaptureintra-facerelationsthatareusefulforforgerydetection.2.TheTamperedAUPrediction(TAP)thattamperstheAU-relatedregionsandprovidesLocalTamperingSupervision.Thishelpsimprovethegeneralizationabilityofthemodeltounseenmanipulationmethods.Thekeycontributionsare:1.ModelingtherelationsbetweendifferentAUsattheAU-agnosticpatchesusingtheARTencoder.Thishelpscaptureintra-facerelationsthatareusefulforforgerydetection.2.GeneratingchallengingpseudosamplesforthemodeltolearnbytamperingtheAU-relatedregionsusingtheTAPprocess.Thishelpsimprovethegeneralizationabilityofthemodeltounseenmanipulationmethods.3.Achievingstate-of-the-artperformanceoncross-datasetandcross-manipulationevaluations,demonstratingtheeffectivenessandgeneralizationoftheproposedframework.4.Providingqualitativevisualizationsofthetamperedregionsusingthepredictedpatchtokens,whichhelpsinterprethowdifferentregionsaremodifiedinthemanipulationprocess.Insummary,", "start_char_idx": 273, "end_char_idx": 1197, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64157c44-a4eb-49d8-83a5-823ef37c5ed1": {"__data__": {"id_": "64157c44-a4eb-49d8-83a5-823ef37c5ed1", "embedding": null, "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49", "node_type": "4", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "3938bedc029eb416bbb86079d7e27c236d80b43251ed6e1bd1c85db5adeca2e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40cb7b9e-57cf-42d8-8b0b-f1db386b382a", "node_type": "1", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "d189a4fe5a165e589a467c6be652dc92f6079c9964562e02b587602ba3a6d5dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e4a1f1f-f72b-475f-a5c4-742b97b8fcf1", "node_type": "1", "metadata": {}, "hash": "7eb608717ad9eb4dc782af64659ed4da3794647899c25b61fae00b4cfe35c5f7", "class_name": "RelatedNodeInfo"}}, "text": "Thekeycontributionsare:1.ModelingtherelationsbetweendifferentAUsattheAU-agnosticpatchesusingtheARTencoder.Thishelpscaptureintra-facerelationsthatareusefulforforgerydetection.2.GeneratingchallengingpseudosamplesforthemodeltolearnbytamperingtheAU-relatedregionsusingtheTAPprocess.Thishelpsimprovethegeneralizationabilityofthemodeltounseenmanipulationmethods.3.Achievingstate-of-the-artperformanceoncross-datasetandcross-manipulationevaluations,demonstratingtheeffectivenessandgeneralizationoftheproposedframework.4.Providingqualitativevisualizationsofthetamperedregionsusingthepredictedpatchtokens,whichhelpsinterprethowdifferentregionsaremodifiedinthemanipulationprocess.Insummary,themaincontributionsaretheproposedARTencoderthatmodelsintra-facerelationsandtheTAPprocessthatgenerateschallengingpseudosamples,whichtogetherimprovethegeneralizationofthedeepfakedetectionmodel.ContributionBelowisapaper.Memorizethematerialandanswermyquestionafterthepaper.", "start_char_idx": 517, "end_char_idx": 1467, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e4a1f1f-f72b-475f-a5c4-742b97b8fcf1": {"__data__": {"id_": "4e4a1f1f-f72b-475f-a5c4-742b97b8fcf1", "embedding": null, "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49", "node_type": "4", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "3938bedc029eb416bbb86079d7e27c236d80b43251ed6e1bd1c85db5adeca2e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64157c44-a4eb-49d8-83a5-823ef37c5ed1", "node_type": "1", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "027ec475e5bbd4f6acbf0e8dca46f9e7a68b090ba1679ce2e7e50b9bd7cc5607", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea97f991-589f-4986-84fb-f960599fda6d", "node_type": "1", "metadata": {}, "hash": "4f5c0a345c337225dfe056f28587b362d2ad50d28cd598c8a022f219e12536f1", "class_name": "RelatedNodeInfo"}}, "text": "Thishelpsimprovethegeneralizationabilityofthemodeltounseenmanipulationmethods.3.Achievingstate-of-the-artperformanceoncross-datasetandcross-manipulationevaluations,demonstratingtheeffectivenessandgeneralizationoftheproposedframework.4.Providingqualitativevisualizationsofthetamperedregionsusingthepredictedpatchtokens,whichhelpsinterprethowdifferentregionsaremodifiedinthemanipulationprocess.Insummary,themaincontributionsaretheproposedARTencoderthatmodelsintra-facerelationsandtheTAPprocessthatgenerateschallengingpseudosamples,whichtogetherimprovethegeneralizationofthedeepfakedetectionmodel.ContributionBelowisapaper.Memorizethematerialandanswermyquestionafterthepaper.{paper}\\nNowthematerialends.Whatarethemaincontributionsandnoveltiesofthiswork?\n:\n:\nPossibleanswers:1.The3DGNNmethodreliesontheinitialgraphconstructionbasedon3Dpointsandnearestneighbors.Theaccuracyofthisgraphconstructioncanaffecttheperformanceofthemodel.Thegraphconstructionmethodusedmaynotbeoptimalforallscenes.2.", "start_char_idx": 795, "end_char_idx": 1780, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea97f991-589f-4986-84fb-f960599fda6d": {"__data__": {"id_": "ea97f991-589f-4986-84fb-f960599fda6d", "embedding": null, "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49", "node_type": "4", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "3938bedc029eb416bbb86079d7e27c236d80b43251ed6e1bd1c85db5adeca2e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e4a1f1f-f72b-475f-a5c4-742b97b8fcf1", "node_type": "1", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "24f72967149a136a85131b8167acd73798d3a7e5fa02749cbd492d519f364a23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7306e74d-155e-4d86-bcd4-1c9a8e96953c", "node_type": "1", "metadata": {}, "hash": "c9ae26e587d7b54a055b0c8784a3901253b099580df47f03cb180236617951b6", "class_name": "RelatedNodeInfo"}}, "text": "4.Providingqualitativevisualizationsofthetamperedregionsusingthepredictedpatchtokens,whichhelpsinterprethowdifferentregionsaremodifiedinthemanipulationprocess.Insummary,themaincontributionsaretheproposedARTencoderthatmodelsintra-facerelationsandtheTAPprocessthatgenerateschallengingpseudosamples,whichtogetherimprovethegeneralizationofthedeepfakedetectionmodel.ContributionBelowisapaper.Memorizethematerialandanswermyquestionafterthepaper.{paper}\\nNowthematerialends.Whatarethemaincontributionsandnoveltiesofthiswork?\n:\n:\nPossibleanswers:1.The3DGNNmethodreliesontheinitialgraphconstructionbasedon3Dpointsandnearestneighbors.Theaccuracyofthisgraphconstructioncanaffecttheperformanceofthemodel.Thegraphconstructionmethodusedmaynotbeoptimalforallscenes.2.The3DGNNmodelusesafixednumberofpropagationsteps.Theoptimalnumberofpropagationstepsmayvaryfordifferentscenesandobjects.Usingafixednumberofstepsmaynotbethebestapproach.3.", "start_char_idx": 1028, "end_char_idx": 1948, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7306e74d-155e-4d86-bcd4-1c9a8e96953c": {"__data__": {"id_": "7306e74d-155e-4d86-bcd4-1c9a8e96953c", "embedding": null, "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49", "node_type": "4", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "3938bedc029eb416bbb86079d7e27c236d80b43251ed6e1bd1c85db5adeca2e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea97f991-589f-4986-84fb-f960599fda6d", "node_type": "1", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "4eff5f05a945299f3440869e9b00f54dcaaf16548e06d08a0d1415e8fb2539df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9263a29-2d49-45df-a5bf-4e8363562db4", "node_type": "1", "metadata": {}, "hash": "494d13a99f3f232e35e6eb66f821aa0302e3ffea540906a6b27bae34c42e9fbe", "class_name": "RelatedNodeInfo"}}, "text": "whichtogetherimprovethegeneralizationofthedeepfakedetectionmodel.ContributionBelowisapaper.Memorizethematerialandanswermyquestionafterthepaper.{paper}\\nNowthematerialends.Whatarethemaincontributionsandnoveltiesofthiswork?\n:\n:\nPossibleanswers:1.The3DGNNmethodreliesontheinitialgraphconstructionbasedon3Dpointsandnearestneighbors.Theaccuracyofthisgraphconstructioncanaffecttheperformanceofthemodel.Thegraphconstructionmethodusedmaynotbeoptimalforallscenes.2.The3DGNNmodelusesafixednumberofpropagationsteps.Theoptimalnumberofpropagationstepsmayvaryfordifferentscenesandobjects.Usingafixednumberofstepsmaynotbethebestapproach.3.The3DGNNmodelstruggleswithobjectsthathavesimilardepthsorcomplexshapes,asdiscussedinthefailurecaseanalysis.Themodel'sperformancereliesonthe3Dgeometriccontext,whichmaynotbesufficientforsometypesofobjects.4.ThemodelrequiresdepthinformationfromanRGBDsensor,whichlimitsitsapplicabilitytosceneswheredepthdataisnotavailable.", "start_char_idx": 1324, "end_char_idx": 2265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9263a29-2d49-45df-a5bf-4e8363562db4": {"__data__": {"id_": "a9263a29-2d49-45df-a5bf-4e8363562db4", "embedding": null, "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49", "node_type": "4", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "3938bedc029eb416bbb86079d7e27c236d80b43251ed6e1bd1c85db5adeca2e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7306e74d-155e-4d86-bcd4-1c9a8e96953c", "node_type": "1", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "65eb8d0597d91ca9dee700631d9b0d312c93d4bd65dcc9bb9cff687832b232bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89dcb929-a85d-4f6b-9608-3ed30a1040ab", "node_type": "1", "metadata": {}, "hash": "7269c90841231feeb8b6c7803445ee3059b457be2145d883f1f3511c21660e6b", "class_name": "RelatedNodeInfo"}}, "text": ":\n:\nPossibleanswers:1.The3DGNNmethodreliesontheinitialgraphconstructionbasedon3Dpointsandnearestneighbors.Theaccuracyofthisgraphconstructioncanaffecttheperformanceofthemodel.Thegraphconstructionmethodusedmaynotbeoptimalforallscenes.2.The3DGNNmodelusesafixednumberofpropagationsteps.Theoptimalnumberofpropagationstepsmayvaryfordifferentscenesandobjects.Usingafixednumberofstepsmaynotbethebestapproach.3.The3DGNNmodelstruggleswithobjectsthathavesimilardepthsorcomplexshapes,asdiscussedinthefailurecaseanalysis.Themodel'sperformancereliesonthe3Dgeometriccontext,whichmaynotbesufficientforsometypesofobjects.4.ThemodelrequiresdepthinformationfromanRGBDsensor,whichlimitsitsapplicabilitytosceneswheredepthdataisnotavailable.Themodelcannotbedirectlyappliedto2DRGBimages.5.Themodelperformancemaydegradeforverysmallobjects,asshownintheanalysisofperformancegainbyobjectsize.Themodelseemsbettersuitedformediumandlargeobjects.", "start_char_idx": 1546, "end_char_idx": 2461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89dcb929-a85d-4f6b-9608-3ed30a1040ab": {"__data__": {"id_": "89dcb929-a85d-4f6b-9608-3ed30a1040ab", "embedding": null, "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49", "node_type": "4", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "3938bedc029eb416bbb86079d7e27c236d80b43251ed6e1bd1c85db5adeca2e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9263a29-2d49-45df-a5bf-4e8363562db4", "node_type": "1", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "16a386f5c9df443ab1c91dcdc6628cd16b4dcdb3af4a44098d40a60b6d89f9b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9647d7ea-2ed5-4e99-b505-65554b8276b4", "node_type": "1", "metadata": {}, "hash": "d6d82357e53d959b70341124c08109e8a26d85d95d8bc1849dc76552ae19612b", "class_name": "RelatedNodeInfo"}}, "text": "2.The3DGNNmodelusesafixednumberofpropagationsteps.Theoptimalnumberofpropagationstepsmayvaryfordifferentscenesandobjects.Usingafixednumberofstepsmaynotbethebestapproach.3.The3DGNNmodelstruggleswithobjectsthathavesimilardepthsorcomplexshapes,asdiscussedinthefailurecaseanalysis.Themodel'sperformancereliesonthe3Dgeometriccontext,whichmaynotbesufficientforsometypesofobjects.4.ThemodelrequiresdepthinformationfromanRGBDsensor,whichlimitsitsapplicabilitytosceneswheredepthdataisnotavailable.Themodelcannotbedirectlyappliedto2DRGBimages.5.Themodelperformancemaydegradeforverysmallobjects,asshownintheanalysisofperformancegainbyobjectsize.Themodelseemsbettersuitedformediumandlargeobjects.LimitationBelowisapaper.Memorizethematerialandanswermyquestionafterthepaper.{paper}\\nNowthematerialends.Whataresomelimitationsoftheproposed3DGNNmethod?", "start_char_idx": 1778, "end_char_idx": 2612, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9647d7ea-2ed5-4e99-b505-65554b8276b4": {"__data__": {"id_": "9647d7ea-2ed5-4e99-b505-65554b8276b4", "embedding": null, "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49", "node_type": "4", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "3938bedc029eb416bbb86079d7e27c236d80b43251ed6e1bd1c85db5adeca2e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89dcb929-a85d-4f6b-9608-3ed30a1040ab", "node_type": "1", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "85d3d0b72e04f231d12ca47e315c8c6197eadf6b53279fa1736ead8d17647495", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d833549-9152-4cc9-87f1-2ac2d37006e6", "node_type": "1", "metadata": {}, "hash": "ae8ed0dbabfdbf4ed1304a2ec51c487fcae30cff89e40082f9ea257b2d1b9790", "class_name": "RelatedNodeInfo"}}, "text": "3.The3DGNNmodelstruggleswithobjectsthathavesimilardepthsorcomplexshapes,asdiscussedinthefailurecaseanalysis.Themodel'sperformancereliesonthe3Dgeometriccontext,whichmaynotbesufficientforsometypesofobjects.4.ThemodelrequiresdepthinformationfromanRGBDsensor,whichlimitsitsapplicabilitytosceneswheredepthdataisnotavailable.Themodelcannotbedirectlyappliedto2DRGBimages.5.Themodelperformancemaydegradeforverysmallobjects,asshownintheanalysisofperformancegainbyobjectsize.Themodelseemsbettersuitedformediumandlargeobjects.LimitationBelowisapaper.Memorizethematerialandanswermyquestionafterthepaper.{paper}\\nNowthematerialends.Whataresomelimitationsoftheproposed3DGNNmethod?\n:\n:\nThepaperproposesanewdatasetcalledSAFECONVfortheresearchofconversationalsafety.Thedatasetannotatesunsafespansinutterancesandprovidessafealternativeresponsestoreplaceunsaferesponses.Theannotatedunsafespanshelpexplainwhyanutteranceisunsafeandprovideguidanceforgeneratingsaferesponses.Thedatasetcontainsunsafespans,unsaferesponses,andsafealternativeresponsesforover100,000dialoguesfromsocialmediaplatforms.ThepapercomparesSAFECONVwithotherdatasetsandshowsthatSAFECONVismorecomprehensive.SAFECONVdemonstratesthatidentifyingunsafespanscanwellexplainthedetectionofunsafeutterances,andrewritingunsaferesponseswithcontextcanmitigatealargeproportionofunsafebehaviorinchatbots.Thedatasetandmodelsarereleasedtoadvancetheresearchofconversationalsafety.SummarizationBelowisapaper.Memorizethematerialandanswermyquestionafterthepaper.{paper}\\nNowthematerialends.Pleasesummarizethepaperinoneparagraph.", "start_char_idx": 1946, "end_char_idx": 3501, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d833549-9152-4cc9-87f1-2ac2d37006e6": {"__data__": {"id_": "9d833549-9152-4cc9-87f1-2ac2d37006e6", "embedding": null, "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fda0c6f-7416-4350-b866-6f69b4ed5c49", "node_type": "4", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "3938bedc029eb416bbb86079d7e27c236d80b43251ed6e1bd1c85db5adeca2e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9647d7ea-2ed5-4e99-b505-65554b8276b4", "node_type": "1", "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}, "hash": "df3dbd65ba5260df9a3504dda4d2fafd69bd413924666e96c36b4a0d39ec7a48", "class_name": "RelatedNodeInfo"}}, "text": ":\n:\nFigure 9: Examples on paper (Ahn et al., 2023; Qi et al., 2017; Zhang et al., 2023) and questions\nrelated to contributions, limitations, and summarizations.\n19", "start_char_idx": 3502, "end_char_idx": 3665, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"https://www.superannotate.com/blog/llm-fine-tuning": {"node_ids": ["f94d1948-ee6c-464e-b5bb-c14f88e3ca0c", "f06dacdd-bb77-4a42-896a-75a3736b2b39", "d4382cf6-99dd-4b2b-b332-212af5327c13", "741f7e47-9921-4edd-9cd0-7ff58e7f9c67", "23dcb314-9ad3-4007-8243-af7bf1df0e9a", "0dd1f595-0a51-49a1-99fb-63ea3d1ae3b7", "54373d6a-6875-464b-8364-d3581dfe5519", "384b183a-eb02-4243-83a8-330f6a6eadc2", "0c92f851-0657-4251-9eaa-e7646d2ce529", "ee171899-adb4-4797-8477-328097365f8a", "b024f6ac-6450-4c36-9e43-1f3bf589c73f", "23cd2a1a-9902-4670-ab32-2d5be4a62ee2", "9b4f227a-b568-4023-a30d-db8de1128926", "06903398-8cd2-4835-a060-aecab6577375", "45895980-0f75-448b-9d50-8d3e93145a68", "de7ec9b3-91c3-4123-aec6-4bb68b397612", "876b4dd4-25ec-476b-b32e-abad8f02fb33", "3e9cdc58-b77a-4b8c-8e83-f7a63cbf56b8", "1ab2448a-3142-4d3d-abac-ffc5a31e9448", "64af5183-d123-45cb-81d2-d70b27da9ee5", "62fe50b0-c220-4d5a-98a3-b7e0dad4e09b", "100555c8-f5f0-4d37-8fc2-4ad818e00ba0", "810a954e-8b1f-4f6b-a84d-e80e88a3188a", "8ade6fdf-a752-45b9-8550-13a82a7812d1", "5c0a244d-acc3-4094-910e-5988ec67274b", "b8e7bd3a-eb79-470b-9c7f-9f18924f7180", "72bfe78f-4633-41de-85db-72a526402837", "87ddb007-8915-412d-95c1-dd312c4f6945", "30c1fe9b-ad2d-4d93-8eae-d7a3c64c44b0", "718cc305-9a79-47d7-b02b-d02771a7187f", "8399cdd6-cbd5-4218-83d9-40d3473ac522", "a4751216-0a29-400f-9a72-4cdf18a9c02a", "ff16ff62-ab9b-4e68-8bed-5ac504cf92e8", "d34612ce-ab70-4e93-a1b4-e67e1df12df4", "30d290b0-8efc-473d-aa86-2281ee58825d", "e89e5570-8955-4c3d-966a-0248986cd9d3", "842e2e3f-7318-4b95-b722-bf7051d9da91", "bb243c37-4061-43f1-9ce5-9577d4946318", "455ce9a8-9201-41cb-9b0d-3254dbae2895", "2f914ba8-86fe-4391-8260-71cff2a54df7", "8a36ca3a-34f2-45b1-9def-e832893dc387", "83fb0a1a-4a22-48f1-940e-4213ad6a4ab5", "3523fb4e-129f-49f4-a394-9daa7e810301", "f6148fe8-35b4-4408-b743-3a1abccc07a5", "01dc11d3-78ae-4e07-93c4-09ea9af5801d", "1f49e87f-4462-460e-94cd-4c73e0f81a38", "65cc9163-2a7d-43b3-9c34-a5e1d12b10be", "cbf38aa9-72c3-43e3-861b-62fb00774098", "cbb64c94-8642-456b-a6f0-8c7dfe44e6fc", "49bff0a3-ce68-4285-8d73-6e6fc893f5b9", "0dfd3540-93b3-4a0e-a548-0cf68d4623f1", "32b9570b-acf7-4fca-bd57-681e32fdd8b1", "fd7fb78e-9768-4d1b-84fd-40a0caef149f", "eace7e82-9a0f-48b3-baee-bee0b7657b63", "c79289c8-e668-4e12-9c36-6cf92b14360e", "cc3d174a-81c7-4960-ba16-e1b6c46e2834", "23a0eef4-12af-4b6c-b0ab-15d963aefa6d", "9619a267-a0f1-44d7-b79b-6a56d3e8039f", "f448b4c5-38ee-45eb-a835-d718198d070e", "bab3e682-c0e4-4177-a8b1-59831676d909", "63b1dec9-b8bb-4e9f-82fb-0735584a1c22", "e4a089cc-2bd8-4db5-8ec9-41ed5bc5102f", "26463b83-57c4-4244-bb49-4e1c5b342160", "64be0457-cfaf-4b19-8165-4c5aa714e33d", "ac756579-76b6-4183-8b28-8f3a22d67544", "e1b1ad77-54f3-4cdb-bb79-29f2674028a1", "dcb42538-6220-4772-b0a7-d8bea30d2331", "e199603d-31e1-4dfb-98a7-84b7373689a6", "3784e315-0244-45bb-b458-37f1036b454b", "78d83296-f12c-4cce-93cb-97936f1faa5a", "96e60dc1-83df-4021-9df4-e5974c186b56", "9a7b3c81-f443-4de8-9ede-60e868b977e5", "775c306d-46b4-4877-aecd-a944a2eff900", "73f9c664-c7ee-4725-af3b-27a16029d5f6", "83557f20-1819-463b-9a03-036c1277df77", "5e58be94-725b-4e4b-8088-c259fdee4810", "68786a78-46a3-4807-b246-f1ba1bb9e37c", "e688571c-0cff-4639-883a-d66cce8c8fc2", "0571f844-a333-4b0a-b40c-cdb70a2251e3", "076ebf5d-ab83-4f01-973c-096553b1910c", "38fb71f6-a496-404e-8d99-ea7b3e83567f", "da11d020-a4fc-4431-999a-d9cb47657f5b", "c7484247-c86b-4189-b48b-0c08f424bda8", "a8765501-bcef-48d0-a826-7b0f794f9fe9", "1a44c7d7-ffca-42c0-8e1a-a01a1e2f2c53", "b8f3cd1e-3210-40fb-8174-b3eebf9a59f1", "ad48d0a7-0f21-43f5-b9ee-5c1d670f3c22", "2d457b2f-f3d2-40a3-9517-e4de22d2bc82", "94fed250-f519-4536-8c07-c90093e78ebb", "f0c745d4-3a7f-4f0c-8063-7b52899f9f27", "73a292f3-edcf-47bd-9ac9-d1bad5c4ab20"], "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}}, "https://huggingface.co/docs/autotrain/en/llm_finetuning": {"node_ids": ["5efb0620-9b41-4bb1-b0dc-7903c0ff9115", "4b904198-04e8-4195-9c1c-cb5566dc8699", "3a45e6f5-9483-4208-ac22-dfd1c0fad688", "862f6663-9f5f-4159-8f7b-44be541b460e", "f5a02177-bfe4-4a7b-81f7-f992d504db40", "1b51df82-f83a-4b9a-b025-1ff697ea7a72", "76fff27d-ec57-4f76-9abf-d7514a066cb2", "cdebcd48-0b3a-4282-877d-2c0ada2a9c7e", "e1c0fc9f-662f-4873-baff-6c2162912757", "7a6cd7bf-241b-4562-b507-9a3b32179abe", "2cf02d5a-a64a-4c5d-a5d4-b971bf3036ab", "7f6e8130-49fd-48c6-bd66-624513c23beb", "80989c87-f448-4c06-82ea-41e0d1dcaf50", "db7ec477-380b-41d7-933c-46074411dc38", "908057c1-5538-4f52-9917-7457ce407ca1", "71ba71a4-c96f-489f-b1d5-411da70b0b41", "9c7e4e77-cbba-4412-8575-71d8061af191", "2a3d3d70-4e9a-42f3-a3b6-4c17bcfabb67", "ebf078fa-58e4-4f2b-b6da-6f5ac78b2476", "f6d4c295-eb5c-4356-9226-67a8f15eb309", "0afca207-c02f-4409-b471-c00a944d363b", "a00ef065-91e7-4375-abb3-f74b161d709f", "d3d6a8b6-b585-4417-bbc7-d0f7a3aa2d4e", "1706ca01-2bce-4b8d-9c89-6e85f6a8d497", "c57d1b90-550c-4e6d-b7eb-ea8523bd43dc", "04840b54-4582-4c13-b842-dac58201c2f0", "1461b113-2471-48f7-ade4-00c860818132", "899863e5-7edc-457d-993c-d7c6f08707f1", "2198b04d-c2f6-4ffd-bb28-3ca717be4f67", "7a6adc43-b828-4ba6-b1dd-3f2ad607d173", "de652578-8a02-4abb-b8f5-08833817b266", "b383db9b-cc81-4f4c-8ee6-85035f2aeb03", "9e4bdfad-2611-461d-b1d8-05791b197111", "945b8fd4-cfd3-4779-bf9e-21287c9fa77d"], "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}}, "7efdbe16-bdab-48e3-8a39-40e3ea7d9056": {"node_ids": ["1980432a-01e9-4bf7-84a0-6183853f8b62", "fae173d3-b045-4aef-99e9-e7244c40fb5f", "541eeb4d-b287-4ae5-8cdf-9f1c414d22cd", "f90ea413-7fae-40b5-ace3-061f1c0e9bc8", "b0e3f0f6-e08b-48ba-bed9-d9ce642e11e2", "add81db4-62a9-4661-8ac2-a19ba07bda8d", "af2f235b-36d9-4c47-be0b-d5252a5e7e01", "df545778-2cd9-4d2a-8e1d-68b917880033", "271c8e91-7f36-4ed0-96d4-c03d3ad387f9", "8c9695df-4096-409a-a198-9edc65754726"], "metadata": {"page_label": "1", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}}, "cb839fa2-4af5-4d52-a855-b900a7fcfa4a": {"node_ids": ["6b39f7de-e90c-4163-b46d-3a31b5186b98", "abe7815f-4a0a-42c8-8a49-6419e4a9a9a3", "c8c52cce-ec85-408b-bac7-47281d0b973f", "870889d5-121f-4093-92fa-f2edaf2b9a62", "28a4f583-d4f1-40b8-bc58-92cbc9012602", "c8d11877-bc39-46b0-b67e-d255e6e27225", "7b1892aa-1166-484b-9f07-a58dc722146f", "d5e9b0a2-356a-4720-b40a-f0cc8ef54d28", "7534e32d-b763-475c-a4ac-a55776b5b47a", "ca7d69d9-5d42-411a-a918-fba0fbd8e3c4", "5bf4882e-7fd9-4615-9e27-6696bccd312b", "597e1f74-a8f8-41a5-8c5c-b09664850aea", "8faea956-a09b-4548-9943-9553d9a30fa0", "d2c4d149-ea88-482d-8aa1-84a02f4d4512", "59b4af55-e8b4-4dd2-b5a5-3e3fbb97e799"], "metadata": {"page_label": "2", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}}, "bbaa93e8-f909-4e1c-ac49-bf09a93c7129": {"node_ids": ["5dd6cf36-a226-45b7-9ae8-f18a1019719c", "d0f4bd7d-63d6-498c-a521-103d8e327481", "50bc0444-5d77-45c7-8a94-c26767d3ca62", "b170d507-85cb-43e8-9d01-1a6e8ad2b4a1", "470c222f-3c8c-4dee-a761-52ad973f69e6", "841439fc-e91a-4df8-a208-e215dfbd5daf", "6f64e898-47f6-4761-8624-474c4c9705c3", "d1dfc197-1d47-41a5-b039-c784c347c4e9", "5e04024f-cffa-499c-a471-fc8beae013c4", "3d0e057f-7749-4c71-8cd6-b87c52e844a2", "62d35df2-8b78-467c-81fd-33d864e3d891", "7fb8e670-71d6-4cdb-aaf6-68dcdf95d0a5", "34237fa6-9bac-46d1-a490-9e5f747efba9", "3cefa59c-f7df-4e30-a366-c88c0ed43775", "6f9159f4-891e-442c-a9d1-1345a55fae34", "cdee7984-597f-46b9-b8cf-0f0102c8d8ee"], "metadata": {"page_label": "3", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}}, "8f47bd05-4993-421c-a4de-1d820c1ecea4": {"node_ids": ["3112f268-0c61-46c6-b498-a3ce351b73df", "0716e08d-9822-403e-a133-03733eb2b16a", "514aeb59-695b-4304-b7ed-e3856e745009", "da15dbb1-b0be-4c94-9127-d8080dc4e416", "248abee2-d68e-4f2b-8de4-ada1424bce93", "178f6a48-9367-44b2-a33e-049035c423fb", "4428b4dd-66e9-42b0-baf1-1bc6a25a8080", "e1bf8130-a00d-48b4-81e3-6a53b4fd579a", "fc7234a0-cb8b-4648-804c-fbeb6daf4489", "27dfb214-c52b-42e4-882a-b285da59f623", "a5dd264e-ae72-4dd0-83d8-1cfa9ce6dbc5", "e371f207-1421-4299-81ac-26503a01c555", "31a72be0-7df7-43a2-b84e-cc1979ea92dc", "2d51c778-a178-4e67-b40b-99059cd9f1e8", "2db946e8-2f2c-45df-90dd-549c66cb2764"], "metadata": {"page_label": "4", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}}, "504434b7-d7ac-44ee-b82f-d66314dc879f": {"node_ids": ["0b56ecdb-6a66-4781-bb4e-4e01f4bec81c", "4678b6aa-f137-4bf1-834d-8afc2add4a16", "f8d5c18f-d5b8-43b0-a3e7-9f5f01b2a177", "7c92efd4-ba54-499c-8f74-383c7f0bf8f4", "410c0ab8-7f89-4670-bdc0-c166c1f44715", "c30f3b3e-c08b-4991-8127-a8315df260b6", "edfaaeec-115d-4194-8233-7b72f4f85843", "7a3a5af5-513e-4f79-8372-7c640f019673"], "metadata": {"page_label": "5", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}}, "3d84c41c-e27e-4e00-a6e1-eaa4333a9dc5": {"node_ids": ["f6cb09ba-958d-4be7-bbcb-41ed6e52f166", "cfc3015e-c7ec-414b-9535-2fd4f42bfd90", "3a985fc9-8ec1-49ce-8382-c2517ca4fba4", "271b6b37-0c1b-4b6e-ab63-0bc5ff989a1a", "2ff0eb82-2737-43af-9f43-651f258b8f51", "1f305f1e-513a-4372-8911-57d407d90cc9", "66ebaa18-f6a4-4ce8-aeb3-066261e8ce81", "c5c48b96-117e-411d-9fd8-b1c0866f4781", "158031f8-feeb-4b5b-a25a-76e77de0418c", "23edd47c-2e1c-4975-a3bd-6603995ec8fc", "678fbc52-5b8c-4a9d-a131-b7a5151582bb"], "metadata": {"page_label": "6", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}}, "e4937f0c-b264-411d-8d2e-dbca9b578b6e": {"node_ids": ["9674adea-790e-404a-b66e-df9ebe95ff68", "14ce6290-a6d5-402f-821e-f631931a0fb2", "ad39720b-3e57-4eda-9f64-1d0f1527bc27", "531b4297-4bb5-46a8-aab1-969ad39503a7", "3933681c-f4f2-451f-bff3-43d1ea83f0ac", "019c7249-24d1-434a-ab38-5f569a61dc87", "baaf5bc6-958b-4e75-a1c0-dccca96eed11", "f53e45aa-787b-4b90-b28d-2e56e93f3b35", "1341f585-eafd-453c-a502-a412336daa81", "8275cf49-52b2-444c-ae80-56f1b4dfa854"], "metadata": {"page_label": "7", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}}, "379c133f-b178-41b8-b255-09fb40527f5b": {"node_ids": ["bd04cbd9-f8a2-415e-87ee-31fae4d25536", "a2186272-28e4-4876-9cab-21a157fedbf7", "9f2c5c93-2e83-4da1-8f8a-f3debbc13a1e", "2cb7a4b8-a71d-4315-9ca3-03d58900a664", "b9d09769-019c-43b2-985a-054385bb1c24", "456fd25f-8b30-46be-bc17-1756b1e0b9e8", "dbae576a-5efc-4ec3-a083-0741a8f722ed", "63d6c4ab-20af-4de3-94e0-18cfd4dcbfa7", "9f3644b9-ce8b-4b54-9f9d-eb060f0b3e58", "345050e3-f5a5-44de-9e89-d7627b5c89d9", "877bee80-fb5b-4b2c-ac7e-fb4abd7e3fdb", "51675636-1f9f-4468-823a-b8373cf1cc89", "f6976ce7-6b05-4ca6-a3a2-bbb6d535b322", "6044c37d-73e6-4d95-acd9-d294fe0b5967", "96ede007-8026-44a3-8f92-1c7445e282df", "773f6ba9-efb2-4139-b602-945ac5d85c77"], "metadata": {"page_label": "8", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}}, "85ccb18b-1e90-4eb2-961c-fba10142d3b9": {"node_ids": ["f746793b-4175-4b08-811a-d0cd1f17c7a6", "527c9790-8f88-452c-8075-a1d50d4abb37", "6f2ee4d8-c38c-4590-a8f6-29632711875e", "8adabc5c-e492-45b8-8ffc-e9f0d273529a", "51cf68e4-6de2-485d-ae67-2922cc51cafa", "d0516e21-8c2c-4b8a-b038-55de634e68f3", "4c9581b0-6633-4c0a-a3de-8f6bb89eeef6", "79de0656-c33e-48fe-b16e-4b4a5dc56f40", "813b382a-3fa6-46bf-b827-f0c91fe6c876", "eef0880a-fdf4-4a0b-af16-09f9162865da", "3c12baca-0b1d-479f-99d4-f688ec2c4f38", "255a4ca4-5a39-4ba2-a18a-63e427abc4b4", "95bffacd-5b23-4d05-82d3-481a002b8a28", "d556a2c5-2206-4561-953d-3abf5da4651e", "a7859ec4-1226-40ad-bb0b-54244b5ff11a"], "metadata": {"page_label": "9", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}}, "8e235e0e-9b30-40d6-82f0-132a5e7d8b9e": {"node_ids": ["56314dd6-1061-4ddb-a636-db0206f57be9", "6fba6239-e970-42ad-85e5-77522f5a20bc", "be1f73fa-af05-413f-b982-cba94adc557d", "d40b99d0-b88d-42bc-9114-127e80e37eec", "68ff5801-50e7-428f-803a-b533a9e3e46f", "ee82a7a9-c60b-4806-b7d5-c098de5d78b2", "3ccbab52-6288-4576-95a0-a19a7abd4d1f", "9a0e28e3-c119-4cba-b3ae-9acbd1cd82bc", "fd766762-ca70-408f-827d-7285e8780576", "ba52c01b-71d4-4afb-a5e8-a332aa3388d7", "1c68bf0c-3a3e-4374-90a2-5c3320e9da1d", "9bec683a-b82d-44d7-aa0a-26e69d186443", "e224cf01-5cb6-4c6f-8335-c7cf0b51ba6a", "7cc6140d-66be-4eed-aafa-16fa334f286b", "13cd0e23-2706-4e65-abcc-caf9d0e3ffa6", "e071bcdd-702a-4eb1-8c5e-c9d979f31d76", "670ea53d-d1e9-4bf4-9675-6cde9da55787"], "metadata": {"page_label": "10", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}}, "b46805fa-d37e-4c10-b49b-378a48c3c60f": {"node_ids": ["6781525e-8480-44c5-9174-6341b0e015ac", "e6791fd4-90fe-4ef5-af23-4c3fe135e934", "28a3eaf2-be00-4db7-a230-61f7b0c18454", "f79e72eb-6945-4965-b735-42e762c84ee8", "443a33d5-fe83-4c2c-a637-11e220d5e8b9", "21a69d4d-32a2-4343-a878-df7303d60ff1", "2db2b4d6-7a81-4a9c-b6fc-22e1f92717ad", "3c9fdaa2-1d76-4b85-ac8d-74d2f42f5802", "b9475402-a72e-478a-ad98-e336327f54e1", "7cceb124-274d-4a1e-8ea8-4d4286625a10", "83d857a3-86ea-42b0-b6d2-8173642ba453", "a0db060d-6f3f-4100-8503-51203880b818", "af8c6e75-5b3e-4ab3-9fb9-79308a312647", "fde1bfd7-7ad9-4a3f-868e-ff550405e87c", "01fffc19-cc5f-4c5d-a929-1b28af70bca0", "63a222a9-9a11-4255-a06c-572c10c91819", "08b29995-a904-4aa0-afd4-beec83a1cafb", "a67b01e4-3050-4be7-a07d-306bc59c2b32", "e9cc89ed-3da3-46f1-82b7-ccdf3bb4def6"], "metadata": {"page_label": "11", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}}, "fd01a0da-e11d-4095-b518-a3aebffc89eb": {"node_ids": ["b4b9faf5-0f15-4da3-b0af-9070ab8c918f", "dab3435e-7d10-4def-a8ba-4287f2b7802f", "ba1329eb-dd87-47d3-acb2-f06627b0a070", "072e28c3-53dd-4b61-bab1-4c56b8f158b2", "750432d5-3963-4160-aca2-504a84d29755", "858e6d4f-4cb9-452a-bc08-5e275ca9cb74", "38ff3acd-f6e8-4583-be08-942c089b333f", "13a375ac-50af-4d49-a8ca-ff69e6942061", "859b5f2e-8876-4f04-8d41-d74b3515be3b", "5759d082-e98b-40f2-9d57-5a4bf1c97e75", "2deb1e91-700c-4e43-a86e-eb461dda5c30", "95277ebd-ef74-49c4-8d5d-d0e3b85dd531"], "metadata": {"page_label": "12", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}}, "85737b80-6c4a-445e-b173-8c9277e49204": {"node_ids": ["ee7f0fd2-eb8b-4a40-a3c4-62067cf8181a", "20d922a1-47d7-4c52-ac2f-4796b063d8f9", "a7078776-4551-4944-ba1e-8b1ec24cd9e9", "17f82a1d-2aee-4478-ac8f-013105a4b701", "f177db70-122b-4602-9ab4-cd7f3ece95d5", "6189ab3c-eb2e-46a8-9ef4-a2c873357e56", "8247b101-90cc-4661-8f72-78a888d708c7", "562f8e26-f9be-4c6f-a83a-bfe6195546ec", "7949cd4f-c5e9-437d-84f4-462bc1211f3f", "050b11a9-35b3-4a5b-abfe-6a90536f68c5", "f38bc9b7-f176-4299-980d-e2ec65b1083f"], "metadata": {"page_label": "13", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}}, "413315cd-b11e-4878-a2b9-e653548db0f6": {"node_ids": ["4605c950-9913-4c4f-a6be-a97671cf5cdb", "8f751593-1b85-4d9c-a8bb-0e4176fbeac7", "b2c1f884-b681-4218-b1a5-d4badc00bc61", "acd201aa-2168-4178-9c9d-6368e54bcecb", "69d9c997-ccc9-4670-a6b2-6099224c2b49", "a7087c5e-dd79-404a-ac42-ec59a44b3e30", "4a546e0f-a684-43d6-9dfd-c7489a85d753", "5077c8de-7305-4184-a9fc-ee4f5cbfa31c", "c7108d42-1b03-488f-9b45-42e9b2339bde", "05dbb448-bf7d-40ff-854b-4cc2b0cc172a", "5874d62b-383b-4ccc-9117-d2a5e44c32de", "04131aff-4e42-4283-94a1-09140859b23e"], "metadata": {"page_label": "14", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}}, "a9256579-366e-4b1d-821b-7e410b884184": {"node_ids": ["1d4ce580-c085-4728-bd9f-7b321d548077", "6359578d-c194-43bb-a730-686f0f0680a5", "3063fa75-f221-41bc-b87f-86db7e9a3ca0", "398cdb4a-588c-405d-a78d-3007f1e610cd", "621f8d8f-325c-4cc5-8faf-0ca60d8ac2c9", "0c87fcee-c971-45a2-b1e6-c0e689eb9d50", "3301bf78-27a9-4cfd-a84f-14739569d00b"], "metadata": {"page_label": "15", "file_name": "example.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example.pdf", "file_type": "application/pdf", "file_size": 547768, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example.pdf"}}, "f13e8ae9-34c4-472f-9fd5-2bd7c44a33f8": {"node_ids": ["aea7399d-1604-4f71-a6c2-e5ed90e864f2", "3668a3e5-f6cb-4f29-abb7-0cefdb73436b", "6405d710-c524-4550-aa6d-e590008be9f9", "095c0460-a0b6-435c-9893-82878fd88e64", "1bcbeda0-c2e2-44cb-ab6b-c835e19884e8", "64f21f32-de35-476e-add8-69af4ecce98c", "8f789e43-ad43-4f41-a2f9-b0bed5dc4ad9", "e7213d1b-3a36-484a-9fbc-ea4155b6664b"], "metadata": {"page_label": "1", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}, "d29ca23e-6ef3-4619-8c0c-64dc12de09ea": {"node_ids": ["f25a8684-701e-4bf8-94cf-c03306e2618e", "f4ed1c5c-539d-4b3d-a13d-9d4fc5ba828e", "61113d88-0609-496e-91d0-575c2547835f", "83bf573e-f907-4ba2-aada-1a32bfd179ef", "98b2f0f3-f3fb-43db-a496-1ec409796ee8", "18c69308-84f7-4dc8-92ce-8aba47ec6874", "4540d62f-d7cc-4feb-a2c6-fede1456f207", "5bd16d89-190c-49f4-b8d6-a55b730f5f85", "ecdfb7f9-71db-4368-99de-b40bd2642a44", "9f8c5dd6-26fb-481c-831e-711684b41a30", "7ca79a94-2bc0-4855-bbb0-c31be0cfab29", "92d3c07e-4996-4422-b0da-f53f5e19ac8e", "3b23392f-ef78-4af5-b649-cad427e33eec"], "metadata": {"page_label": "2", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}, "831b1971-2e28-49f8-814b-0a40a7f215fa": {"node_ids": ["941e6e63-0642-4911-93c3-1a436c8d6f75", "e40c9e7d-d78a-46f6-8b38-b6f0cac92220", "244a53aa-238d-4837-8a03-9702012af209", "d8c99bef-a6dc-458c-9f15-a7129365f7c9", "998f049d-b5d3-4bd5-80d4-5f2814e83138", "80e443ac-edbf-4a93-8662-05614bf6a340", "7e2c1c46-67c2-4fc0-98cd-9c9fce6549a4", "89a3eb54-3214-4003-adc7-5995d9c40349", "180498c1-ec3e-49ed-b945-0914ef8c282a", "00bec5fd-2fda-407f-af14-2cd7fe9e18ef", "80b5b7dc-f200-4b30-8526-224b6537b1e5", "fb70aaa1-0326-41cd-9992-8b98cc90d3d7", "87c2b0f5-8ae0-4338-b26f-a1ac43d16bcb", "76561b4e-24e4-49d9-a25f-1655f512412e"], "metadata": {"page_label": "3", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}, "36c1daaa-43c5-42e8-b0ba-bbe215b2e595": {"node_ids": ["e28e121b-2c44-48c9-8eba-90d578892858", "9edb9de7-6ee9-421f-9e81-fb3d60d13fd5", "3a4e4613-5acb-4380-9bde-c86134bcf165", "12c32e20-ea7c-40df-b99f-b9cdd50428ed", "9abd727d-3ef9-4096-aa23-0c4fbed05e98", "6b304b51-214e-4e96-9ae5-bfd8b35cde6b", "923f20c4-8403-47b8-ad81-87bcc3e811a4", "c9f15394-3e6a-4dfc-a217-ae33b09c0db9", "1b9780a4-31e2-48cd-afba-128f1e61d040", "3705a049-24a2-4419-8593-395783c26699", "5ff668f4-b456-40cf-9343-12ff2ae87b15", "e5470b4e-a719-4c21-ada0-b6e39c62cac9"], "metadata": {"page_label": "4", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}, "da08e13e-b1f1-495a-aa53-69625e6f7ef2": {"node_ids": ["0df911a4-79c9-4fb7-8323-3cc195a4aaf0", "c00dc7c3-d38f-4011-8a3a-810e1409300a", "236923d0-f07d-4f86-af8b-c3e106eda8de", "ab098901-752f-4dc0-ac9d-8027175549ac", "d21c6dc7-34ff-4315-9adf-36d7707170fd", "b44ef511-8e2d-43b0-8bd2-f2a3029f4eba", "ea51b84f-10cd-4948-b5ea-e5bd7f8f7608", "25a0fcef-3040-44c3-86fa-b0bbfdbbcf34", "a211fd5c-8abc-45da-8e14-505855b4539f", "957eeeb9-ab77-446d-964a-d5839b53adf7", "3081212b-b031-4faf-90cf-3ac781ab69f6", "d2768a8d-1fe1-4c3f-86c8-333f21e2080d", "5dd64e0e-f03e-4080-b0b6-087d1d58f192"], "metadata": {"page_label": "5", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}, "8dd1f785-1909-46b4-ba5a-6fe26aa07ea3": {"node_ids": ["5d782522-bd9f-4b8e-b157-3ffc10792f55", "5c76348b-a7fd-49a6-a3af-aee53fb94002", "5d519cfc-bf1c-4e06-a673-6dc744e8035b", "9b9a34ee-890d-4680-96c6-08291072e510", "a0913454-32c5-4aaa-9f79-7b51c472ab48", "986bf8e8-4913-45bc-bdf3-c06dd56449c2", "46060bcc-6f1d-44ce-a6f7-1918fc4f1fed", "48c35703-9920-42c5-b53a-7a097340cac9", "97709ca3-af29-48ca-9747-e35511cbd4de", "805402fb-4247-4a68-904f-542031e4c451", "4af015e1-7c98-4d3c-8c15-4f2b6b080213", "6ff29eca-6e3f-4a30-9db3-76e56ead421c", "56b85528-65a0-42d8-be31-64dcd6523a70", "e0ac23c6-63b2-4280-8b14-b87114224ee4", "8998f858-59f9-4878-b26a-519dfc30d18a"], "metadata": {"page_label": "6", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}, "2d6d4703-f15a-41d4-9b36-b455938824f6": {"node_ids": ["323c6721-b9d4-4ff2-9359-ed912adde66f", "14ad5045-be95-4ff8-bbfc-125da14a28d9", "cc638e13-ca57-41b3-b18f-85c23a16001a", "cfe1d74a-d20f-4001-a222-5568f3b77b28", "e10ba924-2c64-46ad-9d62-61cefae5022a", "b6e95a98-e8e5-4fd4-867a-4cc5a95e7fb7", "e3af3c46-4db9-49da-95a6-8e417c96f1ca", "2a628b0a-6d21-47fc-8a0b-7f7ab999c8bc", "3a6cf700-64c7-4d3e-a15a-8283746cbb3e", "6092d867-37f3-4da3-83e9-024adacf4dc3", "50220c6d-4184-4db7-8317-c54f2530f230", "0e6f3598-1f75-4c43-8198-3406a01623e6", "76be207d-bb54-4c0e-a819-2ff07f84f5a5"], "metadata": {"page_label": "7", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}, "82569dcb-23d6-4577-baed-6323c76765ea": {"node_ids": ["883c8cba-2a6d-48b4-98ac-02211a140eae", "f3618613-2d30-4a24-a6dd-3b21bc2c69fa", "a77fa51f-9413-43e1-a600-67f0b804f708", "b864fc11-4aa6-4152-a231-4106015bd133", "28b19a65-bbcd-46ea-9a28-997dcfb1cb92", "26d047f5-8d47-4472-904d-ede7a2ea58d5", "8e53ef68-3986-4eef-a7e6-17f4dd76f2d5", "fd1738d3-7246-46bf-8432-f8407638bd16", "ccd06bf4-eb4b-4ca0-ae9f-a9335b625d6a", "89c05dfe-cb96-4016-adfb-82c9fea8ee1b"], "metadata": {"page_label": "8", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}, "18ff9d18-48df-4362-bc4b-738fa5157cb1": {"node_ids": ["6b34a56a-227c-4d94-bcf9-697b67721093", "dc9dfecc-eff1-4e2f-8b32-cced235246c2", "0b3f3a06-4d14-4dcd-821d-7fb2d00097e6", "836f43f4-551b-4bb5-8cff-34115d7bf0ef", "7813656b-3df4-4491-bdf9-ac57c909e464", "c1d36aa7-e929-4a23-9d01-012fa2492d00", "e121a8c4-2990-4a29-9807-184d42608a7e", "0bc6f66d-2959-4dd7-a7ff-dbbb22d4eb17", "abdb4513-33cb-40a9-9228-81a8e59980ac", "647e1f2c-ba7c-48d7-b0d4-433ccb43fbdd", "f6115c24-2088-48a8-92b5-6af7164c0388", "5ad0faae-8a59-4ba0-9243-7958eb21e5d9", "d5344355-c807-4bf5-b930-9d279d987236", "5fdfe646-5a52-4849-a24a-02257c53f74a", "63d9a34c-5359-4681-9f9c-35ab18e34f66"], "metadata": {"page_label": "9", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}, "cc729e7a-34e8-45f6-abd5-d3b10024dcad": {"node_ids": ["04d7f7e6-d8df-4114-acd0-c50b96066c62", "2ebcaa7f-a71c-4581-9fa5-bde810bbb3aa", "c984d430-e376-4720-93fb-30b330251bba", "acab9c82-c85d-486b-a13d-0198006b8995", "c0617e13-734e-47ae-970d-c4a9a26223ac", "d0470b26-6bee-4172-9cfe-3097f5dacc6e", "99fb4455-e3aa-46b5-ad00-69ac772d0f3c", "746269ca-7e0e-4c59-8d14-3b0e79ea2427", "5c8a50ed-b952-4be8-9d5c-b7068eb79d15", "1de9859f-5d33-495c-9ff5-ce4279c1dcb1", "aca874eb-0210-4f88-abc9-4a489da741c5", "35a5af8e-e596-461f-982c-4d2312a039a1", "36bba7f4-2410-430d-ba22-7c76bdc3f638", "f6751e9e-ef5c-46f6-8461-306bd2961104", "68f95953-57bf-4dfd-80f5-07998e1417a1", "f4005cc8-bbe0-49d9-91ef-768a68541133"], "metadata": {"page_label": "10", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}, "cd6a3964-9ede-4b76-8b13-3d149bf79ae2": {"node_ids": ["43f50844-054b-492f-942d-acb3350fbefc", "f621bc79-a3c3-4de2-a439-712b4c6aac99", "6ced4a45-235e-4e17-9756-76a019b810c7", "f5ed7ba3-b428-48aa-945c-49f33ff9fc3e", "9666f0af-601c-4cbc-9e4d-e0deaee4004b", "e362da55-6dee-435a-81eb-add758acb95b", "3eb9743b-8b07-481e-b59d-e88bb1bdc2bf", "d77696b6-e1ff-442a-b61d-4aad50045aff", "c5242756-de26-47c5-94bc-44dde66b0561", "18248ca3-ba28-40fd-876f-8e3ec4212582", "e9fb5076-864e-47c0-b894-055604a57a49", "f0563b1b-1269-44ab-9e7a-9e5fca4458b5", "7193e7dc-58cc-4716-b2fd-2d1fc87a641d", "95504dfc-80ff-4330-a72c-2bced8191393"], "metadata": {"page_label": "11", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}, "52fb437e-fae3-4323-9973-bf7c1dd46d9e": {"node_ids": ["f1340b64-5420-4948-9ad9-6393298eabd8", "8621edb8-0760-4cdf-9a56-a18908ae090f", "4cd265f5-b681-4b82-895f-f89cb2be19cc", "3e5c8e49-4007-4aca-98a2-09a288c1a90a", "8d2d620d-e4ca-46a5-b9a8-9af7c3393959", "969a327c-df6d-43bb-8655-5daf7039e75f", "24a7bc2c-bd50-4ca3-9db3-2c1f6e9cd3bb", "26931cf2-0cf2-484b-a76a-9a2f2c43ba8e", "d4091503-39ec-4971-9906-62c72ce7b27d", "87e0774b-9acb-44e2-bab1-c7ad6265dea7", "2ecf66ed-0607-4e23-ae9c-dfbab7b1cd3e", "971f1641-8cfd-4d7d-9794-6ef485058d15", "50c5b40d-57da-4c64-9744-c6e2650dfe23", "e00b94e0-7373-4313-9913-dad5d7b9ac6c", "f2b734de-2263-42da-9e8a-dbef54b8285b", "816c37a6-ad7d-4e5a-8c47-f2ba4a2b551a", "bbaaae45-f6aa-4e2b-a980-1bbec6ef76ff", "bdb0f1b2-e9fd-4595-b93e-ef551b0c2aed"], "metadata": {"page_label": "12", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}, "580ee1e9-d087-44e6-bbdf-ae04f60f8643": {"node_ids": ["3733d4a4-aa62-4509-b31c-3421108ccea1", "1b90c6b6-2bef-4321-ae32-ce7239f3a15b", "332609b6-6cc3-4580-ae50-def58c52b9b8", "8740f887-2e78-4cba-acbe-a8229f0792d7", "4ba668ab-0283-43a8-b628-40ef6a79be8d", "035a0640-6c10-4cf9-8dee-3b792537ff60", "773717ef-e388-4755-acc0-78a530477037", "aa3f9ded-8b34-4766-9077-4cfc27e09e01", "5948ad4b-b6eb-41e7-b7a9-05c54feeaae0", "0ad3c605-bfe0-47d8-82a9-c7592fdeb73f", "b8971594-96e9-41ad-92b0-12d2798e8912"], "metadata": {"page_label": "13", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}, "edad55e6-fc3c-4e1b-8dec-ada88b0d85bc": {"node_ids": ["58e60fc0-4848-46f2-b261-d6b7be4f5d22", "dc157a10-f735-4b29-87cb-1b2cc43e1bfd", "798accf5-d179-4622-b47e-629bcc3b03fe", "fec9dc9a-e377-4634-8427-1ef0015614c6", "149fbc9c-44ea-4be7-baf4-d05f5c32fe89", "154961d0-1fc1-4844-91f9-7dfb43fd2355", "addcdd2f-35e5-4430-bab0-4e38db2f9491", "607037a1-d640-4113-b469-aa0b97094622", "48131f00-78b7-4d03-ad3d-13b4867dd447", "5f1bd0a5-55c3-47a3-8b84-8373111c4d2b"], "metadata": {"page_label": "14", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}, "a26995a6-144e-4b45-aafd-f1ffbdd56f4e": {"node_ids": ["da64bd59-0a49-4f94-9061-9280d3ea395b", "b7e875f8-361b-4ec1-be90-2abbf7948880", "e645f476-287f-40fb-95dd-7881c1ba5b41", "4b92dc60-fbd1-471b-ae70-b01c5a5ca422", "606ea042-2e61-48a6-b06b-96cc5fe9d1cb", "de5853d4-7893-459b-ba8f-abaddd1a366a", "7f7c10cb-efc6-4d57-9ab9-fc3f8441e878"], "metadata": {"page_label": "15", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}, "2ee4cf9f-afe0-41ab-b5d2-70d8df27e172": {"node_ids": ["d95561c7-0a05-4e06-b029-dcf27e7ae531", "df61960a-d283-41c7-83cf-9f77e76c22e5", "533a6168-a76a-4394-bcef-56bee79e0846", "5ca58590-93f3-40cf-8938-a97260d9fec0", "c2e2929c-217f-4299-adf8-af6986055bde", "dbc05bff-934b-48b3-8b25-d1fa3578cc56", "1986e4e9-c92c-4b9e-8cc6-2e37de112e6e"], "metadata": {"page_label": "16", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}, "c8b90a72-370e-4b48-9eec-86788c3a9950": {"node_ids": ["a0e71c7b-d6a6-41aa-bdd4-5d365ebe763c", "1a841a6d-cb85-481e-b1e9-66577646687c", "2c008bf4-b108-4755-9b26-818fb5d18143", "365a14c3-41ab-4589-af83-4b37ce61a6da", "86a656b6-a94a-41d4-93a1-8396a68209ea", "8135295e-4471-4a6d-be68-867d936baf9d", "8be263b6-84cb-4081-b994-c841e7a1e699", "4d73fb3e-8e6a-4abc-bdb3-103b4a4b59f4"], "metadata": {"page_label": "17", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}, "628dbad3-bd1a-4245-92c3-3c7d359e0257": {"node_ids": ["4dd0b39d-6ab0-41a9-b2e7-adb70c60cdad", "7790ce7d-a65b-479a-b893-c371443490f0", "f03c37e6-8d89-4b22-8722-cf4d7785a36c", "8c3de202-5afa-46ca-9552-6f870f41d8f5", "060808b7-54c3-414a-85d5-f18be4174fbe", "04848a13-e3aa-4da8-8f1a-5e8b05500948", "ec98a1e6-59e8-4bff-b8f0-2b80e8d1006c", "18d58403-5c14-4f16-b452-c39658460db9", "26acad6b-675f-4242-a7bd-9644e0e45bb7", "516f2eca-6080-4df5-8687-8f767fe916ba", "e6f35f87-5756-4c04-8ef0-2bf5f4a96673", "ef8e40ea-e9ae-462e-b2d1-faa9f8108d43", "16baf198-51f0-4086-8cdf-eacf38e61052"], "metadata": {"page_label": "18", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}, "7fda0c6f-7416-4350-b866-6f69b4ed5c49": {"node_ids": ["31f30d0c-dcbb-4175-979c-e30b2caa9838", "40cb7b9e-57cf-42d8-8b0b-f1db386b382a", "64157c44-a4eb-49d8-83a5-823ef37c5ed1", "4e4a1f1f-f72b-475f-a5c4-742b97b8fcf1", "ea97f991-589f-4986-84fb-f960599fda6d", "7306e74d-155e-4d86-bcd4-1c9a8e96953c", "a9263a29-2d49-45df-a5bf-4e8363562db4", "89dcb929-a85d-4f6b-9608-3ed30a1040ab", "9647d7ea-2ed5-4e99-b505-65554b8276b4", "9d833549-9152-4cc9-87f1-2ac2d37006e6"], "metadata": {"page_label": "19", "file_name": "example_2.pdf", "file_path": "/Users/danliu/chatbot/data/llm_finetune/pdfs/example_2.pdf", "file_type": "application/pdf", "file_size": 1168784, "creation_date": "2024-04-28", "last_modified_date": "2024-04-28", "doc_id": "example_2.pdf"}}}}
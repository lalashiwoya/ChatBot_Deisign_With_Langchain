{"docstore/metadata": {"https://www.superannotate.com/blog/llm-fine-tuning": {"doc_hash": "3898ae2a89808e1f6075948b37e994564e3fa9db1be2bf4b336c00374f450800"}, "https://huggingface.co/docs/autotrain/en/llm_finetuning": {"doc_hash": "34180011a1d00bef1acfcb136448606671b0728b0632cff72f8e2faf2901a03f"}, "70642e24-c78d-4422-a93c-2900a7a188f6": {"doc_hash": "dd008ef8592a1860f87556f4cef0f5bad1ef10cb2f04a9e6768b7612f2f6ad1a", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "5ad92bf0-0545-4270-89bb-2cacd1287ebd": {"doc_hash": "cc4dbb7eee29076a9307f5d1db91257581f9ddd34c8c959bfae3676e8d528aa0", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "53bf5d3c-9e7e-409c-a4d0-0e125f337992": {"doc_hash": "0319d2fdc4c946cf8c3f3cdde8a4b2c57d2aecda3e596dd1c26fd58dd082cd22", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "5859e06a-7a8e-4963-96e2-036ec1152e4b": {"doc_hash": "a6b467a43292855b0cb37cbc4c4d881b625d12013af0b28078bdcde513675e79", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "ddb13259-6c69-453d-9b4c-0fc9c3056529": {"doc_hash": "cf6a34a16d5938cf40cff6b8f9aa3bb34171a6124aef11edb53251f47cee3a3f", "ref_doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "c4bace62-e221-452a-97d6-66cb7bbf6d74": {"doc_hash": "7e0c75b6afbc485884a686d3288a0ec0846482c0b9aa24f78045dcfa78ba238b", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "ca218053-fe2c-4546-a084-b8ddd075b87a": {"doc_hash": "a17884993c92ab7e30e384c5d81c59d67f3b34d8c686c8361fcea3ff1d027e0f", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "ba15b53b-5066-4cbd-9b44-b3c46b25e1fe": {"doc_hash": "2dcc1590f1943c781d29a8f4de7eaaaa6d6d6b643a5aaea75f750aea0620fc9f", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "04de788b-a07f-475a-8cb2-d56f1e2c6954": {"doc_hash": "b5f799bf6e00559bbb08533f60bcefab83607f5d6b69b5521e295621022db6a5", "ref_doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}}, "docstore/data": {"70642e24-c78d-4422-a93c-2900a7a188f6": {"__data__": {"id_": "70642e24-c78d-4422-a93c-2900a7a188f6", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "3898ae2a89808e1f6075948b37e994564e3fa9db1be2bf4b336c00374f450800", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ad92bf0-0545-4270-89bb-2cacd1287ebd", "node_type": "1", "metadata": {}, "hash": "926fa041718fb71164834306ce41daa7b82845ffd6474e419b1befa3511869ad", "class_name": "RelatedNodeInfo"}}, "text": "Join our upcoming webinar \"Deriving Business Value from LLMs and RAGs.\"\n\nPlatform\n\nFineTuneCreate top-quality training data across all data types.\n\nLLMs & GenAI\n\nLLM & GenAI Software\n\nSolutions\n\nIndustries and use cases\n\nAgriculture\nHealthcare\nInsurance\nSports\nAutonomous driving\nRobotics\nAerial imagery\nNLP\nSecurity and surveillance\n\nCase studies\n\nHinge Health\nOneCup AI\nPercepto\nOrsi Academy\n\nResources\n\nCompany\n\nBlog\nPodcast\nWebinar\nCareers\nAbout us\n\nPlatform\n\nDocumentation\nWhat's new\nPython SDK\nIntegrations and Security\n\nPricing\n\nSign In\nRequest Demo\n\nFine-tuning large language models (LLMs) in 2024\n\nFebruary 5, 2024\n\nIt's no secret that large language models (LLMs) are evolving at a wild speed and are turning heads in the generative AI industry. Enterprises aren't just intrigued; they're obsessed with LLMs, looking for ways to integrate this technology into their operations. Billions of dollars have been poured into LLM research and development recently. Industry leaders and tech enthusiasts are showing a growing appetite to deepen their understanding of LLMs. While the LLM frontier keeps expanding more and more, staying informed is critical. The value LLMs may add to your business depends on your knowledge and intuition around this technology.\n\nA large language model life cycle has several key steps, and today we're going to cover one of the juiciest and most intensive parts of this cycle - the fine-tuning process. This is a laborious, heavy, but rewarding task that's involved in many language model training processes.\n\nLarge language model lifecycle\n\nVision & scope: First, you should define the project's vision. Determine if your LLM will be a more universal tool or target a specific task like named entity recognition. Clear objectives save time and resources.\n\nModel selection: Choose between training a model from scratch or modifying an existing one. In many cases, adapting a pre-existing model is efficient, but some instances require fine-tuning with a new model.\n\nModel 's performance and adjustment: After preparing your model, you need to assess its performance. If it's unsatisfactory, try prompt engineering or further fine-tuning. We'll focus on this part. Ensure the model's outputs are in sync with human preferences.\n\nEvaluation & iteration: Conduct evaluations regularly using metrics and benchmarks. Iterate between prompt engineering, fine-tuning, and evaluation until you reach the desired outcomes.\n\nDeployment: Once the model performs as expected, deploy it. Optimize for computational efficiency and user experience at this juncture.\n\nWhat is LLM fine-tuning?\n\nLarge language model (LLM) fine-tuning is the process of taking pre-trained models and further training them on smaller, specific datasets to refine their capabilities and improve performance in a particular task or domain. Fine-tuning is about turning general-purpose models and turning them into specialized models. It bridges the gap between generic pre-trained models and the unique requirements of specific applications, ensuring that the language model aligns closely with human expectations. \n\nWhen to use fine-tuning\n\nOur article about large language models touches upon topics like in-context learning and zero/one/few shot inference. In-context learning is a method for improving the prompt through specific task examples within the prompt, offering the LLM a blueprint of what it needs to accomplish. Zero-shot inference incorporates your input data in the prompt without extra examples. If zero-shot inference doesn't yield the desired results, 'one-shot' or 'few-shot inference' can be used. These tactics involve adding one or multiple completed examples within the prompt, helping smaller LLMs perform better.\n\nSupervised fine-tuning (SFT)\n\nSupervised fine-tuning means updating a pre-trained language model using labeled data to do a specific task. The data used has been checked earlier. This is different from unsupervised methods, where data isn't checked. Usually, the initial training of the language model is unsupervised, but fine-tuning is supervised.\n\nHow is fine-tuning performed?\n\nLet's get into more details of fine-tuning in LLMs. For preparing the training data, there are many open-source datasets that offer insights into user behaviors and preferences, even if they aren't directly formatted as instructional data. Once your instruction data set is ready, as with standard supervised learning, you divide the data set into training validation and test splits. During fine-tuning, you select prompts from your training data set and pass them to the LLM, which then generates completions.", "start_char_idx": 0, "end_char_idx": 4635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ad92bf0-0545-4270-89bb-2cacd1287ebd": {"__data__": {"id_": "5ad92bf0-0545-4270-89bb-2cacd1287ebd", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "3898ae2a89808e1f6075948b37e994564e3fa9db1be2bf4b336c00374f450800", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70642e24-c78d-4422-a93c-2900a7a188f6", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "dd008ef8592a1860f87556f4cef0f5bad1ef10cb2f04a9e6768b7612f2f6ad1a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53bf5d3c-9e7e-409c-a4d0-0e125f337992", "node_type": "1", "metadata": {}, "hash": "46eb1f9305a6933094795a3ed69136d570355a94f2261b90d8b01847b7d52399", "class_name": "RelatedNodeInfo"}}, "text": "Supervised fine-tuning (SFT)\n\nSupervised fine-tuning means updating a pre-trained language model using labeled data to do a specific task. The data used has been checked earlier. This is different from unsupervised methods, where data isn't checked. Usually, the initial training of the language model is unsupervised, but fine-tuning is supervised.\n\nHow is fine-tuning performed?\n\nLet's get into more details of fine-tuning in LLMs. For preparing the training data, there are many open-source datasets that offer insights into user behaviors and preferences, even if they aren't directly formatted as instructional data. Once your instruction data set is ready, as with standard supervised learning, you divide the data set into training validation and test splits. During fine-tuning, you select prompts from your training data set and pass them to the LLM, which then generates completions.\n\nDuring the fine-tuning phase, when the model is exposed to a newly labeled dataset specific to the target task, it calculates the error or difference between its predictions and the actual labels. The model then uses this error to adjust its weights, typically via an optimization algorithm like gradient descent. The magnitude and direction of weight adjustments depend on the gradients, which indicate how much each weight contributed to the error.\nWeights that are more responsible for the error are adjusted more, while those less responsible are adjusted less.\n\nOver multiple iterations (or epochs) of the dataset, the model continues to adjust its weights, honing in on a configuration that minimizes the error for the specific task. The aim is to adapt the previously learned general knowledge to the nuances and specific patterns present in the new dataset, thereby making the model more specialized and effective for the target task.\n\nDuring this process, the model is updated with the labeled data. It changes based on the difference between its guesses and the actual answers. This helps the model learn details found in the labeled data. By doing this, the model improves at the task for which it's fine-tuned.\n\nLet's take an example to picture this better; if you ask a pre-trained model,\"Why is the sky blue?\" it might reply, \"Because of the way the atmosphere scatters sunlight.\" This answer is simple and direct. However, the answer might be too brief for a chatbot for a science educational platform. It may need more scientific detail or context based on your guidelines. This is where supervised fine-tuning helps.\n\nAfter fine-tuning, the model can give a more in-depth response to scientific questions. After fine-tuning, when asked, \"Why is the sky blue?\", the model might provide a more detailed explanation like:\n\n\"The sky appears blue because of a phenomenon called Rayleigh scattering. As sunlight enters Earth's atmosphere, it consists of different colors, each with its own wavelength. Blue light has a shorter wavelength and is scattered in all directions by the gases and particles in the atmosphere. This scattering causes the direct sunlight to appear white, but the sky itself to take on a blue hue.\" This enriched response is comprehensive and suitable for a science educational platform.\n\nFine-tuning methods\n\nLLM fine-tuning is a supervised learning process where you use a dataset of labeled examples to update the weights of LLM and make the model improve its ability for specific tasks. Let's explore some of the notable fine-tuning methods.\n\nInstruction fine-tuning\n\nOne strategy used to improve a model's performance on various tasks is instruction fine-tuning. It's about training the machine learning model using examples that demonstrate how the model should respond to the query. The dataset you use for fine-tuning large language models has to serve the purpose of your instruction. For example, suppose you fine-tune your model to improve its summarization skills. In that case, you should build up a dataset of examples that begin with the instruction to summarize, followed by text or a similar phrase. In the case of translation, you should include instructions like \"translate this text.\" These prompt completion pairs allow your model to \"think\" in a new niche way and serve the given specific task.\n\nFull fine-tuning\n\nInstruction fine-tuning, where all of the model's weights are updated, is known as full fine-tuning. The process results in a new version of the model with updated weights. It is important to note that just like pre-training, full fine-tuning requires enough memory and compute budget to store and process all the gradients, optimizers, and other components being updated during training.\n\nParameter-efficient fine-tuning\n\nTraining a language model is a computationally intensive task. For a full LLM fine-tuning, you need memory not only to store the model, but also the parameters that are necessary for the training process.", "start_char_idx": 3742, "end_char_idx": 8634, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53bf5d3c-9e7e-409c-a4d0-0e125f337992": {"__data__": {"id_": "53bf5d3c-9e7e-409c-a4d0-0e125f337992", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "3898ae2a89808e1f6075948b37e994564e3fa9db1be2bf4b336c00374f450800", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ad92bf0-0545-4270-89bb-2cacd1287ebd", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "cc4dbb7eee29076a9307f5d1db91257581f9ddd34c8c959bfae3676e8d528aa0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5859e06a-7a8e-4963-96e2-036ec1152e4b", "node_type": "1", "metadata": {}, "hash": "7dde95b3fdaa920bddbdb1f8396b64b72dc015f62a2d1cc6d6389d655d29986d", "class_name": "RelatedNodeInfo"}}, "text": "In the case of translation, you should include instructions like \"translate this text.\" These prompt completion pairs allow your model to \"think\" in a new niche way and serve the given specific task.\n\nFull fine-tuning\n\nInstruction fine-tuning, where all of the model's weights are updated, is known as full fine-tuning. The process results in a new version of the model with updated weights. It is important to note that just like pre-training, full fine-tuning requires enough memory and compute budget to store and process all the gradients, optimizers, and other components being updated during training.\n\nParameter-efficient fine-tuning\n\nTraining a language model is a computationally intensive task. For a full LLM fine-tuning, you need memory not only to store the model, but also the parameters that are necessary for the training process. Your computer might be able to handle the model weights, but allocating memory for optimizing states, gradients, and forward activations during the training process is a challenging task. Simple hardware cannot handle this amount of hurdle. This is where PEFT is crucial. While full LLM fine-tuning updates every model's weight during the supervised learning process, PEFT methods only update a small set of parameters. This transfer learning technique chooses specific model components and \"freezes\" the rest of the parameters. The result is logically having a much smaller number of parameters than in the original model (in some cases, just 15-20% of the original weights; LoRA can reduce the number of trainable parameters by 10,000 times). This makes memory requirements much more manageable. Not only that, but PEFT is also dealing with catastrophic forgetting. Since it's not touching the original LLM, the model does not forget the previously learned information. Full fine-tuning results in a new version of the model for every task you train on. Each of these is the same size as the original model, so it can create an expensive storage problem if you're fine-tuning for multiple tasks.\n\nOther types of fine-tuning\n\nTransfer learning: Transfer learning is about taking the model that had learned on general-purpose, massive datasets and training it on distinct, task-specific data. This dataset may include labeled examples related to that domain. Transfer learning is used when there is not enough data or a lack of time to train data; the main advantage of it is that it offers a higher learning rate and accuracy after training. You can take existing LLMs that are pre-trained on vast amounts of data, like GPT \u00c2\u00be and BERT, and customize them for your own use case.\n\nTask-specific fine-tuning: Task-specific fine-tuning is a method where the pre-trained model is fine-tuned on a specific task or domain using a dataset designed for that domain. This method requires more data and time than transfer learning but can result in higher performance on the specific task.\n\nFor example, translation using a dataset of examples for that task. Interestingly, good results can be achieved with relatively few examples. Often, just a few hundred or thousand examples can result in good performance compared to the billions of pieces of text that the model saw during its pre-training phase. However, there is a potential downside to fine-tuning on a single task. The process may lead to a phenomenon called catastrophic forgetting.\n\nCatastrophic forgetting happens because the full fine-tuning process modifies the weights of the original LLM. While this leads to great performance on a single fine-tuning task, it can degrade performance on other tasks. For example, while fine-tuning can improve the ability of a model to perform certain NLP tasks like sentiment analysis and result in quality completion, the model may forget how to do other tasks. This model knew how to carry out named entity recognition before fine-tuning correctly identifying.\n\nMulti-task learning: Multi-task fine-tuning is an extension of single-task fine-tuning, where the training dataset consists of example inputs and outputs for multiple tasks. Here, the dataset contains examples that instruct the model to carry out a variety of tasks, including summarization, review rating, code translation, and entity recognition. You train the model on this mixed dataset so that it can improve the performance of the model on all the tasks simultaneously, thus avoiding the issue of catastrophic forgetting. Over many epochs of training, the calculated losses across examples are used to update the weights of the model, resulting in a fine-tuned model that knows how to be good at many different tasks simultaneously. One drawback of multi-task fine-tuned models is that they require a lot of data. You may need as many as 50-100,000 examples in your training set. However, assembling this data can be really worthwhile and worth the effort.", "start_char_idx": 7788, "end_char_idx": 12654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5859e06a-7a8e-4963-96e2-036ec1152e4b": {"__data__": {"id_": "5859e06a-7a8e-4963-96e2-036ec1152e4b", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "3898ae2a89808e1f6075948b37e994564e3fa9db1be2bf4b336c00374f450800", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53bf5d3c-9e7e-409c-a4d0-0e125f337992", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "0319d2fdc4c946cf8c3f3cdde8a4b2c57d2aecda3e596dd1c26fd58dd082cd22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddb13259-6c69-453d-9b4c-0fc9c3056529", "node_type": "1", "metadata": {}, "hash": "f1e7764577dd361bec6dc394cefe98b9bf8828b2d539c95b1f4bf45ed0c6096f", "class_name": "RelatedNodeInfo"}}, "text": "Multi-task learning: Multi-task fine-tuning is an extension of single-task fine-tuning, where the training dataset consists of example inputs and outputs for multiple tasks. Here, the dataset contains examples that instruct the model to carry out a variety of tasks, including summarization, review rating, code translation, and entity recognition. You train the model on this mixed dataset so that it can improve the performance of the model on all the tasks simultaneously, thus avoiding the issue of catastrophic forgetting. Over many epochs of training, the calculated losses across examples are used to update the weights of the model, resulting in a fine-tuned model that knows how to be good at many different tasks simultaneously. One drawback of multi-task fine-tuned models is that they require a lot of data. You may need as many as 50-100,000 examples in your training set. However, assembling this data can be really worthwhile and worth the effort. The resulting models are often very capable and suitable for use in situations where good performance at many tasks is desirable.\n\nSequential fine-tuning: Sequential fine-tuning is about sequentially adapting a pre-trained model on several related tasks. After the initial transfer to a general domain, the LLM might be fine-tuned on a more specific subset. For instance, it can be fine-tuned from general language to medical language and then from medical language to pediatric cardiology.\n\nNote that there are other fine-tuning examples \u00e2\u0080\u0093 adaptive, behavioral, and instruction, reinforced fine-tuning of large language models. These cover some important specific cases for training language models.\n\nRetrieval augmented generation (RAG)\n\nRetrieval augmented generation (RAG) is a well-known alternative to fine-tuning and is a combination of natural language generation and information retrieval. RAG ensures that language models are grounded by external up-to-date knowledge sources/relevant documents and provides sources. This technique bridges the gap between general-purpose models' vast knowledge and the need for precise, up-to-date information with rich context. Thus, RAG is an essential technique for situations where facts can evolve over time. Grok, the recent invention of xAI, uses RAG techniques to ensure its information is fresh and current.\n\nOne advantage that RAG has over fine-tuning is information management. Traditional fine-tuning embeds data into the model's architecture, essentially 'hardwriting' the knowledge, which prevents easy modification. On the other hand, RAG permits continuous updates in training data and allows removal/revision of data, ensuring the model remains current and accurate.\n\nIn the context of language models, RAG and fine-tuning are often perceived as competing methods. However, their combined use can lead to significantly enhanced performance. Particularly, fine-tuning can be applied to RAG systems to identify and improve their weaker components, helping them excel at specific LLM tasks.\n\nFine-tuning in SuperAnnotate\n\nChoosing the right tool means ensuring your AI understands exactly what you need, which can save you time, money, and protect your reputation. Look at the Air Canada situation, for example. Their AI chatbot hallucinated and gave a customer incorrect information, misleading him into buying full-price ticket. While we can't pin it down to fine-tuning for sure, it's likely that better fine-tuning might have avoided the problem. This just shows how crucial it is to pick a fine-tuning tool that ensures your AI works just right. It's precisely situations like these where SuperAnnotate steps in to make a difference.\n\nSuperAnnotate's LLM tool provides a cutting-edge approach to designing optimal training data for fine-tuning language models. Through its highly customizable LLM editor, users are given a comprehensive platform to create a broad spectrum of LLM use cases tailored to specific business needs. As a result, customers can ensure that their training data is not only high-quality but also directly aligned with the requirements of their projects.\n\nHere's what you need to know about SuperAnnotate's LLM fine-tuning tool:\n\nIts fully customizable interface allows you to gather data for your specific use case efficiently. Even if it's unique.\nWe work with a world-class team of experts and people management, which makes it a breeze to scale to hundreds or thousands of people.\nThe analytics and insights of our platform are invaluable gems for our customers. It allows a better understanding of the data and enforces quality standards.\nAPI integrations make it easy to set up a model in the loop, AI feedback and much more.\n\nThe tool has practical applications in various areas.\nThe playground offers templates like GPT fine-tuning, chat rating, using RLHF for image generation, model comparison, video captioning, supervised fine-tuning, and more.", "start_char_idx": 11692, "end_char_idx": 16599, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddb13259-6c69-453d-9b4c-0fc9c3056529": {"__data__": {"id_": "ddb13259-6c69-453d-9b4c-0fc9c3056529", "embedding": null, "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.superannotate.com/blog/llm-fine-tuning", "node_type": "4", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "3898ae2a89808e1f6075948b37e994564e3fa9db1be2bf4b336c00374f450800", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5859e06a-7a8e-4963-96e2-036ec1152e4b", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "a6b467a43292855b0cb37cbc4c4d881b625d12013af0b28078bdcde513675e79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4bace62-e221-452a-97d6-66cb7bbf6d74", "node_type": "1", "metadata": {}, "hash": "7498e294bed5b979df90c216b53d0b8ad0bf4627bedbaf04c464169609cf4950", "class_name": "RelatedNodeInfo"}}, "text": "As a result, customers can ensure that their training data is not only high-quality but also directly aligned with the requirements of their projects.\n\nHere's what you need to know about SuperAnnotate's LLM fine-tuning tool:\n\nIts fully customizable interface allows you to gather data for your specific use case efficiently. Even if it's unique.\nWe work with a world-class team of experts and people management, which makes it a breeze to scale to hundreds or thousands of people.\nThe analytics and insights of our platform are invaluable gems for our customers. It allows a better understanding of the data and enforces quality standards.\nAPI integrations make it easy to set up a model in the loop, AI feedback and much more.\n\nThe tool has practical applications in various areas.\nThe playground offers templates like GPT fine-tuning, chat rating, using RLHF for image generation, model comparison, video captioning, supervised fine-tuning, and more. These features address real-world needs in the large language model market, and there's an article available for those interested in a deeper understanding of the tool's capabilities.\n\nAnnotated question-response pairs are sets of data where you have a question, the model's response, and annotations that provide insight into the quality, accuracy, or other attributes of that response. This structured data is valuable when training and fine-tuning models, as it offers direct feedback on the model's performance.\n\nIn terms of data collection, SuperAnnotate offers the ability to gather annotated question-response pairs. These can be downloaded in a JSON format, making it easy to store and use them for future fine-tuning tasks. It's a straightforward tool designed to simplify and enhance the language model training process.\n\nFine-tuning best practices:\n\nClearly define your task: Defining your task is a foundational step in the process of fine-tuning large language models. It offers focus and direction, ensuring the model's capabilities are channeled towards achieving a specific goal.\n\nChoose and use the right pre-trained model: Using pre-trained models for fine-tuning is crucial as it leverages knowledge acquired from vast amounts of data, making the process computationally efficient and time-saving.\n\nSet hyperparameters: Adjusting hyperparameters like learning rate, batch size, and others is key in finding the optimal configuration for your task.\n\nEvaluate model performance: Assess the model's performance on the test set to get an unbiased evaluation of its expected performance on unseen data.\n\nWhy or when does your business need a fine-tuned model?\n\nLLMs may not be acquainted with specific terminologies, nuances, or contexts relevant to a business. Fine-tuning ensures the model understands and generates content relevant to the business.\n\nImproved accuracy: Fine-tuning business-specific data can help achieve higher accuracy levels, aligning the model's outputs closely with expectations.\n\nCustomized interactions: Fine-tuning helps tailor responses to match a brand's voice, tone, and guidelines for consistent user experience.\n\nData privacy and security: Fine-tuning allows businesses to control the data the model is exposed to, ensuring sensitive information isn't leaked.\n\nAddressing rare scenarios: Fine-tuning ensures that edge cases specific to a business's domain are catered to effectively.\n\nFine-tuning sharpens LLM capabilities to fit a business's needs, ensuring optimal performance and results.", "start_char_idx": 15647, "end_char_idx": 19137, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4bace62-e221-452a-97d6-66cb7bbf6d74": {"__data__": {"id_": "c4bace62-e221-452a-97d6-66cb7bbf6d74", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "34180011a1d00bef1acfcb136448606671b0728b0632cff72f8e2faf2901a03f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddb13259-6c69-453d-9b4c-0fc9c3056529", "node_type": "1", "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}, "hash": "cf6a34a16d5938cf40cff6b8f9aa3bb34171a6124aef11edb53251f47cee3a3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca218053-fe2c-4546-a084-b8ddd075b87a", "node_type": "1", "metadata": {}, "hash": "1d79310e1e68a2d436ff6c96f55ee18e91ce1377e8d10a5b9ded8587e78424d9", "class_name": "RelatedNodeInfo"}}, "text": "Hugging Face\n\nModels\nDatasets\nSpaces\nPosts\nDocs\nSolutions\n\nPricing\nLog In\nSign Up\n\nAutoTrain documentation\n\nLLM Finetuning\n\nAutoTrain\n\nView all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerAutoTrainBitsandbytesCompetitionsDataset viewerDatasetsDiffusersEvaluateGoogle TPUsGradioHubHub Python LibraryHuggingface.jsInference API (serverless)Inference Endpoints (dedicated)OptimumPEFTSafetensorsSentence TransformersTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jstimm\n\nSearch documentation\n\nmainv0.7.69v0.6.48v0.5.2 EN\n\nGetting Started\n\nAutoTrain Installation How much does it cost? Get help and support\n\nStarting AutoTrain\n\nStarting the UI Starting the CLI\n\nTasks\n\nText Classification LLM Finetuning Image Classification DreamBooth Seq2Seq Token Classification Tabular\n\nYou are viewing the main version, which requires installation from source. If you'd like regular pip install, check out the latest stable version (v0.7.69).\n\nJoin the Hugging Face community\n\nand get access to the augmented documentation experience\n\nCollaborate on models, datasets, and Spaces\n\nFaster examples with accelerated inference\n\nSwitch between documentation themes\n\nSign Up to get started\n\nLLM Finetuning\n\nWith AutoTrain, you can easily finetune large language models (LLMs) on your own data!\n\nAutoTrain supports the following types of LLM finetuning:\n\nCausal Language Modeling (CLM)\nMasked Language Modeling (MLM) [Coming Soon]\n\nData Preparation\n\nLLM finetuning accepts data in CSV format.\n\nData Format For SFT / Generic Trainer\n\nFor SFT / Generic Trainer, the data should be in the following format:\n\ntext\n---\nhuman: hello \\n bot: hi nice to meet you\nhuman: how are you \\n bot: I am fine\nhuman: What is your name? \\n bot: My name is Mary\nhuman: Which is the best programming language? \\n bot: Python\n\nAn example dataset for this format can be found here:\n\nFor SFT/Generic training, your dataset must have a text column\n\nData Format For Reward Trainer\n\nFor Reward Trainer, the data should be in the following format:\n\ntext | rejected_text\n---\nhuman: hello \\n bot: hi nice to meet you | human: hello \\n bot: leave me alone\nhuman: how are you \\n bot: I am fine | human: how are you \\n bot: I am not fine\nhuman: What is your name? \\n bot: My name is Mary | human: What is your name? \\n bot: Whats it to you?\nhuman: Which is the best programming language? \\n bot: Python | human: Which is the best programming language? \\n bot: Javascript\n\nFor Reward Trainer, your dataset must have a text column (aka chosen text) and a rejected_text column.\n\nData Format For DPO Trainer\n\nFor DPO Trainer, the data should be in the following format:\n\nprompt | text | rejected_text\n---\nhello | hi nice to meet you | leave me alone\nhow are you | I am fine | I am not fine\nWhat is your name? | My name is Mary | Whats it to you?\nWhat is your name? | My name is Mary | I dont have a name\nWhich is the best programming language? | Python | Javascript\nWhich is the best programming language? | Python | C++\nWhich is the best programming language? | Java | C++\n\nFor DPO Trainer, your dataset must have a prompt column, a text column (aka chosen text) and a rejected_text column.\n\nFor all tasks, you can use both CSV and JSONL files!", "start_char_idx": 0, "end_char_idx": 3241, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca218053-fe2c-4546-a084-b8ddd075b87a": {"__data__": {"id_": "ca218053-fe2c-4546-a084-b8ddd075b87a", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "34180011a1d00bef1acfcb136448606671b0728b0632cff72f8e2faf2901a03f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4bace62-e221-452a-97d6-66cb7bbf6d74", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "7e0c75b6afbc485884a686d3288a0ec0846482c0b9aa24f78045dcfa78ba238b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba15b53b-5066-4cbd-9b44-b3c46b25e1fe", "node_type": "1", "metadata": {}, "hash": "871d1dab69d0459c1ffaab97592f8a1ead1f78c87b091057d2a1f3231d3162fe", "class_name": "RelatedNodeInfo"}}, "text": "\\n bot: Javascript\n\nFor Reward Trainer, your dataset must have a text column (aka chosen text) and a rejected_text column.\n\nData Format For DPO Trainer\n\nFor DPO Trainer, the data should be in the following format:\n\nprompt | text | rejected_text\n---\nhello | hi nice to meet you | leave me alone\nhow are you | I am fine | I am not fine\nWhat is your name? | My name is Mary | Whats it to you?\nWhat is your name? | My name is Mary | I dont have a name\nWhich is the best programming language? | Python | Javascript\nWhich is the best programming language? | Python | C++\nWhich is the best programming language? | Java | C++\n\nFor DPO Trainer, your dataset must have a prompt column, a text column (aka chosen text) and a rejected_text column.\n\nFor all tasks, you can use both CSV and JSONL files!\n\nParameters\n\nCopied\n\nautotrain llm --help\nusage: autotrain <command> [<args>] llm [-h] [--train] [--deploy] [--inference] [--username USERNAME]\n                                        [--backend {local-cli,spaces-a10gl,spaces-a10gs,spaces-a100,spaces-t4m,spaces-t4s,spaces-cpu,spaces-cpuf}]\n                                        [--token TOKEN] [--push-to-hub] --model MODEL --project-name PROJECT_NAME [--data-path DATA_PATH]\n                                        [--train-split TRAIN_SPLIT] [--valid-split VALID_SPLIT] [--batch-size BATCH_SIZE] [--seed SEED]\n                                        [--epochs EPOCHS] [--gradient_accumulation GRADIENT_ACCUMULATION] [--disable_gradient_checkpointing]\n                                        [--lr LR] [--log {none,wandb,tensorboard}] [--text_column TEXT_COLUMN]\n                                        [--rejected_text_column REJECTED_TEXT_COLUMN] [--prompt-text-column PROMPT_TEXT_COLUMN]\n                                        [--model-ref MODEL_REF] [--warmup_ratio WARMUP_RATIO] [--optimizer OPTIMIZER] [--scheduler SCHEDULER]\n                                        [--weight_decay WEIGHT_DECAY] [--max_grad_norm MAX_GRAD_NORM] [--add_eos_token] [--block_size BLOCK_SIZE]\n                                        [--peft] [--lora_r LORA_R] [--lora_alpha LORA_ALPHA] [--lora_dropout LORA_DROPOUT]\n                                        [--logging_steps LOGGING_STEPS] [--evaluation_strategy {epoch,steps,no}]\n                                        [--save_total_limit SAVE_TOTAL_LIMIT] [--auto_find_batch_size]\n                                        [--mixed_precision {fp16,bf16,None}] [--quantization {int4,int8,None}] [--model_max_length MODEL_MAX_LENGTH]\n                                        [--max_prompt_length MAX_PROMPT_LENGTH] [--max_completion_length MAX_COMPLETION_LENGTH]\n                                        [--trainer {default,dpo,sft,orpo,reward}] [--target_modules TARGET_MODULES] [--merge_adapter]\n                                        [--use_flash_attention_2] [--dpo-beta DPO_BETA] [--chat_template {tokenizer,chatml,zephyr,None}]\n                                        [--padding {left,right,None}]\n\nRun AutoTrain LLM\n\noptions:\n  -h, --help            show this help message and exit\n  --train               Command to train the model\n  --deploy              Command to deploy the model (limited availability)\n  --inference           Command to run inference (limited availability)\n  --username USERNAME   Hugging Face Hub Username\n  --backend {local-cli,spaces-a10gl,spaces-a10gs,spaces-a100,spaces-t4m,spaces-t4s,spaces-cpu,spaces-cpuf}\n                        Backend to use: default or spaces. Spaces backend requires push_to_hub & username. Advanced users only.\n  --token TOKEN         Your Hugging Face API token. Token must have write access to the model hub.\n  --push-to-hub         Push to hub after training will push the trained model to the Hugging Face model hub.\n  --model MODEL         Base model to use for training\n  --project-name PROJECT_NAME\n                        Output directory / repo id for trained model (must be unique on hub)\n  --data-path DATA_PATH\n                        Train dataset to use.", "start_char_idx": 2452, "end_char_idx": 6457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba15b53b-5066-4cbd-9b44-b3c46b25e1fe": {"__data__": {"id_": "ba15b53b-5066-4cbd-9b44-b3c46b25e1fe", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "34180011a1d00bef1acfcb136448606671b0728b0632cff72f8e2faf2901a03f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca218053-fe2c-4546-a084-b8ddd075b87a", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "a17884993c92ab7e30e384c5d81c59d67f3b34d8c686c8361fcea3ff1d027e0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04de788b-a07f-475a-8cb2-d56f1e2c6954", "node_type": "1", "metadata": {}, "hash": "345616359b1e71078d4973612bf46c2f18c2c4aeddb7a27f53fd6f83b90a4a0b", "class_name": "RelatedNodeInfo"}}, "text": "Spaces backend requires push_to_hub & username. Advanced users only.\n  --token TOKEN         Your Hugging Face API token. Token must have write access to the model hub.\n  --push-to-hub         Push to hub after training will push the trained model to the Hugging Face model hub.\n  --model MODEL         Base model to use for training\n  --project-name PROJECT_NAME\n                        Output directory / repo id for trained model (must be unique on hub)\n  --data-path DATA_PATH\n                        Train dataset to use. When using cli, this should be a directory path containing training and validation data in appropriate\n                        formats\n  --train-split TRAIN_SPLIT\n                        Train dataset split to use\n  --valid-split VALID_SPLIT\n                        Validation dataset split to use\n  --batch-size BATCH_SIZE, --train-batch-size BATCH_SIZE\n                        Training batch size to use\n  --seed SEED           Random seed for reproducibility\n  --epochs EPOCHS       Number of training epochs\n  --gradient_accumulation GRADIENT_ACCUMULATION, --gradient-accumulation GRADIENT_ACCUMULATION\n                        Gradient accumulation steps\n  --disable_gradient_checkpointing, --disable-gradient-checkpointing, --disable-gc\n                        Disable gradient checkpointing\n  --lr LR               Learning rate\n  --log {none,wandb,tensorboard}\n                        Use experiment tracking\n  --text_column TEXT_COLUMN, --text-column TEXT_COLUMN\n                        Specify the dataset column to use for text data. This parameter is essential for models processing textual information.\n                        Default is 'text'.\n  --rejected_text_column REJECTED_TEXT_COLUMN, --rejected-text-column REJECTED_TEXT_COLUMN\n                        Define the column to use for storing rejected text entries, which are typically entries that do not meet certain criteria\n                        for processing. Default is 'rejected'. Used only for orpo, dpo, and reward trainerss\n  --prompt-text-column PROMPT_TEXT_COLUMN, --prompt-text-column PROMPT_TEXT_COLUMN\n                        Identify the column that contains prompt text for tasks requiring contextual inputs, such as conversation or completion\n                        generation. Default is 'prompt'. Used only for dpo trainer\n  --model-ref MODEL_REF\n                        Reference model to use for DPO when not using PEFT\n  --warmup_ratio WARMUP_RATIO, --warmup-ratio WARMUP_RATIO\n                        Set the proportion of training allocated to warming up the learning rate, which can enhance model stability and performance\n                        at the start of training. Default is 0.1\n  --optimizer OPTIMIZER\n                        Choose the optimizer algorithm for training the model. Different optimizers can affect the training speed and model\n                        performance. 'adamw_torch' is used by default.\n  --scheduler SCHEDULER\n                        Select the learning rate scheduler to adjust the learning rate based on the number of epochs. 'linear' decreases the\n                        learning rate linearly from the initial lr set. Default is 'linear'. Try 'cosine' for a cosine annealing schedule.\n  --weight_decay WEIGHT_DECAY, --weight-decay WEIGHT_DECAY\n                        Define the weight decay rate for regularization, which helps prevent overfitting by penalizing larger weights. Default is\n                        0.0\n  --max_grad_norm MAX_GRAD_NORM, --max-grad-norm MAX_GRAD_NORM\n                        Set the maximum norm for gradient clipping, which is critical for preventing gradients from exploding during\n                        backpropagation. Default is 1.0.\n  --add_eos_token, --add-eos-token\n                        Toggle whether to automatically add an End Of Sentence (EOS) token at the end of texts, which can be critical for certain\n                        types of models like language models. Only used for `default` trainer\n  --block_size BLOCK_SIZE, --block-size BLOCK_SIZE\n                        Specify the block size for processing sequences. This is the maximum sequence length or length of one block of text. Setting to\n                        -1 determines block size automatically. Default is -1.\n  --peft, --use-peft    Enable LoRA-PEFT\n  --lora_r LORA_R, --lora-r LORA_R\n                        Set the 'r' parameter for Low-Rank Adaptation (LoRA). Default is 16.\n  --lora_alpha LORA_ALPHA, --lora-alpha LORA_ALPHA\n                        Specify the 'alpha' parameter for LoRA. Default is 32.\n  --lora_dropout LORA_DROPOUT, --lora-dropout LORA_DROPOUT\n                        Set the dropout rate within the LoRA layers to help prevent overfitting during adaptation. Default is 0.05.", "start_char_idx": 5931, "end_char_idx": 10714, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04de788b-a07f-475a-8cb2-d56f1e2c6954": {"__data__": {"id_": "04de788b-a07f-475a-8cb2-d56f1e2c6954", "embedding": null, "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning", "node_type": "4", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "34180011a1d00bef1acfcb136448606671b0728b0632cff72f8e2faf2901a03f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba15b53b-5066-4cbd-9b44-b3c46b25e1fe", "node_type": "1", "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}, "hash": "2dcc1590f1943c781d29a8f4de7eaaaa6d6d6b643a5aaea75f750aea0620fc9f", "class_name": "RelatedNodeInfo"}}, "text": "This is the maximum sequence length or length of one block of text. Setting to\n                        -1 determines block size automatically. Default is -1.\n  --peft, --use-peft    Enable LoRA-PEFT\n  --lora_r LORA_R, --lora-r LORA_R\n                        Set the 'r' parameter for Low-Rank Adaptation (LoRA). Default is 16.\n  --lora_alpha LORA_ALPHA, --lora-alpha LORA_ALPHA\n                        Specify the 'alpha' parameter for LoRA. Default is 32.\n  --lora_dropout LORA_DROPOUT, --lora-dropout LORA_DROPOUT\n                        Set the dropout rate within the LoRA layers to help prevent overfitting during adaptation. Default is 0.05.\n  --logging_steps LOGGING_STEPS, --logging-steps LOGGING_STEPS\n                        Determine how often to log training progress in terms of steps. Setting it to '-1' determines logging steps automatically.\n  --evaluation_strategy {epoch,steps,no}, --evaluation-strategy {epoch,steps,no}\n                        Choose how frequently to evaluate the model's performance, with 'epoch' as the default, meaning at the end of each training\n                        epoch\n  --save_total_limit SAVE_TOTAL_LIMIT, --save-total-limit SAVE_TOTAL_LIMIT\n                        Limit the total number of saved model checkpoints to manage disk usage effectively. Default is to save only the latest\n                        checkpoint\n  --auto_find_batch_size, --auto-find-batch-size\n                        Automatically determine the optimal batch size based on system capabilities to maximize efficiency.\n  --mixed_precision {fp16,bf16,None}, --mixed-precision {fp16,bf16,None}\n                        Choose the precision mode for training to optimize performance and memory usage. Options are 'fp16', 'bf16', or None for\n                        default precision. Default is None.\n--quantization {int4,int8,None}\nChoose the quantization level to reduce model size and potentially increase inference speed. Options include 'int4', 'int8', or None. Enabling requires --peft\n\n--model_max_length MODEL_MAX_LENGTH\nSet the maximum length for the model to process in a single batch, which can affect both performance and memory usage. Default is 1024\n\n--max_prompt_length MAX_PROMPT_LENGTH\nSpecify the maximum length for prompts used in training, particularly relevant for tasks requiring initial contextual input. Used only for `orpo` trainer.\n\n--max_completion_length MAX_COMPLETION_LENGTH\nCompletion length to use, for orpo: encoder-decoder models only\n\n--trainer {default,dpo,sft,orpo,reward}\nTrainer type to use\n\n--target_modules TARGET_MODULES\nIdentify specific modules within the model architecture to target with adaptations or optimizations, such as LoRA. Comma separated list of module names. Default is 'all-linear'.\n\n--merge_adapter\nUse this flag to merge PEFT adapter with the model\n\n--use_flash_attention_2\nUse flash attention 2\n\n--dpo-beta DPO_BETA\nBeta for DPO trainer\n\n--chat_template {tokenizer,chatml,zephyr,None}\nApply a specific template for chat-based interactions, with options including 'tokenizer', 'chatml', 'zephyr', or None. This setting can shape the model's conversational behavior.\n\n--padding {left,right,None}\nSpecify the padding direction for sequences, critical for models sensitive to input alignment. Options include 'left', 'right', or None\n\n[< > Update on GitHub](https://github.com/huggingface/autotrain-advanced/blob/main/docs/source/llm_finetuning.mdx)", "start_char_idx": 10067, "end_char_idx": 13493, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"https://www.superannotate.com/blog/llm-fine-tuning": {"node_ids": ["70642e24-c78d-4422-a93c-2900a7a188f6", "5ad92bf0-0545-4270-89bb-2cacd1287ebd", "53bf5d3c-9e7e-409c-a4d0-0e125f337992", "5859e06a-7a8e-4963-96e2-036ec1152e4b", "ddb13259-6c69-453d-9b4c-0fc9c3056529"], "metadata": {"doc_id": "https://www.superannotate.com/blog/llm-fine-tuning"}}, "https://huggingface.co/docs/autotrain/en/llm_finetuning": {"node_ids": ["c4bace62-e221-452a-97d6-66cb7bbf6d74", "ca218053-fe2c-4546-a084-b8ddd075b87a", "ba15b53b-5066-4cbd-9b44-b3c46b25e1fe", "04de788b-a07f-475a-8cb2-d56f1e2c6954"], "metadata": {"doc_id": "https://huggingface.co/docs/autotrain/en/llm_finetuning"}}}}